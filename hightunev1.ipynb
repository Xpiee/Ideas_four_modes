{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning for classifier for EDA Peak stat features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from numpy import vstack, hstack, stack\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import lightgbm\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training_Code.config import SELECTCOLS, ECG_SELECTCOLS, EDA_SELECTCOLS\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kfolds(dr_feat_path):\n",
    "    subjects = os.listdir(dr_feat_path)\n",
    "    xtrainDriv = pd.DataFrame()\n",
    "    for subTrain in subjects:\n",
    "        train = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(subTrain)))\n",
    "        train[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0)\n",
    "        if np.isinf(train).values.sum():\n",
    "            cinf = np.isinf(train).values.sum()\n",
    "            print(\"Train Dataframe contains {} values\".format(cinf))\n",
    "        train.replace([np.inf], 9999, inplace=True)        \n",
    "        train.replace([-np.inf], -9999, inplace=True)        \n",
    "\n",
    "        train['scrNumPeaks'] = train['scrNumPeaks'].values.astype(int)\n",
    "        train['scrNumPeaks'] = train['scrNumPeaks'].values.clip(min=0) # converting negatives to zero\n",
    "\n",
    "        train.dropna(inplace=True)\n",
    "        xtrainDriv = xtrainDriv.append(train)\n",
    "        xtrainDriv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return xtrainDriv.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(dataset, folder, basefolder):\n",
    "    dr_feat_path = r'X:\\All Modes\\{}\\ECG EDA\\Combined\\{}'.format(dataset, folder) # ECG_EDA_Features_Combined_scld\n",
    "    bs_feat_path = r'X:\\All Modes\\{}\\ECG EDA\\Combined\\{}'.format(dataset, basefolder) # ECG_EDA_Base2_Features_Combined\n",
    "\n",
    "    XtrainDriv = make_kfolds(dr_feat_path)\n",
    "    XtrainBase = make_kfolds(bs_feat_path)\n",
    "\n",
    "    XtrainDriv = XtrainDriv[SELECTCOLS].copy()\n",
    "    ytrainDriv = list(XtrainDriv['scaled label'].copy())\n",
    "\n",
    "    XtrainBase = XtrainBase[SELECTCOLS[:-3]].copy()\n",
    "    ytrainBase = XtrainBase.shape[0] * [0]\n",
    "\n",
    "    XtrainDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "    XtrainDriv = XtrainDriv.append(XtrainBase)\n",
    "\n",
    "    ytrain = ytrainDriv + ytrainBase\n",
    "\n",
    "    X = XtrainDriv.values\n",
    "    \n",
    "    X, ytrain = shuffle(X, ytrain, random_state=42)\n",
    "\n",
    "    for idx, val in enumerate(ytrain):\n",
    "        if val <= 4:\n",
    "            ytrain[idx] = 0\n",
    "        else: ytrain[idx] = 1\n",
    "\n",
    "    return X, ytrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = read_dataset('MatbII', 'ECG_EDA_Features_Combined_scld', 'ECG_EDA_Base2_Features_Combined')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.20, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3399, 89)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=50,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [30, 50, 70, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 800, 1000, 2000,\n",
       "                                                         3000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_grid = {'bootstrap': [True, False],\n",
    " 'max_depth': [30, 50, 70, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [100, 800, 1000, 2000, 3000]}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1000,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 30,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81       459\n",
      "           1       0.79      0.71      0.75       391\n",
      "\n",
      "    accuracy                           0.78       850\n",
      "   macro avg       0.79      0.78      0.78       850\n",
      "weighted avg       0.78      0.78      0.78       850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = rf_random.best_estimator_\n",
    "hist = model.fit(X_train, y_train)\n",
    "predicted_y = model.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = read_dataset('Virage', 'ECG_EDA_Features_Combined_scld', 'ECG_EDA_Base2_Features_Combined')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.20, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4168, 89)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=50,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [30, 50, 70, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 800, 1000, 2000,\n",
       "                                                         3000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_grid = {'bootstrap': [True, False],\n",
    " 'max_depth': [30, 50, 70, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [100, 800, 1000, 2000, 3000]}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 2000,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 30,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       636\n",
      "           1       0.85      0.83      0.84       407\n",
      "\n",
      "    accuracy                           0.88      1043\n",
      "   macro avg       0.87      0.87      0.87      1043\n",
      "weighted avg       0.88      0.88      0.88      1043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = rf_random.best_estimator_\n",
    "hist = model.fit(X_train, y_train)\n",
    "predicted_y = model.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(bootstrap=False, max_depth=30,\n",
       "                                                 n_estimators=2000))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "sel = SelectFromModel(model)\n",
    "sel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anubhav\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 4 is smaller than n_iter=50. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=SVC(), n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'C': [1, 10, 50, 100],\n",
       "                                        'class_weight': ['balanced']},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': [1, 10, 50, 100], 'class_weight': ['balanced']}\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "data, label = read_dataset('MatbII', 'ECG_EDA_Features_Combined_scld', 'ECG_EDA_Base2_Features_Combined')\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.20, random_state=42, shuffle=True)\n",
    "\n",
    "svc_ = SVC()\n",
    "rf_random = RandomizedSearchCV(estimator = svc_, param_distributions = parameters, n_iter = 50, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced', 'C': 10}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       459\n",
      "           1       0.69      0.73      0.71       391\n",
      "\n",
      "    accuracy                           0.72       850\n",
      "   macro avg       0.72      0.72      0.72       850\n",
      "weighted avg       0.73      0.72      0.73       850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = rf_random.best_estimator_\n",
    "hist = model.fit(X_train, y_train)\n",
    "predicted_y = model.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anubhav\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 4 is smaller than n_iter=50. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=SVC(), n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'C': [1, 10, 50, 100],\n",
       "                                        'class_weight': ['balanced']},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': [1, 10, 50, 100], 'class_weight': ['balanced']}\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "data, label = read_dataset('Virage', 'ECG_EDA_Features_Combined_scld', 'ECG_EDA_Base2_Features_Combined')\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.20, random_state=42, shuffle=True)\n",
    "\n",
    "svc_ = SVC()\n",
    "rf_random = RandomizedSearchCV(estimator = svc_, param_distributions = parameters, n_iter = 50, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced', 'C': 10}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87       636\n",
      "           1       0.80      0.78      0.79       407\n",
      "\n",
      "    accuracy                           0.84      1043\n",
      "   macro avg       0.83      0.83      0.83      1043\n",
      "weighted avg       0.84      0.84      0.84      1043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = rf_random.best_estimator_\n",
    "hist = model.fit(X_train, y_train)\n",
    "predicted_y = model.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anubhav\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 16 is smaller than n_iter=100. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=LogisticRegression(), n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'C': [0.5, 1, 5, 10],\n",
       "                                        'max_iter': [400, 500, 600, 1000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data, label = read_dataset('MatbII', 'ECG_EDA_Features_Combined_scld', 'ECG_EDA_Base2_Features_Combined')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.20, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "parameters_lr = {'C': [0.5, 1, 5, 10], 'max_iter': [400, 500, 600, 1000]}\n",
    "\n",
    "lr_ = LogisticRegression()\n",
    "rf_random = RandomizedSearchCV(estimator = lr_, param_distributions = parameters_lr, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_iter': 400, 'C': 1}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.70      0.69       459\n",
      "           1       0.63      0.60      0.62       391\n",
      "\n",
      "    accuracy                           0.66       850\n",
      "   macro avg       0.65      0.65      0.65       850\n",
      "weighted avg       0.65      0.66      0.65       850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = rf_random.best_estimator_\n",
    "hist = model.fit(X_train, y_train)\n",
    "predicted_y = model.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anubhav\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 16 is smaller than n_iter=100. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=LogisticRegression(), n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'C': [0.5, 1, 5, 10],\n",
       "                                        'max_iter': [400, 500, 600, 1000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data, label = read_dataset('Virage', 'ECG_EDA_Features_Combined_scld', 'ECG_EDA_Base2_Features_Combined')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.20, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "parameters_lr = {'C': [0.5, 1, 5, 10], 'max_iter': [400, 500, 600, 1000]}\n",
    "\n",
    "lr_ = LogisticRegression()\n",
    "rf_random = RandomizedSearchCV(estimator = lr_, param_distributions = parameters_lr, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_iter': 400, 'C': 0.5}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83       636\n",
      "           1       0.80      0.57      0.67       407\n",
      "\n",
      "    accuracy                           0.78      1043\n",
      "   macro avg       0.78      0.74      0.75      1043\n",
      "weighted avg       0.78      0.78      0.77      1043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = rf_random.best_estimator_\n",
    "hist = model.fit(X_train, y_train)\n",
    "predicted_y = model.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=LGBMClassifier(), n_jobs=-1,\n",
       "                   param_distributions={'boosting_type': ['gbdt', 'dart'],\n",
       "                                        'importance_type': ['split', 'gains'],\n",
       "                                        'learning_rate': [0.001],\n",
       "                                        'n_estimators': [1000, 2000],\n",
       "                                        'num_leaves': [10, 50, 100]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = read_dataset('MatbII', 'ECG_EDA_Features_Combined_scld', 'ECG_EDA_Base2_Features_Combined')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.20, random_state=42, shuffle=True)\n",
    "\n",
    "import lightgbm as lgb\n",
    "lgbmcls = lgb.LGBMClassifier()\n",
    "default_params = lgbmcls.get_params()\n",
    "\n",
    "params = {'boosting_type': ['gbdt', 'dart'],\n",
    " 'importance_type': ['split', 'gains'],\n",
    " 'learning_rate': [0.001],\n",
    " 'n_estimators': [1000, 2000],\n",
    " 'num_leaves': [10, 50, 100]} # , 'reg_lambda': [0.01, 0.001]\n",
    "\n",
    "lgb_ = lgb.LGBMClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = lgb_, param_distributions = params, n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'importance_type': 'split',\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': -1,\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 0.001,\n",
       " 'min_split_gain': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'num_leaves': 31,\n",
       " 'objective': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 0.0,\n",
       " 'silent': True,\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample_freq': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 100,\n",
       " 'n_estimators': 2000,\n",
       " 'learning_rate': 0.001,\n",
       " 'importance_type': 'gains',\n",
       " 'boosting_type': 'gbdt'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79       459\n",
      "           1       0.76      0.71      0.74       391\n",
      "\n",
      "    accuracy                           0.77       850\n",
      "   macro avg       0.77      0.76      0.76       850\n",
      "weighted avg       0.77      0.77      0.77       850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = rf_random.best_estimator_\n",
    "hist = model.fit(X_train, y_train)\n",
    "predicted_y = model.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=LGBMClassifier(), n_jobs=-1,\n",
       "                   param_distributions={'boosting_type': ['gbdt', 'dart'],\n",
       "                                        'importance_type': ['split', 'gains'],\n",
       "                                        'learning_rate': [0.001],\n",
       "                                        'n_estimators': [1000, 2000],\n",
       "                                        'num_leaves': [10, 50, 100]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = read_dataset('Virage', 'ECG_EDA_Features_Combined_scld', 'ECG_EDA_Base2_Features_Combined')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.20, random_state=42, shuffle=True)\n",
    "\n",
    "import lightgbm as lgb\n",
    "lgbmcls = lgb.LGBMClassifier()\n",
    "default_params = lgbmcls.get_params()\n",
    "\n",
    "params = {'boosting_type': ['gbdt', 'dart'],\n",
    " 'importance_type': ['split', 'gains'],\n",
    " 'learning_rate': [0.001],\n",
    " 'n_estimators': [1000, 2000],\n",
    " 'num_leaves': [10, 50, 100]} # , 'reg_lambda': [0.01, 0.001]\n",
    "\n",
    "lgb_ = lgb.LGBMClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = lgb_, param_distributions = params, n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 100,\n",
       " 'n_estimators': 2000,\n",
       " 'learning_rate': 0.001,\n",
       " 'importance_type': 'gains',\n",
       " 'boosting_type': 'gbdt'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       636\n",
      "           1       0.86      0.82      0.84       407\n",
      "\n",
      "    accuracy                           0.88      1043\n",
      "   macro avg       0.87      0.87      0.87      1043\n",
      "weighted avg       0.88      0.88      0.88      1043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = rf_random.best_estimator_\n",
    "hist = model.fit(X_train, y_train)\n",
    "predicted_y = model.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17747\n",
      "[LightGBM] [Info] Number of data points in the train set: 5224, number of used features: 70\n",
      "[LightGBM] [Info] Start training from score -1.062703\n",
      "[LightGBM] [Info] Start training from score -1.098804\n",
      "[LightGBM] [Info] Start training from score -1.135661\n",
      "[1]\ttraining's multi_logloss: 1.09439\tvalid_1's multi_logloss: 1.09528\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's multi_logloss: 1.09041\tvalid_1's multi_logloss: 1.09264\n",
      "[3]\ttraining's multi_logloss: 1.08675\tvalid_1's multi_logloss: 1.09011\n",
      "[4]\ttraining's multi_logloss: 1.08293\tvalid_1's multi_logloss: 1.08743\n",
      "[5]\ttraining's multi_logloss: 1.0792\tvalid_1's multi_logloss: 1.08488\n",
      "[6]\ttraining's multi_logloss: 1.07528\tvalid_1's multi_logloss: 1.08219\n",
      "[7]\ttraining's multi_logloss: 1.07161\tvalid_1's multi_logloss: 1.07956\n",
      "[8]\ttraining's multi_logloss: 1.06801\tvalid_1's multi_logloss: 1.07704\n",
      "[9]\ttraining's multi_logloss: 1.06442\tvalid_1's multi_logloss: 1.07452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\ttraining's multi_logloss: 1.06106\tvalid_1's multi_logloss: 1.07221\n",
      "[11]\ttraining's multi_logloss: 1.05738\tvalid_1's multi_logloss: 1.06975\n",
      "[12]\ttraining's multi_logloss: 1.054\tvalid_1's multi_logloss: 1.06753\n",
      "[13]\ttraining's multi_logloss: 1.05065\tvalid_1's multi_logloss: 1.06519\n",
      "[14]\ttraining's multi_logloss: 1.04741\tvalid_1's multi_logloss: 1.06304\n",
      "[15]\ttraining's multi_logloss: 1.04416\tvalid_1's multi_logloss: 1.06086\n",
      "[16]\ttraining's multi_logloss: 1.04097\tvalid_1's multi_logloss: 1.05858\n",
      "[17]\ttraining's multi_logloss: 1.0375\tvalid_1's multi_logloss: 1.05627\n",
      "[18]\ttraining's multi_logloss: 1.03422\tvalid_1's multi_logloss: 1.05404\n",
      "[19]\ttraining's multi_logloss: 1.03088\tvalid_1's multi_logloss: 1.05174\n",
      "[20]\ttraining's multi_logloss: 1.02771\tvalid_1's multi_logloss: 1.04961\n",
      "[21]\ttraining's multi_logloss: 1.02458\tvalid_1's multi_logloss: 1.04751\n",
      "[22]\ttraining's multi_logloss: 1.02112\tvalid_1's multi_logloss: 1.04539\n",
      "[23]\ttraining's multi_logloss: 1.01773\tvalid_1's multi_logloss: 1.04344\n",
      "[24]\ttraining's multi_logloss: 1.01434\tvalid_1's multi_logloss: 1.04149\n",
      "[25]\ttraining's multi_logloss: 1.01123\tvalid_1's multi_logloss: 1.03924\n",
      "[26]\ttraining's multi_logloss: 1.00793\tvalid_1's multi_logloss: 1.03728\n",
      "[27]\ttraining's multi_logloss: 1.00459\tvalid_1's multi_logloss: 1.03536\n",
      "[28]\ttraining's multi_logloss: 1.00156\tvalid_1's multi_logloss: 1.03333\n",
      "[29]\ttraining's multi_logloss: 0.998357\tvalid_1's multi_logloss: 1.03153\n",
      "[30]\ttraining's multi_logloss: 0.995116\tvalid_1's multi_logloss: 1.02963\n",
      "[31]\ttraining's multi_logloss: 0.992105\tvalid_1's multi_logloss: 1.02748\n",
      "[32]\ttraining's multi_logloss: 0.988945\tvalid_1's multi_logloss: 1.02568\n",
      "[33]\ttraining's multi_logloss: 0.985972\tvalid_1's multi_logloss: 1.02381\n",
      "[34]\ttraining's multi_logloss: 0.982777\tvalid_1's multi_logloss: 1.02197\n",
      "[35]\ttraining's multi_logloss: 0.97992\tvalid_1's multi_logloss: 1.02021\n",
      "[36]\ttraining's multi_logloss: 0.976743\tvalid_1's multi_logloss: 1.01824\n",
      "[37]\ttraining's multi_logloss: 0.973625\tvalid_1's multi_logloss: 1.01638\n",
      "[38]\ttraining's multi_logloss: 0.970478\tvalid_1's multi_logloss: 1.01446\n",
      "[39]\ttraining's multi_logloss: 0.96738\tvalid_1's multi_logloss: 1.01275\n",
      "[40]\ttraining's multi_logloss: 0.964237\tvalid_1's multi_logloss: 1.01095\n",
      "[41]\ttraining's multi_logloss: 0.961469\tvalid_1's multi_logloss: 1.00919\n",
      "[42]\ttraining's multi_logloss: 0.95842\tvalid_1's multi_logloss: 1.00751\n",
      "[43]\ttraining's multi_logloss: 0.955442\tvalid_1's multi_logloss: 1.00581\n",
      "[44]\ttraining's multi_logloss: 0.952395\tvalid_1's multi_logloss: 1.00408\n",
      "[45]\ttraining's multi_logloss: 0.949685\tvalid_1's multi_logloss: 1.0023\n",
      "[46]\ttraining's multi_logloss: 0.946715\tvalid_1's multi_logloss: 1.00049\n",
      "[47]\ttraining's multi_logloss: 0.943949\tvalid_1's multi_logloss: 0.998623\n",
      "[48]\ttraining's multi_logloss: 0.940979\tvalid_1's multi_logloss: 0.996839\n",
      "[49]\ttraining's multi_logloss: 0.938058\tvalid_1's multi_logloss: 0.995153\n",
      "[50]\ttraining's multi_logloss: 0.935438\tvalid_1's multi_logloss: 0.993361\n",
      "[51]\ttraining's multi_logloss: 0.932538\tvalid_1's multi_logloss: 0.991733\n",
      "[52]\ttraining's multi_logloss: 0.929894\tvalid_1's multi_logloss: 0.98991\n",
      "[53]\ttraining's multi_logloss: 0.927087\tvalid_1's multi_logloss: 0.988286\n",
      "[54]\ttraining's multi_logloss: 0.924307\tvalid_1's multi_logloss: 0.986888\n",
      "[55]\ttraining's multi_logloss: 0.921656\tvalid_1's multi_logloss: 0.985148\n",
      "[56]\ttraining's multi_logloss: 0.918865\tvalid_1's multi_logloss: 0.983478\n",
      "[57]\ttraining's multi_logloss: 0.916085\tvalid_1's multi_logloss: 0.981818\n",
      "[58]\ttraining's multi_logloss: 0.913556\tvalid_1's multi_logloss: 0.980068\n",
      "[59]\ttraining's multi_logloss: 0.910776\tvalid_1's multi_logloss: 0.978547\n",
      "[60]\ttraining's multi_logloss: 0.908008\tvalid_1's multi_logloss: 0.976864\n",
      "[61]\ttraining's multi_logloss: 0.905507\tvalid_1's multi_logloss: 0.97522\n",
      "[62]\ttraining's multi_logloss: 0.902704\tvalid_1's multi_logloss: 0.973503\n",
      "[63]\ttraining's multi_logloss: 0.899996\tvalid_1's multi_logloss: 0.971974\n",
      "[64]\ttraining's multi_logloss: 0.897482\tvalid_1's multi_logloss: 0.970254\n",
      "[65]\ttraining's multi_logloss: 0.894752\tvalid_1's multi_logloss: 0.968639\n",
      "[66]\ttraining's multi_logloss: 0.892087\tvalid_1's multi_logloss: 0.967236\n",
      "[67]\ttraining's multi_logloss: 0.889405\tvalid_1's multi_logloss: 0.965733\n",
      "[68]\ttraining's multi_logloss: 0.886964\tvalid_1's multi_logloss: 0.964001\n",
      "[69]\ttraining's multi_logloss: 0.884512\tvalid_1's multi_logloss: 0.962476\n",
      "[70]\ttraining's multi_logloss: 0.881886\tvalid_1's multi_logloss: 0.960895\n",
      "[71]\ttraining's multi_logloss: 0.879495\tvalid_1's multi_logloss: 0.959356\n",
      "[72]\ttraining's multi_logloss: 0.876842\tvalid_1's multi_logloss: 0.957742\n",
      "[73]\ttraining's multi_logloss: 0.874228\tvalid_1's multi_logloss: 0.956208\n",
      "[74]\ttraining's multi_logloss: 0.87188\tvalid_1's multi_logloss: 0.95468\n",
      "[75]\ttraining's multi_logloss: 0.869433\tvalid_1's multi_logloss: 0.953156\n",
      "[76]\ttraining's multi_logloss: 0.867101\tvalid_1's multi_logloss: 0.951667\n",
      "[77]\ttraining's multi_logloss: 0.864622\tvalid_1's multi_logloss: 0.950218\n",
      "[78]\ttraining's multi_logloss: 0.861983\tvalid_1's multi_logloss: 0.948865\n",
      "[79]\ttraining's multi_logloss: 0.859739\tvalid_1's multi_logloss: 0.947493\n",
      "[80]\ttraining's multi_logloss: 0.857224\tvalid_1's multi_logloss: 0.946054\n",
      "[81]\ttraining's multi_logloss: 0.854725\tvalid_1's multi_logloss: 0.944613\n",
      "[82]\ttraining's multi_logloss: 0.852235\tvalid_1's multi_logloss: 0.943273\n",
      "[83]\ttraining's multi_logloss: 0.849743\tvalid_1's multi_logloss: 0.941779\n",
      "[84]\ttraining's multi_logloss: 0.847244\tvalid_1's multi_logloss: 0.940235\n",
      "[85]\ttraining's multi_logloss: 0.845005\tvalid_1's multi_logloss: 0.938856\n",
      "[86]\ttraining's multi_logloss: 0.842621\tvalid_1's multi_logloss: 0.937457\n",
      "[87]\ttraining's multi_logloss: 0.840186\tvalid_1's multi_logloss: 0.936208\n",
      "[88]\ttraining's multi_logloss: 0.838078\tvalid_1's multi_logloss: 0.934979\n",
      "[89]\ttraining's multi_logloss: 0.835653\tvalid_1's multi_logloss: 0.933615\n",
      "[90]\ttraining's multi_logloss: 0.833178\tvalid_1's multi_logloss: 0.932075\n",
      "[91]\ttraining's multi_logloss: 0.830797\tvalid_1's multi_logloss: 0.930765\n",
      "[92]\ttraining's multi_logloss: 0.828439\tvalid_1's multi_logloss: 0.929402\n",
      "[93]\ttraining's multi_logloss: 0.826272\tvalid_1's multi_logloss: 0.927911\n",
      "[94]\ttraining's multi_logloss: 0.823927\tvalid_1's multi_logloss: 0.926594\n",
      "[95]\ttraining's multi_logloss: 0.821766\tvalid_1's multi_logloss: 0.925245\n",
      "[96]\ttraining's multi_logloss: 0.819499\tvalid_1's multi_logloss: 0.924045\n",
      "[97]\ttraining's multi_logloss: 0.817266\tvalid_1's multi_logloss: 0.922873\n",
      "[98]\ttraining's multi_logloss: 0.814924\tvalid_1's multi_logloss: 0.921502\n",
      "[99]\ttraining's multi_logloss: 0.812599\tvalid_1's multi_logloss: 0.920109\n",
      "[100]\ttraining's multi_logloss: 0.810245\tvalid_1's multi_logloss: 0.918738\n",
      "[101]\ttraining's multi_logloss: 0.808167\tvalid_1's multi_logloss: 0.917458\n",
      "[102]\ttraining's multi_logloss: 0.805997\tvalid_1's multi_logloss: 0.916349\n",
      "[103]\ttraining's multi_logloss: 0.803698\tvalid_1's multi_logloss: 0.915075\n",
      "[104]\ttraining's multi_logloss: 0.801492\tvalid_1's multi_logloss: 0.913934\n",
      "[105]\ttraining's multi_logloss: 0.799295\tvalid_1's multi_logloss: 0.912732\n",
      "[106]\ttraining's multi_logloss: 0.797032\tvalid_1's multi_logloss: 0.91144\n",
      "[107]\ttraining's multi_logloss: 0.794829\tvalid_1's multi_logloss: 0.910284\n",
      "[108]\ttraining's multi_logloss: 0.792591\tvalid_1's multi_logloss: 0.909052\n",
      "[109]\ttraining's multi_logloss: 0.790422\tvalid_1's multi_logloss: 0.907955\n",
      "[110]\ttraining's multi_logloss: 0.788291\tvalid_1's multi_logloss: 0.906903\n",
      "[111]\ttraining's multi_logloss: 0.786106\tvalid_1's multi_logloss: 0.905687\n",
      "[112]\ttraining's multi_logloss: 0.784041\tvalid_1's multi_logloss: 0.904653\n",
      "[113]\ttraining's multi_logloss: 0.781989\tvalid_1's multi_logloss: 0.903678\n",
      "[114]\ttraining's multi_logloss: 0.779855\tvalid_1's multi_logloss: 0.9025\n",
      "[115]\ttraining's multi_logloss: 0.777696\tvalid_1's multi_logloss: 0.901341\n",
      "[116]\ttraining's multi_logloss: 0.775573\tvalid_1's multi_logloss: 0.900175\n",
      "[117]\ttraining's multi_logloss: 0.773561\tvalid_1's multi_logloss: 0.899209\n",
      "[118]\ttraining's multi_logloss: 0.771518\tvalid_1's multi_logloss: 0.898117\n",
      "[119]\ttraining's multi_logloss: 0.769409\tvalid_1's multi_logloss: 0.89692\n",
      "[120]\ttraining's multi_logloss: 0.767359\tvalid_1's multi_logloss: 0.895929\n",
      "[121]\ttraining's multi_logloss: 0.765491\tvalid_1's multi_logloss: 0.894803\n",
      "[122]\ttraining's multi_logloss: 0.763426\tvalid_1's multi_logloss: 0.893681\n",
      "[123]\ttraining's multi_logloss: 0.761348\tvalid_1's multi_logloss: 0.892527\n",
      "[124]\ttraining's multi_logloss: 0.759293\tvalid_1's multi_logloss: 0.891424\n",
      "[125]\ttraining's multi_logloss: 0.757283\tvalid_1's multi_logloss: 0.890352\n",
      "[126]\ttraining's multi_logloss: 0.75522\tvalid_1's multi_logloss: 0.889279\n",
      "[127]\ttraining's multi_logloss: 0.753224\tvalid_1's multi_logloss: 0.888284\n",
      "[128]\ttraining's multi_logloss: 0.751349\tvalid_1's multi_logloss: 0.88708\n",
      "[129]\ttraining's multi_logloss: 0.749369\tvalid_1's multi_logloss: 0.88616\n",
      "[130]\ttraining's multi_logloss: 0.747523\tvalid_1's multi_logloss: 0.885115\n",
      "[131]\ttraining's multi_logloss: 0.745593\tvalid_1's multi_logloss: 0.883991\n",
      "[132]\ttraining's multi_logloss: 0.74364\tvalid_1's multi_logloss: 0.882907\n",
      "[133]\ttraining's multi_logloss: 0.741617\tvalid_1's multi_logloss: 0.881905\n",
      "[134]\ttraining's multi_logloss: 0.739683\tvalid_1's multi_logloss: 0.880921\n",
      "[135]\ttraining's multi_logloss: 0.737745\tvalid_1's multi_logloss: 0.879925\n",
      "[136]\ttraining's multi_logloss: 0.735812\tvalid_1's multi_logloss: 0.878927\n",
      "[137]\ttraining's multi_logloss: 0.733899\tvalid_1's multi_logloss: 0.877912\n",
      "[138]\ttraining's multi_logloss: 0.731976\tvalid_1's multi_logloss: 0.876946\n",
      "[139]\ttraining's multi_logloss: 0.730231\tvalid_1's multi_logloss: 0.875975\n",
      "[140]\ttraining's multi_logloss: 0.72826\tvalid_1's multi_logloss: 0.874994\n",
      "[141]\ttraining's multi_logloss: 0.726533\tvalid_1's multi_logloss: 0.873907\n",
      "[142]\ttraining's multi_logloss: 0.724734\tvalid_1's multi_logloss: 0.872818\n",
      "[143]\ttraining's multi_logloss: 0.722741\tvalid_1's multi_logloss: 0.871691\n",
      "[144]\ttraining's multi_logloss: 0.720808\tvalid_1's multi_logloss: 0.870681\n",
      "[145]\ttraining's multi_logloss: 0.718893\tvalid_1's multi_logloss: 0.869515\n",
      "[146]\ttraining's multi_logloss: 0.717085\tvalid_1's multi_logloss: 0.868568\n",
      "[147]\ttraining's multi_logloss: 0.715369\tvalid_1's multi_logloss: 0.867425\n",
      "[148]\ttraining's multi_logloss: 0.713544\tvalid_1's multi_logloss: 0.8666\n",
      "[149]\ttraining's multi_logloss: 0.711836\tvalid_1's multi_logloss: 0.865588\n",
      "[150]\ttraining's multi_logloss: 0.710064\tvalid_1's multi_logloss: 0.864782\n",
      "[151]\ttraining's multi_logloss: 0.708408\tvalid_1's multi_logloss: 0.863832\n",
      "[152]\ttraining's multi_logloss: 0.706479\tvalid_1's multi_logloss: 0.86262\n",
      "[153]\ttraining's multi_logloss: 0.704824\tvalid_1's multi_logloss: 0.861703\n",
      "[154]\ttraining's multi_logloss: 0.703072\tvalid_1's multi_logloss: 0.860849\n",
      "[155]\ttraining's multi_logloss: 0.70143\tvalid_1's multi_logloss: 0.859885\n",
      "[156]\ttraining's multi_logloss: 0.699683\tvalid_1's multi_logloss: 0.859034\n",
      "[157]\ttraining's multi_logloss: 0.697916\tvalid_1's multi_logloss: 0.858091\n",
      "[158]\ttraining's multi_logloss: 0.696262\tvalid_1's multi_logloss: 0.857018\n",
      "[159]\ttraining's multi_logloss: 0.694402\tvalid_1's multi_logloss: 0.856026\n",
      "[160]\ttraining's multi_logloss: 0.692704\tvalid_1's multi_logloss: 0.855092\n",
      "[161]\ttraining's multi_logloss: 0.69095\tvalid_1's multi_logloss: 0.854204\n",
      "[162]\ttraining's multi_logloss: 0.689361\tvalid_1's multi_logloss: 0.853251\n",
      "[163]\ttraining's multi_logloss: 0.687658\tvalid_1's multi_logloss: 0.852374\n",
      "[164]\ttraining's multi_logloss: 0.6859\tvalid_1's multi_logloss: 0.851505\n",
      "[165]\ttraining's multi_logloss: 0.684329\tvalid_1's multi_logloss: 0.85062\n",
      "[166]\ttraining's multi_logloss: 0.68283\tvalid_1's multi_logloss: 0.849786\n",
      "[167]\ttraining's multi_logloss: 0.681102\tvalid_1's multi_logloss: 0.848922\n",
      "[168]\ttraining's multi_logloss: 0.679559\tvalid_1's multi_logloss: 0.848026\n",
      "[169]\ttraining's multi_logloss: 0.677914\tvalid_1's multi_logloss: 0.847283\n",
      "[170]\ttraining's multi_logloss: 0.676302\tvalid_1's multi_logloss: 0.846312\n",
      "[171]\ttraining's multi_logloss: 0.674663\tvalid_1's multi_logloss: 0.845502\n",
      "[172]\ttraining's multi_logloss: 0.673104\tvalid_1's multi_logloss: 0.844646\n",
      "[173]\ttraining's multi_logloss: 0.671582\tvalid_1's multi_logloss: 0.843682\n",
      "[174]\ttraining's multi_logloss: 0.669992\tvalid_1's multi_logloss: 0.842818\n",
      "[175]\ttraining's multi_logloss: 0.668325\tvalid_1's multi_logloss: 0.841965\n",
      "[176]\ttraining's multi_logloss: 0.666677\tvalid_1's multi_logloss: 0.841123\n",
      "[177]\ttraining's multi_logloss: 0.665003\tvalid_1's multi_logloss: 0.840337\n",
      "[178]\ttraining's multi_logloss: 0.663513\tvalid_1's multi_logloss: 0.839529\n",
      "[179]\ttraining's multi_logloss: 0.661856\tvalid_1's multi_logloss: 0.838779\n",
      "[180]\ttraining's multi_logloss: 0.660367\tvalid_1's multi_logloss: 0.837997\n",
      "[181]\ttraining's multi_logloss: 0.65888\tvalid_1's multi_logloss: 0.837322\n",
      "[182]\ttraining's multi_logloss: 0.65721\tvalid_1's multi_logloss: 0.83646\n",
      "[183]\ttraining's multi_logloss: 0.655596\tvalid_1's multi_logloss: 0.835728\n",
      "[184]\ttraining's multi_logloss: 0.654148\tvalid_1's multi_logloss: 0.835033\n",
      "[185]\ttraining's multi_logloss: 0.652566\tvalid_1's multi_logloss: 0.834306\n",
      "[186]\ttraining's multi_logloss: 0.650962\tvalid_1's multi_logloss: 0.833663\n",
      "[187]\ttraining's multi_logloss: 0.649425\tvalid_1's multi_logloss: 0.832833\n",
      "[188]\ttraining's multi_logloss: 0.648001\tvalid_1's multi_logloss: 0.832142\n",
      "[189]\ttraining's multi_logloss: 0.646417\tvalid_1's multi_logloss: 0.831404\n",
      "[190]\ttraining's multi_logloss: 0.644873\tvalid_1's multi_logloss: 0.830631\n",
      "[191]\ttraining's multi_logloss: 0.643275\tvalid_1's multi_logloss: 0.829929\n",
      "[192]\ttraining's multi_logloss: 0.641734\tvalid_1's multi_logloss: 0.829316\n",
      "[193]\ttraining's multi_logloss: 0.640233\tvalid_1's multi_logloss: 0.828443\n",
      "[194]\ttraining's multi_logloss: 0.638812\tvalid_1's multi_logloss: 0.827595\n",
      "[195]\ttraining's multi_logloss: 0.637276\tvalid_1's multi_logloss: 0.826894\n",
      "[196]\ttraining's multi_logloss: 0.635716\tvalid_1's multi_logloss: 0.82621\n",
      "[197]\ttraining's multi_logloss: 0.634224\tvalid_1's multi_logloss: 0.825408\n",
      "[198]\ttraining's multi_logloss: 0.632766\tvalid_1's multi_logloss: 0.824714\n",
      "[199]\ttraining's multi_logloss: 0.631259\tvalid_1's multi_logloss: 0.824043\n",
      "[200]\ttraining's multi_logloss: 0.629756\tvalid_1's multi_logloss: 0.82344\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[201]\ttraining's multi_logloss: 0.628424\tvalid_1's multi_logloss: 0.822763\n",
      "[202]\ttraining's multi_logloss: 0.626917\tvalid_1's multi_logloss: 0.822131\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[203]\ttraining's multi_logloss: 0.625682\tvalid_1's multi_logloss: 0.821589\n",
      "[204]\ttraining's multi_logloss: 0.624147\tvalid_1's multi_logloss: 0.820932\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[205]\ttraining's multi_logloss: 0.622828\tvalid_1's multi_logloss: 0.820206\n",
      "[206]\ttraining's multi_logloss: 0.621357\tvalid_1's multi_logloss: 0.819532\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[207]\ttraining's multi_logloss: 0.620008\tvalid_1's multi_logloss: 0.818769\n",
      "[208]\ttraining's multi_logloss: 0.618554\tvalid_1's multi_logloss: 0.817986\n",
      "[209]\ttraining's multi_logloss: 0.617129\tvalid_1's multi_logloss: 0.817369\n",
      "[210]\ttraining's multi_logloss: 0.615717\tvalid_1's multi_logloss: 0.816632\n",
      "[211]\ttraining's multi_logloss: 0.61428\tvalid_1's multi_logloss: 0.815882\n",
      "[212]\ttraining's multi_logloss: 0.612834\tvalid_1's multi_logloss: 0.815284\n",
      "[213]\ttraining's multi_logloss: 0.611381\tvalid_1's multi_logloss: 0.814613\n",
      "[214]\ttraining's multi_logloss: 0.609822\tvalid_1's multi_logloss: 0.813958\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[215]\ttraining's multi_logloss: 0.608532\tvalid_1's multi_logloss: 0.813182\n",
      "[216]\ttraining's multi_logloss: 0.607109\tvalid_1's multi_logloss: 0.812513\n",
      "[217]\ttraining's multi_logloss: 0.605775\tvalid_1's multi_logloss: 0.811749\n",
      "[218]\ttraining's multi_logloss: 0.604373\tvalid_1's multi_logloss: 0.811134\n",
      "[219]\ttraining's multi_logloss: 0.602943\tvalid_1's multi_logloss: 0.810511\n",
      "[220]\ttraining's multi_logloss: 0.601547\tvalid_1's multi_logloss: 0.809932\n",
      "[221]\ttraining's multi_logloss: 0.600106\tvalid_1's multi_logloss: 0.809297\n",
      "[222]\ttraining's multi_logloss: 0.598865\tvalid_1's multi_logloss: 0.808652\n",
      "[223]\ttraining's multi_logloss: 0.597457\tvalid_1's multi_logloss: 0.807907\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[224]\ttraining's multi_logloss: 0.596283\tvalid_1's multi_logloss: 0.807328\n",
      "[225]\ttraining's multi_logloss: 0.594973\tvalid_1's multi_logloss: 0.806834\n",
      "[226]\ttraining's multi_logloss: 0.593714\tvalid_1's multi_logloss: 0.80609\n",
      "[227]\ttraining's multi_logloss: 0.59241\tvalid_1's multi_logloss: 0.805615\n",
      "[228]\ttraining's multi_logloss: 0.591004\tvalid_1's multi_logloss: 0.804921\n",
      "[229]\ttraining's multi_logloss: 0.589721\tvalid_1's multi_logloss: 0.804205\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[230]\ttraining's multi_logloss: 0.588544\tvalid_1's multi_logloss: 0.803526\n",
      "[231]\ttraining's multi_logloss: 0.587295\tvalid_1's multi_logloss: 0.802834\n",
      "[232]\ttraining's multi_logloss: 0.585916\tvalid_1's multi_logloss: 0.802239\n",
      "[233]\ttraining's multi_logloss: 0.584484\tvalid_1's multi_logloss: 0.801613\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[234]\ttraining's multi_logloss: 0.583334\tvalid_1's multi_logloss: 0.800961\n",
      "[235]\ttraining's multi_logloss: 0.582013\tvalid_1's multi_logloss: 0.800304\n",
      "[236]\ttraining's multi_logloss: 0.580562\tvalid_1's multi_logloss: 0.799653\n",
      "[237]\ttraining's multi_logloss: 0.579243\tvalid_1's multi_logloss: 0.799032\n",
      "[238]\ttraining's multi_logloss: 0.577946\tvalid_1's multi_logloss: 0.798482\n",
      "[239]\ttraining's multi_logloss: 0.576665\tvalid_1's multi_logloss: 0.797979\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[240]\ttraining's multi_logloss: 0.575489\tvalid_1's multi_logloss: 0.797364\n",
      "[241]\ttraining's multi_logloss: 0.574147\tvalid_1's multi_logloss: 0.796856\n",
      "[242]\ttraining's multi_logloss: 0.572913\tvalid_1's multi_logloss: 0.796271\n",
      "[243]\ttraining's multi_logloss: 0.571598\tvalid_1's multi_logloss: 0.795778\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[244]\ttraining's multi_logloss: 0.57049\tvalid_1's multi_logloss: 0.795183\n",
      "[245]\ttraining's multi_logloss: 0.569136\tvalid_1's multi_logloss: 0.794575\n",
      "[246]\ttraining's multi_logloss: 0.567814\tvalid_1's multi_logloss: 0.79402\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[247]\ttraining's multi_logloss: 0.566686\tvalid_1's multi_logloss: 0.793426\n",
      "[248]\ttraining's multi_logloss: 0.565397\tvalid_1's multi_logloss: 0.792916\n",
      "[249]\ttraining's multi_logloss: 0.564136\tvalid_1's multi_logloss: 0.792454\n",
      "[250]\ttraining's multi_logloss: 0.562944\tvalid_1's multi_logloss: 0.791861\n",
      "[251]\ttraining's multi_logloss: 0.561834\tvalid_1's multi_logloss: 0.791282\n",
      "[252]\ttraining's multi_logloss: 0.560626\tvalid_1's multi_logloss: 0.790735\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[253]\ttraining's multi_logloss: 0.559577\tvalid_1's multi_logloss: 0.790107\n",
      "[254]\ttraining's multi_logloss: 0.558355\tvalid_1's multi_logloss: 0.789514\n",
      "[255]\ttraining's multi_logloss: 0.557127\tvalid_1's multi_logloss: 0.788995\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[256]\ttraining's multi_logloss: 0.556012\tvalid_1's multi_logloss: 0.788393\n",
      "[257]\ttraining's multi_logloss: 0.554727\tvalid_1's multi_logloss: 0.787858\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[258]\ttraining's multi_logloss: 0.553641\tvalid_1's multi_logloss: 0.787352\n",
      "[259]\ttraining's multi_logloss: 0.552431\tvalid_1's multi_logloss: 0.786751\n",
      "[260]\ttraining's multi_logloss: 0.551202\tvalid_1's multi_logloss: 0.786171\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[261]\ttraining's multi_logloss: 0.550117\tvalid_1's multi_logloss: 0.785608\n",
      "[262]\ttraining's multi_logloss: 0.548809\tvalid_1's multi_logloss: 0.78497\n",
      "[263]\ttraining's multi_logloss: 0.547599\tvalid_1's multi_logloss: 0.784518\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[264]\ttraining's multi_logloss: 0.546543\tvalid_1's multi_logloss: 0.783962\n",
      "[265]\ttraining's multi_logloss: 0.545241\tvalid_1's multi_logloss: 0.783411\n",
      "[266]\ttraining's multi_logloss: 0.544111\tvalid_1's multi_logloss: 0.782899\n",
      "[267]\ttraining's multi_logloss: 0.542869\tvalid_1's multi_logloss: 0.782242\n",
      "[268]\ttraining's multi_logloss: 0.541701\tvalid_1's multi_logloss: 0.781776\n",
      "[269]\ttraining's multi_logloss: 0.540477\tvalid_1's multi_logloss: 0.781251\n",
      "[270]\ttraining's multi_logloss: 0.539394\tvalid_1's multi_logloss: 0.78067\n",
      "[271]\ttraining's multi_logloss: 0.538167\tvalid_1's multi_logloss: 0.780196\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[272]\ttraining's multi_logloss: 0.537142\tvalid_1's multi_logloss: 0.779621\n",
      "[273]\ttraining's multi_logloss: 0.536004\tvalid_1's multi_logloss: 0.77907\n",
      "[274]\ttraining's multi_logloss: 0.534771\tvalid_1's multi_logloss: 0.778598\n",
      "[275]\ttraining's multi_logloss: 0.533557\tvalid_1's multi_logloss: 0.778005\n",
      "[276]\ttraining's multi_logloss: 0.532328\tvalid_1's multi_logloss: 0.777448\n",
      "[277]\ttraining's multi_logloss: 0.531149\tvalid_1's multi_logloss: 0.776907\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[278]\ttraining's multi_logloss: 0.530111\tvalid_1's multi_logloss: 0.776371\n",
      "[279]\ttraining's multi_logloss: 0.5289\tvalid_1's multi_logloss: 0.775858\n",
      "[280]\ttraining's multi_logloss: 0.527772\tvalid_1's multi_logloss: 0.775259\n",
      "[281]\ttraining's multi_logloss: 0.526479\tvalid_1's multi_logloss: 0.774658\n",
      "[282]\ttraining's multi_logloss: 0.525414\tvalid_1's multi_logloss: 0.774147\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[283]\ttraining's multi_logloss: 0.524462\tvalid_1's multi_logloss: 0.773776\n",
      "[284]\ttraining's multi_logloss: 0.523255\tvalid_1's multi_logloss: 0.773171\n",
      "[285]\ttraining's multi_logloss: 0.522065\tvalid_1's multi_logloss: 0.7726\n",
      "[286]\ttraining's multi_logloss: 0.520939\tvalid_1's multi_logloss: 0.772023\n",
      "[287]\ttraining's multi_logloss: 0.519813\tvalid_1's multi_logloss: 0.77145\n",
      "[288]\ttraining's multi_logloss: 0.518656\tvalid_1's multi_logloss: 0.770868\n",
      "[289]\ttraining's multi_logloss: 0.517496\tvalid_1's multi_logloss: 0.770298\n",
      "[290]\ttraining's multi_logloss: 0.516357\tvalid_1's multi_logloss: 0.769694\n",
      "[291]\ttraining's multi_logloss: 0.515198\tvalid_1's multi_logloss: 0.769152\n",
      "[292]\ttraining's multi_logloss: 0.514065\tvalid_1's multi_logloss: 0.768541\n",
      "[293]\ttraining's multi_logloss: 0.513102\tvalid_1's multi_logloss: 0.768007\n",
      "[294]\ttraining's multi_logloss: 0.511931\tvalid_1's multi_logloss: 0.76754\n",
      "[295]\ttraining's multi_logloss: 0.510788\tvalid_1's multi_logloss: 0.766997\n",
      "[296]\ttraining's multi_logloss: 0.509661\tvalid_1's multi_logloss: 0.766422\n",
      "[297]\ttraining's multi_logloss: 0.508668\tvalid_1's multi_logloss: 0.765898\n",
      "[298]\ttraining's multi_logloss: 0.507575\tvalid_1's multi_logloss: 0.765491\n",
      "[299]\ttraining's multi_logloss: 0.506495\tvalid_1's multi_logloss: 0.765015\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\ttraining's multi_logloss: 0.505536\tvalid_1's multi_logloss: 0.764539\n",
      "[301]\ttraining's multi_logloss: 0.504404\tvalid_1's multi_logloss: 0.764001\n",
      "[302]\ttraining's multi_logloss: 0.503279\tvalid_1's multi_logloss: 0.763529\n",
      "[303]\ttraining's multi_logloss: 0.502233\tvalid_1's multi_logloss: 0.76308\n",
      "[304]\ttraining's multi_logloss: 0.501082\tvalid_1's multi_logloss: 0.762591\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[305]\ttraining's multi_logloss: 0.500187\tvalid_1's multi_logloss: 0.762142\n",
      "[306]\ttraining's multi_logloss: 0.499127\tvalid_1's multi_logloss: 0.761665\n",
      "[307]\ttraining's multi_logloss: 0.498014\tvalid_1's multi_logloss: 0.761167\n",
      "[308]\ttraining's multi_logloss: 0.496962\tvalid_1's multi_logloss: 0.760717\n",
      "[309]\ttraining's multi_logloss: 0.495854\tvalid_1's multi_logloss: 0.760128\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[310]\ttraining's multi_logloss: 0.494943\tvalid_1's multi_logloss: 0.759704\n",
      "[311]\ttraining's multi_logloss: 0.493924\tvalid_1's multi_logloss: 0.759198\n",
      "[312]\ttraining's multi_logloss: 0.49287\tvalid_1's multi_logloss: 0.758817\n",
      "[313]\ttraining's multi_logloss: 0.49181\tvalid_1's multi_logloss: 0.758364\n",
      "[314]\ttraining's multi_logloss: 0.490743\tvalid_1's multi_logloss: 0.757941\n",
      "[315]\ttraining's multi_logloss: 0.489674\tvalid_1's multi_logloss: 0.757531\n",
      "[316]\ttraining's multi_logloss: 0.488588\tvalid_1's multi_logloss: 0.757116\n",
      "[317]\ttraining's multi_logloss: 0.487424\tvalid_1's multi_logloss: 0.756637\n",
      "[318]\ttraining's multi_logloss: 0.486419\tvalid_1's multi_logloss: 0.756112\n",
      "[319]\ttraining's multi_logloss: 0.485294\tvalid_1's multi_logloss: 0.755655\n",
      "[320]\ttraining's multi_logloss: 0.484271\tvalid_1's multi_logloss: 0.755247\n",
      "[321]\ttraining's multi_logloss: 0.483243\tvalid_1's multi_logloss: 0.754818\n",
      "[322]\ttraining's multi_logloss: 0.482192\tvalid_1's multi_logloss: 0.754396\n",
      "[323]\ttraining's multi_logloss: 0.48109\tvalid_1's multi_logloss: 0.753973\n",
      "[324]\ttraining's multi_logloss: 0.48011\tvalid_1's multi_logloss: 0.75351\n",
      "[325]\ttraining's multi_logloss: 0.479092\tvalid_1's multi_logloss: 0.753004\n",
      "[326]\ttraining's multi_logloss: 0.477979\tvalid_1's multi_logloss: 0.752528\n",
      "[327]\ttraining's multi_logloss: 0.47696\tvalid_1's multi_logloss: 0.75211\n",
      "[328]\ttraining's multi_logloss: 0.475902\tvalid_1's multi_logloss: 0.75157\n",
      "[329]\ttraining's multi_logloss: 0.474826\tvalid_1's multi_logloss: 0.751031\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[330]\ttraining's multi_logloss: 0.473891\tvalid_1's multi_logloss: 0.75054\n",
      "[331]\ttraining's multi_logloss: 0.472788\tvalid_1's multi_logloss: 0.750035\n",
      "[332]\ttraining's multi_logloss: 0.47169\tvalid_1's multi_logloss: 0.749571\n",
      "[333]\ttraining's multi_logloss: 0.47058\tvalid_1's multi_logloss: 0.749043\n",
      "[334]\ttraining's multi_logloss: 0.469477\tvalid_1's multi_logloss: 0.748527\n",
      "[335]\ttraining's multi_logloss: 0.468487\tvalid_1's multi_logloss: 0.748179\n",
      "[336]\ttraining's multi_logloss: 0.467486\tvalid_1's multi_logloss: 0.74759\n",
      "[337]\ttraining's multi_logloss: 0.466401\tvalid_1's multi_logloss: 0.747053\n",
      "[338]\ttraining's multi_logloss: 0.465464\tvalid_1's multi_logloss: 0.746541\n",
      "[339]\ttraining's multi_logloss: 0.464391\tvalid_1's multi_logloss: 0.746038\n",
      "[340]\ttraining's multi_logloss: 0.463305\tvalid_1's multi_logloss: 0.745628\n",
      "[341]\ttraining's multi_logloss: 0.462288\tvalid_1's multi_logloss: 0.745264\n",
      "[342]\ttraining's multi_logloss: 0.461257\tvalid_1's multi_logloss: 0.744884\n",
      "[343]\ttraining's multi_logloss: 0.460261\tvalid_1's multi_logloss: 0.744449\n",
      "[344]\ttraining's multi_logloss: 0.459318\tvalid_1's multi_logloss: 0.743912\n",
      "[345]\ttraining's multi_logloss: 0.4583\tvalid_1's multi_logloss: 0.743488\n",
      "[346]\ttraining's multi_logloss: 0.457237\tvalid_1's multi_logloss: 0.743098\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[347]\ttraining's multi_logloss: 0.456322\tvalid_1's multi_logloss: 0.742572\n",
      "[348]\ttraining's multi_logloss: 0.455323\tvalid_1's multi_logloss: 0.742196\n",
      "[349]\ttraining's multi_logloss: 0.454269\tvalid_1's multi_logloss: 0.741691\n",
      "[350]\ttraining's multi_logloss: 0.453367\tvalid_1's multi_logloss: 0.741322\n",
      "[351]\ttraining's multi_logloss: 0.452402\tvalid_1's multi_logloss: 0.74097\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[352]\ttraining's multi_logloss: 0.451572\tvalid_1's multi_logloss: 0.740539\n",
      "[353]\ttraining's multi_logloss: 0.450578\tvalid_1's multi_logloss: 0.740091\n",
      "[354]\ttraining's multi_logloss: 0.449561\tvalid_1's multi_logloss: 0.739657\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[355]\ttraining's multi_logloss: 0.448677\tvalid_1's multi_logloss: 0.739198\n",
      "[356]\ttraining's multi_logloss: 0.447684\tvalid_1's multi_logloss: 0.73876\n",
      "[357]\ttraining's multi_logloss: 0.446752\tvalid_1's multi_logloss: 0.738404\n",
      "[358]\ttraining's multi_logloss: 0.445782\tvalid_1's multi_logloss: 0.738013\n",
      "[359]\ttraining's multi_logloss: 0.444784\tvalid_1's multi_logloss: 0.737579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[360]\ttraining's multi_logloss: 0.443886\tvalid_1's multi_logloss: 0.737021\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[361]\ttraining's multi_logloss: 0.443143\tvalid_1's multi_logloss: 0.736602\n",
      "[362]\ttraining's multi_logloss: 0.442118\tvalid_1's multi_logloss: 0.736083\n",
      "[363]\ttraining's multi_logloss: 0.441132\tvalid_1's multi_logloss: 0.735723\n",
      "[364]\ttraining's multi_logloss: 0.440132\tvalid_1's multi_logloss: 0.735265\n",
      "[365]\ttraining's multi_logloss: 0.439124\tvalid_1's multi_logloss: 0.734838\n",
      "[366]\ttraining's multi_logloss: 0.438161\tvalid_1's multi_logloss: 0.734471\n",
      "[367]\ttraining's multi_logloss: 0.437185\tvalid_1's multi_logloss: 0.73405\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[368]\ttraining's multi_logloss: 0.436371\tvalid_1's multi_logloss: 0.733553\n",
      "[369]\ttraining's multi_logloss: 0.435418\tvalid_1's multi_logloss: 0.733168\n",
      "[370]\ttraining's multi_logloss: 0.434528\tvalid_1's multi_logloss: 0.732859\n",
      "[371]\ttraining's multi_logloss: 0.433563\tvalid_1's multi_logloss: 0.732458\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[372]\ttraining's multi_logloss: 0.432837\tvalid_1's multi_logloss: 0.73211\n",
      "[373]\ttraining's multi_logloss: 0.431946\tvalid_1's multi_logloss: 0.731798\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[374]\ttraining's multi_logloss: 0.431128\tvalid_1's multi_logloss: 0.73132\n",
      "[375]\ttraining's multi_logloss: 0.430222\tvalid_1's multi_logloss: 0.730977\n",
      "[376]\ttraining's multi_logloss: 0.429305\tvalid_1's multi_logloss: 0.730546\n",
      "[377]\ttraining's multi_logloss: 0.428379\tvalid_1's multi_logloss: 0.730253\n",
      "[378]\ttraining's multi_logloss: 0.427458\tvalid_1's multi_logloss: 0.729786\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[379]\ttraining's multi_logloss: 0.426661\tvalid_1's multi_logloss: 0.729355\n",
      "[380]\ttraining's multi_logloss: 0.42577\tvalid_1's multi_logloss: 0.729016\n",
      "[381]\ttraining's multi_logloss: 0.424928\tvalid_1's multi_logloss: 0.728623\n",
      "[382]\ttraining's multi_logloss: 0.424064\tvalid_1's multi_logloss: 0.72835\n",
      "[383]\ttraining's multi_logloss: 0.423147\tvalid_1's multi_logloss: 0.727995\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[384]\ttraining's multi_logloss: 0.422379\tvalid_1's multi_logloss: 0.727619\n",
      "[385]\ttraining's multi_logloss: 0.421481\tvalid_1's multi_logloss: 0.727313\n",
      "[386]\ttraining's multi_logloss: 0.420655\tvalid_1's multi_logloss: 0.726924\n",
      "[387]\ttraining's multi_logloss: 0.41978\tvalid_1's multi_logloss: 0.726565\n",
      "[388]\ttraining's multi_logloss: 0.418853\tvalid_1's multi_logloss: 0.726223\n",
      "[389]\ttraining's multi_logloss: 0.417958\tvalid_1's multi_logloss: 0.725922\n",
      "[390]\ttraining's multi_logloss: 0.417082\tvalid_1's multi_logloss: 0.725586\n",
      "[391]\ttraining's multi_logloss: 0.416191\tvalid_1's multi_logloss: 0.725192\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[392]\ttraining's multi_logloss: 0.415459\tvalid_1's multi_logloss: 0.724911\n",
      "[393]\ttraining's multi_logloss: 0.414651\tvalid_1's multi_logloss: 0.724571\n",
      "[394]\ttraining's multi_logloss: 0.413793\tvalid_1's multi_logloss: 0.724213\n",
      "[395]\ttraining's multi_logloss: 0.412888\tvalid_1's multi_logloss: 0.723852\n",
      "[396]\ttraining's multi_logloss: 0.411978\tvalid_1's multi_logloss: 0.723438\n",
      "[397]\ttraining's multi_logloss: 0.411142\tvalid_1's multi_logloss: 0.723181\n",
      "[398]\ttraining's multi_logloss: 0.410279\tvalid_1's multi_logloss: 0.72289\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[399]\ttraining's multi_logloss: 0.4095\tvalid_1's multi_logloss: 0.72255\n",
      "[400]\ttraining's multi_logloss: 0.408722\tvalid_1's multi_logloss: 0.722268\n",
      "[401]\ttraining's multi_logloss: 0.407913\tvalid_1's multi_logloss: 0.722018\n",
      "[402]\ttraining's multi_logloss: 0.407087\tvalid_1's multi_logloss: 0.721685\n",
      "[403]\ttraining's multi_logloss: 0.406215\tvalid_1's multi_logloss: 0.721372\n",
      "[404]\ttraining's multi_logloss: 0.405379\tvalid_1's multi_logloss: 0.721026\n",
      "[405]\ttraining's multi_logloss: 0.404583\tvalid_1's multi_logloss: 0.72071\n",
      "[406]\ttraining's multi_logloss: 0.403738\tvalid_1's multi_logloss: 0.720398\n",
      "[407]\ttraining's multi_logloss: 0.402914\tvalid_1's multi_logloss: 0.720159\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[408]\ttraining's multi_logloss: 0.402187\tvalid_1's multi_logloss: 0.719809\n",
      "[409]\ttraining's multi_logloss: 0.401364\tvalid_1's multi_logloss: 0.719477\n",
      "[410]\ttraining's multi_logloss: 0.400622\tvalid_1's multi_logloss: 0.719164\n",
      "[411]\ttraining's multi_logloss: 0.399789\tvalid_1's multi_logloss: 0.718846\n",
      "[412]\ttraining's multi_logloss: 0.399017\tvalid_1's multi_logloss: 0.718575\n",
      "[413]\ttraining's multi_logloss: 0.398183\tvalid_1's multi_logloss: 0.71827\n",
      "[414]\ttraining's multi_logloss: 0.397282\tvalid_1's multi_logloss: 0.717915\n",
      "[415]\ttraining's multi_logloss: 0.39647\tvalid_1's multi_logloss: 0.717649\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[416]\ttraining's multi_logloss: 0.395775\tvalid_1's multi_logloss: 0.717489\n",
      "[417]\ttraining's multi_logloss: 0.394955\tvalid_1's multi_logloss: 0.717348\n",
      "[418]\ttraining's multi_logloss: 0.394157\tvalid_1's multi_logloss: 0.717153\n",
      "[419]\ttraining's multi_logloss: 0.393314\tvalid_1's multi_logloss: 0.716851\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[420]\ttraining's multi_logloss: 0.392579\tvalid_1's multi_logloss: 0.716494\n",
      "[421]\ttraining's multi_logloss: 0.391778\tvalid_1's multi_logloss: 0.716281\n",
      "[422]\ttraining's multi_logloss: 0.390967\tvalid_1's multi_logloss: 0.715996\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[423]\ttraining's multi_logloss: 0.390262\tvalid_1's multi_logloss: 0.71565\n",
      "[424]\ttraining's multi_logloss: 0.389448\tvalid_1's multi_logloss: 0.715291\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[425]\ttraining's multi_logloss: 0.388738\tvalid_1's multi_logloss: 0.714946\n",
      "[426]\ttraining's multi_logloss: 0.387939\tvalid_1's multi_logloss: 0.714751\n",
      "[427]\ttraining's multi_logloss: 0.387151\tvalid_1's multi_logloss: 0.714478\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[428]\ttraining's multi_logloss: 0.386455\tvalid_1's multi_logloss: 0.71415\n",
      "[429]\ttraining's multi_logloss: 0.385762\tvalid_1's multi_logloss: 0.713798\n",
      "[430]\ttraining's multi_logloss: 0.384927\tvalid_1's multi_logloss: 0.713483\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[431]\ttraining's multi_logloss: 0.384232\tvalid_1's multi_logloss: 0.713193\n",
      "[432]\ttraining's multi_logloss: 0.383482\tvalid_1's multi_logloss: 0.712874\n",
      "[433]\ttraining's multi_logloss: 0.382722\tvalid_1's multi_logloss: 0.712719\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[434]\ttraining's multi_logloss: 0.382031\tvalid_1's multi_logloss: 0.712347\n",
      "[435]\ttraining's multi_logloss: 0.381192\tvalid_1's multi_logloss: 0.71199\n",
      "[436]\ttraining's multi_logloss: 0.380363\tvalid_1's multi_logloss: 0.711607\n",
      "[437]\ttraining's multi_logloss: 0.37955\tvalid_1's multi_logloss: 0.71133\n",
      "[438]\ttraining's multi_logloss: 0.378779\tvalid_1's multi_logloss: 0.711148\n",
      "[439]\ttraining's multi_logloss: 0.378016\tvalid_1's multi_logloss: 0.71099\n",
      "[440]\ttraining's multi_logloss: 0.377193\tvalid_1's multi_logloss: 0.710674\n",
      "[441]\ttraining's multi_logloss: 0.376384\tvalid_1's multi_logloss: 0.710401\n",
      "[442]\ttraining's multi_logloss: 0.375599\tvalid_1's multi_logloss: 0.71005\n",
      "[443]\ttraining's multi_logloss: 0.374804\tvalid_1's multi_logloss: 0.709723\n",
      "[444]\ttraining's multi_logloss: 0.37409\tvalid_1's multi_logloss: 0.709392\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[445]\ttraining's multi_logloss: 0.37346\tvalid_1's multi_logloss: 0.709186\n",
      "[446]\ttraining's multi_logloss: 0.372677\tvalid_1's multi_logloss: 0.708904\n",
      "[447]\ttraining's multi_logloss: 0.37192\tvalid_1's multi_logloss: 0.708565\n",
      "[448]\ttraining's multi_logloss: 0.371108\tvalid_1's multi_logloss: 0.708271\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[449]\ttraining's multi_logloss: 0.370477\tvalid_1's multi_logloss: 0.708056\n",
      "[450]\ttraining's multi_logloss: 0.369776\tvalid_1's multi_logloss: 0.707655\n",
      "[451]\ttraining's multi_logloss: 0.368993\tvalid_1's multi_logloss: 0.707357\n",
      "[452]\ttraining's multi_logloss: 0.368234\tvalid_1's multi_logloss: 0.70711\n",
      "[453]\ttraining's multi_logloss: 0.367457\tvalid_1's multi_logloss: 0.706737\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[454]\ttraining's multi_logloss: 0.36684\tvalid_1's multi_logloss: 0.706562\n",
      "[455]\ttraining's multi_logloss: 0.366125\tvalid_1's multi_logloss: 0.70623\n",
      "[456]\ttraining's multi_logloss: 0.365372\tvalid_1's multi_logloss: 0.705959\n",
      "[457]\ttraining's multi_logloss: 0.364679\tvalid_1's multi_logloss: 0.705737\n",
      "[458]\ttraining's multi_logloss: 0.36391\tvalid_1's multi_logloss: 0.705405\n",
      "[459]\ttraining's multi_logloss: 0.363198\tvalid_1's multi_logloss: 0.705125\n",
      "[460]\ttraining's multi_logloss: 0.362508\tvalid_1's multi_logloss: 0.704897\n",
      "[461]\ttraining's multi_logloss: 0.361815\tvalid_1's multi_logloss: 0.704588\n",
      "[462]\ttraining's multi_logloss: 0.36106\tvalid_1's multi_logloss: 0.704309\n",
      "[463]\ttraining's multi_logloss: 0.360376\tvalid_1's multi_logloss: 0.704128\n",
      "[464]\ttraining's multi_logloss: 0.359659\tvalid_1's multi_logloss: 0.703909\n",
      "[465]\ttraining's multi_logloss: 0.358901\tvalid_1's multi_logloss: 0.703686\n",
      "[466]\ttraining's multi_logloss: 0.358149\tvalid_1's multi_logloss: 0.703376\n",
      "[467]\ttraining's multi_logloss: 0.357396\tvalid_1's multi_logloss: 0.703082\n",
      "[468]\ttraining's multi_logloss: 0.35667\tvalid_1's multi_logloss: 0.702774\n",
      "[469]\ttraining's multi_logloss: 0.356025\tvalid_1's multi_logloss: 0.702513\n",
      "[470]\ttraining's multi_logloss: 0.35525\tvalid_1's multi_logloss: 0.702152\n",
      "[471]\ttraining's multi_logloss: 0.354594\tvalid_1's multi_logloss: 0.701884\n",
      "[472]\ttraining's multi_logloss: 0.353873\tvalid_1's multi_logloss: 0.701584\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[473]\ttraining's multi_logloss: 0.353261\tvalid_1's multi_logloss: 0.701333\n",
      "[474]\ttraining's multi_logloss: 0.352636\tvalid_1's multi_logloss: 0.701115\n",
      "[475]\ttraining's multi_logloss: 0.351936\tvalid_1's multi_logloss: 0.700837\n",
      "[476]\ttraining's multi_logloss: 0.351293\tvalid_1's multi_logloss: 0.700605\n",
      "[477]\ttraining's multi_logloss: 0.350578\tvalid_1's multi_logloss: 0.700311\n",
      "[478]\ttraining's multi_logloss: 0.349889\tvalid_1's multi_logloss: 0.700081\n",
      "[479]\ttraining's multi_logloss: 0.349184\tvalid_1's multi_logloss: 0.699861\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[480]\ttraining's multi_logloss: 0.348573\tvalid_1's multi_logloss: 0.699541\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[481]\ttraining's multi_logloss: 0.347985\tvalid_1's multi_logloss: 0.699331\n",
      "[482]\ttraining's multi_logloss: 0.347305\tvalid_1's multi_logloss: 0.699091\n",
      "[483]\ttraining's multi_logloss: 0.346665\tvalid_1's multi_logloss: 0.698857\n",
      "[484]\ttraining's multi_logloss: 0.345954\tvalid_1's multi_logloss: 0.698624\n",
      "[485]\ttraining's multi_logloss: 0.345272\tvalid_1's multi_logloss: 0.698343\n",
      "[486]\ttraining's multi_logloss: 0.344637\tvalid_1's multi_logloss: 0.698043\n",
      "[487]\ttraining's multi_logloss: 0.343945\tvalid_1's multi_logloss: 0.697845\n",
      "[488]\ttraining's multi_logloss: 0.343213\tvalid_1's multi_logloss: 0.697568\n",
      "[489]\ttraining's multi_logloss: 0.342459\tvalid_1's multi_logloss: 0.697247\n",
      "[490]\ttraining's multi_logloss: 0.341794\tvalid_1's multi_logloss: 0.697022\n",
      "[491]\ttraining's multi_logloss: 0.341203\tvalid_1's multi_logloss: 0.696713\n",
      "[492]\ttraining's multi_logloss: 0.340573\tvalid_1's multi_logloss: 0.696419\n",
      "[493]\ttraining's multi_logloss: 0.339949\tvalid_1's multi_logloss: 0.6962\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[494]\ttraining's multi_logloss: 0.339326\tvalid_1's multi_logloss: 0.695966\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[495]\ttraining's multi_logloss: 0.338747\tvalid_1's multi_logloss: 0.69572\n",
      "[496]\ttraining's multi_logloss: 0.338029\tvalid_1's multi_logloss: 0.69547\n",
      "[497]\ttraining's multi_logloss: 0.33737\tvalid_1's multi_logloss: 0.695308\n",
      "[498]\ttraining's multi_logloss: 0.336743\tvalid_1's multi_logloss: 0.695125\n",
      "[499]\ttraining's multi_logloss: 0.336078\tvalid_1's multi_logloss: 0.694925\n",
      "[500]\ttraining's multi_logloss: 0.335361\tvalid_1's multi_logloss: 0.694661\n",
      "[501]\ttraining's multi_logloss: 0.334663\tvalid_1's multi_logloss: 0.694374\n",
      "[502]\ttraining's multi_logloss: 0.333992\tvalid_1's multi_logloss: 0.694124\n",
      "[503]\ttraining's multi_logloss: 0.333437\tvalid_1's multi_logloss: 0.693835\n",
      "[504]\ttraining's multi_logloss: 0.332776\tvalid_1's multi_logloss: 0.693582\n",
      "[505]\ttraining's multi_logloss: 0.332117\tvalid_1's multi_logloss: 0.693336\n",
      "[506]\ttraining's multi_logloss: 0.331432\tvalid_1's multi_logloss: 0.693096\n",
      "[507]\ttraining's multi_logloss: 0.330782\tvalid_1's multi_logloss: 0.692934\n",
      "[508]\ttraining's multi_logloss: 0.33009\tvalid_1's multi_logloss: 0.692723\n",
      "[509]\ttraining's multi_logloss: 0.329397\tvalid_1's multi_logloss: 0.69255\n",
      "[510]\ttraining's multi_logloss: 0.328718\tvalid_1's multi_logloss: 0.692234\n",
      "[511]\ttraining's multi_logloss: 0.328089\tvalid_1's multi_logloss: 0.69203\n",
      "[512]\ttraining's multi_logloss: 0.32744\tvalid_1's multi_logloss: 0.691761\n",
      "[513]\ttraining's multi_logloss: 0.326773\tvalid_1's multi_logloss: 0.691491\n",
      "[514]\ttraining's multi_logloss: 0.326102\tvalid_1's multi_logloss: 0.691284\n",
      "[515]\ttraining's multi_logloss: 0.325437\tvalid_1's multi_logloss: 0.691057\n",
      "[516]\ttraining's multi_logloss: 0.32476\tvalid_1's multi_logloss: 0.690843\n",
      "[517]\ttraining's multi_logloss: 0.32405\tvalid_1's multi_logloss: 0.690528\n",
      "[518]\ttraining's multi_logloss: 0.323498\tvalid_1's multi_logloss: 0.690183\n",
      "[519]\ttraining's multi_logloss: 0.322829\tvalid_1's multi_logloss: 0.689963\n",
      "[520]\ttraining's multi_logloss: 0.3222\tvalid_1's multi_logloss: 0.689849\n",
      "[521]\ttraining's multi_logloss: 0.321533\tvalid_1's multi_logloss: 0.689624\n",
      "[522]\ttraining's multi_logloss: 0.320866\tvalid_1's multi_logloss: 0.689352\n",
      "[523]\ttraining's multi_logloss: 0.320214\tvalid_1's multi_logloss: 0.689139\n",
      "[524]\ttraining's multi_logloss: 0.319563\tvalid_1's multi_logloss: 0.688948\n",
      "[525]\ttraining's multi_logloss: 0.318905\tvalid_1's multi_logloss: 0.68873\n",
      "[526]\ttraining's multi_logloss: 0.318239\tvalid_1's multi_logloss: 0.688458\n",
      "[527]\ttraining's multi_logloss: 0.317577\tvalid_1's multi_logloss: 0.688174\n",
      "[528]\ttraining's multi_logloss: 0.316969\tvalid_1's multi_logloss: 0.687923\n",
      "[529]\ttraining's multi_logloss: 0.316334\tvalid_1's multi_logloss: 0.687755\n",
      "[530]\ttraining's multi_logloss: 0.315699\tvalid_1's multi_logloss: 0.687504\n",
      "[531]\ttraining's multi_logloss: 0.315073\tvalid_1's multi_logloss: 0.687221\n",
      "[532]\ttraining's multi_logloss: 0.314404\tvalid_1's multi_logloss: 0.686947\n",
      "[533]\ttraining's multi_logloss: 0.313753\tvalid_1's multi_logloss: 0.686749\n",
      "[534]\ttraining's multi_logloss: 0.313088\tvalid_1's multi_logloss: 0.68654\n",
      "[535]\ttraining's multi_logloss: 0.312529\tvalid_1's multi_logloss: 0.686318\n",
      "[536]\ttraining's multi_logloss: 0.3119\tvalid_1's multi_logloss: 0.686112\n",
      "[537]\ttraining's multi_logloss: 0.311261\tvalid_1's multi_logloss: 0.685909\n",
      "[538]\ttraining's multi_logloss: 0.310659\tvalid_1's multi_logloss: 0.685674\n",
      "[539]\ttraining's multi_logloss: 0.310028\tvalid_1's multi_logloss: 0.685433\n",
      "[540]\ttraining's multi_logloss: 0.309377\tvalid_1's multi_logloss: 0.685204\n",
      "[541]\ttraining's multi_logloss: 0.308754\tvalid_1's multi_logloss: 0.685012\n",
      "[542]\ttraining's multi_logloss: 0.308113\tvalid_1's multi_logloss: 0.684774\n",
      "[543]\ttraining's multi_logloss: 0.307482\tvalid_1's multi_logloss: 0.684532\n",
      "[544]\ttraining's multi_logloss: 0.306864\tvalid_1's multi_logloss: 0.684333\n",
      "[545]\ttraining's multi_logloss: 0.306242\tvalid_1's multi_logloss: 0.684142\n",
      "[546]\ttraining's multi_logloss: 0.305611\tvalid_1's multi_logloss: 0.683846\n",
      "[547]\ttraining's multi_logloss: 0.305027\tvalid_1's multi_logloss: 0.68365\n",
      "[548]\ttraining's multi_logloss: 0.304493\tvalid_1's multi_logloss: 0.683458\n",
      "[549]\ttraining's multi_logloss: 0.303877\tvalid_1's multi_logloss: 0.683241\n",
      "[550]\ttraining's multi_logloss: 0.303268\tvalid_1's multi_logloss: 0.68312\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[551]\ttraining's multi_logloss: 0.302795\tvalid_1's multi_logloss: 0.682932\n",
      "[552]\ttraining's multi_logloss: 0.302181\tvalid_1's multi_logloss: 0.682751\n",
      "[553]\ttraining's multi_logloss: 0.301547\tvalid_1's multi_logloss: 0.682522\n",
      "[554]\ttraining's multi_logloss: 0.300916\tvalid_1's multi_logloss: 0.682366\n",
      "[555]\ttraining's multi_logloss: 0.300288\tvalid_1's multi_logloss: 0.682045\n",
      "[556]\ttraining's multi_logloss: 0.29971\tvalid_1's multi_logloss: 0.681861\n",
      "[557]\ttraining's multi_logloss: 0.299129\tvalid_1's multi_logloss: 0.681637\n",
      "[558]\ttraining's multi_logloss: 0.29853\tvalid_1's multi_logloss: 0.681419\n",
      "[559]\ttraining's multi_logloss: 0.297919\tvalid_1's multi_logloss: 0.68129\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[560]\ttraining's multi_logloss: 0.297415\tvalid_1's multi_logloss: 0.681039\n",
      "[561]\ttraining's multi_logloss: 0.29685\tvalid_1's multi_logloss: 0.680872\n",
      "[562]\ttraining's multi_logloss: 0.296265\tvalid_1's multi_logloss: 0.680596\n",
      "[563]\ttraining's multi_logloss: 0.295635\tvalid_1's multi_logloss: 0.680361\n",
      "[564]\ttraining's multi_logloss: 0.295027\tvalid_1's multi_logloss: 0.680196\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[565]\ttraining's multi_logloss: 0.294521\tvalid_1's multi_logloss: 0.679867\n",
      "[566]\ttraining's multi_logloss: 0.293998\tvalid_1's multi_logloss: 0.679733\n",
      "[567]\ttraining's multi_logloss: 0.29341\tvalid_1's multi_logloss: 0.679598\n",
      "[568]\ttraining's multi_logloss: 0.292826\tvalid_1's multi_logloss: 0.679386\n",
      "[569]\ttraining's multi_logloss: 0.2922\tvalid_1's multi_logloss: 0.679163\n",
      "[570]\ttraining's multi_logloss: 0.291685\tvalid_1's multi_logloss: 0.679057\n",
      "[571]\ttraining's multi_logloss: 0.291092\tvalid_1's multi_logloss: 0.678811\n",
      "[572]\ttraining's multi_logloss: 0.290551\tvalid_1's multi_logloss: 0.678638\n",
      "[573]\ttraining's multi_logloss: 0.290016\tvalid_1's multi_logloss: 0.678429\n",
      "[574]\ttraining's multi_logloss: 0.289505\tvalid_1's multi_logloss: 0.678238\n",
      "[575]\ttraining's multi_logloss: 0.288995\tvalid_1's multi_logloss: 0.678141\n",
      "[576]\ttraining's multi_logloss: 0.288453\tvalid_1's multi_logloss: 0.677927\n",
      "[577]\ttraining's multi_logloss: 0.287852\tvalid_1's multi_logloss: 0.677713\n",
      "[578]\ttraining's multi_logloss: 0.287277\tvalid_1's multi_logloss: 0.677519\n",
      "[579]\ttraining's multi_logloss: 0.286673\tvalid_1's multi_logloss: 0.677312\n",
      "[580]\ttraining's multi_logloss: 0.286095\tvalid_1's multi_logloss: 0.677121\n",
      "[581]\ttraining's multi_logloss: 0.285535\tvalid_1's multi_logloss: 0.677015\n",
      "[582]\ttraining's multi_logloss: 0.284966\tvalid_1's multi_logloss: 0.67678\n",
      "[583]\ttraining's multi_logloss: 0.284499\tvalid_1's multi_logloss: 0.676605\n",
      "[584]\ttraining's multi_logloss: 0.283986\tvalid_1's multi_logloss: 0.676379\n",
      "[585]\ttraining's multi_logloss: 0.283467\tvalid_1's multi_logloss: 0.676221\n",
      "[586]\ttraining's multi_logloss: 0.282953\tvalid_1's multi_logloss: 0.676039\n",
      "[587]\ttraining's multi_logloss: 0.282443\tvalid_1's multi_logloss: 0.675858\n",
      "[588]\ttraining's multi_logloss: 0.281869\tvalid_1's multi_logloss: 0.67561\n",
      "[589]\ttraining's multi_logloss: 0.281316\tvalid_1's multi_logloss: 0.675404\n",
      "[590]\ttraining's multi_logloss: 0.280819\tvalid_1's multi_logloss: 0.675246\n",
      "[591]\ttraining's multi_logloss: 0.280262\tvalid_1's multi_logloss: 0.675027\n",
      "[592]\ttraining's multi_logloss: 0.279732\tvalid_1's multi_logloss: 0.674851\n",
      "[593]\ttraining's multi_logloss: 0.279229\tvalid_1's multi_logloss: 0.674616\n",
      "[594]\ttraining's multi_logloss: 0.278659\tvalid_1's multi_logloss: 0.674371\n",
      "[595]\ttraining's multi_logloss: 0.278209\tvalid_1's multi_logloss: 0.674238\n",
      "[596]\ttraining's multi_logloss: 0.277677\tvalid_1's multi_logloss: 0.674014\n",
      "[597]\ttraining's multi_logloss: 0.277169\tvalid_1's multi_logloss: 0.673834\n",
      "[598]\ttraining's multi_logloss: 0.276682\tvalid_1's multi_logloss: 0.673616\n",
      "[599]\ttraining's multi_logloss: 0.276147\tvalid_1's multi_logloss: 0.673369\n",
      "[600]\ttraining's multi_logloss: 0.275732\tvalid_1's multi_logloss: 0.673275\n",
      "[601]\ttraining's multi_logloss: 0.275239\tvalid_1's multi_logloss: 0.673109\n",
      "[602]\ttraining's multi_logloss: 0.274688\tvalid_1's multi_logloss: 0.672901\n",
      "[603]\ttraining's multi_logloss: 0.274156\tvalid_1's multi_logloss: 0.672681\n",
      "[604]\ttraining's multi_logloss: 0.273675\tvalid_1's multi_logloss: 0.672503\n",
      "[605]\ttraining's multi_logloss: 0.273161\tvalid_1's multi_logloss: 0.672342\n",
      "[606]\ttraining's multi_logloss: 0.272633\tvalid_1's multi_logloss: 0.672169\n",
      "[607]\ttraining's multi_logloss: 0.272159\tvalid_1's multi_logloss: 0.67203\n",
      "[608]\ttraining's multi_logloss: 0.271655\tvalid_1's multi_logloss: 0.671828\n",
      "[609]\ttraining's multi_logloss: 0.27113\tvalid_1's multi_logloss: 0.671609\n",
      "[610]\ttraining's multi_logloss: 0.270638\tvalid_1's multi_logloss: 0.671456\n",
      "[611]\ttraining's multi_logloss: 0.270152\tvalid_1's multi_logloss: 0.671334\n",
      "[612]\ttraining's multi_logloss: 0.269661\tvalid_1's multi_logloss: 0.671099\n",
      "[613]\ttraining's multi_logloss: 0.269128\tvalid_1's multi_logloss: 0.670878\n",
      "[614]\ttraining's multi_logloss: 0.268675\tvalid_1's multi_logloss: 0.670762\n",
      "[615]\ttraining's multi_logloss: 0.268153\tvalid_1's multi_logloss: 0.670544\n",
      "[616]\ttraining's multi_logloss: 0.267638\tvalid_1's multi_logloss: 0.670343\n",
      "[617]\ttraining's multi_logloss: 0.267129\tvalid_1's multi_logloss: 0.670163\n",
      "[618]\ttraining's multi_logloss: 0.266622\tvalid_1's multi_logloss: 0.66995\n",
      "[619]\ttraining's multi_logloss: 0.26611\tvalid_1's multi_logloss: 0.669808\n",
      "[620]\ttraining's multi_logloss: 0.265563\tvalid_1's multi_logloss: 0.669561\n",
      "[621]\ttraining's multi_logloss: 0.265089\tvalid_1's multi_logloss: 0.669419\n",
      "[622]\ttraining's multi_logloss: 0.264626\tvalid_1's multi_logloss: 0.669251\n",
      "[623]\ttraining's multi_logloss: 0.264177\tvalid_1's multi_logloss: 0.669075\n",
      "[624]\ttraining's multi_logloss: 0.263739\tvalid_1's multi_logloss: 0.668905\n",
      "[625]\ttraining's multi_logloss: 0.263222\tvalid_1's multi_logloss: 0.668757\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[626]\ttraining's multi_logloss: 0.262817\tvalid_1's multi_logloss: 0.668586\n",
      "[627]\ttraining's multi_logloss: 0.262324\tvalid_1's multi_logloss: 0.668411\n",
      "[628]\ttraining's multi_logloss: 0.261829\tvalid_1's multi_logloss: 0.668234\n",
      "[629]\ttraining's multi_logloss: 0.261291\tvalid_1's multi_logloss: 0.668019\n",
      "[630]\ttraining's multi_logloss: 0.260761\tvalid_1's multi_logloss: 0.667782\n",
      "[631]\ttraining's multi_logloss: 0.260246\tvalid_1's multi_logloss: 0.667626\n",
      "[632]\ttraining's multi_logloss: 0.25974\tvalid_1's multi_logloss: 0.66745\n",
      "[633]\ttraining's multi_logloss: 0.259271\tvalid_1's multi_logloss: 0.667282\n",
      "[634]\ttraining's multi_logloss: 0.258801\tvalid_1's multi_logloss: 0.667116\n",
      "[635]\ttraining's multi_logloss: 0.258278\tvalid_1's multi_logloss: 0.666923\n",
      "[636]\ttraining's multi_logloss: 0.257831\tvalid_1's multi_logloss: 0.666673\n",
      "[637]\ttraining's multi_logloss: 0.257341\tvalid_1's multi_logloss: 0.666553\n",
      "[638]\ttraining's multi_logloss: 0.256858\tvalid_1's multi_logloss: 0.666424\n",
      "[639]\ttraining's multi_logloss: 0.256329\tvalid_1's multi_logloss: 0.666168\n",
      "[640]\ttraining's multi_logloss: 0.255856\tvalid_1's multi_logloss: 0.66599\n",
      "[641]\ttraining's multi_logloss: 0.255333\tvalid_1's multi_logloss: 0.665789\n",
      "[642]\ttraining's multi_logloss: 0.254832\tvalid_1's multi_logloss: 0.665576\n",
      "[643]\ttraining's multi_logloss: 0.25436\tvalid_1's multi_logloss: 0.665427\n",
      "[644]\ttraining's multi_logloss: 0.253857\tvalid_1's multi_logloss: 0.665242\n",
      "[645]\ttraining's multi_logloss: 0.253332\tvalid_1's multi_logloss: 0.665084\n",
      "[646]\ttraining's multi_logloss: 0.252874\tvalid_1's multi_logloss: 0.664908\n",
      "[647]\ttraining's multi_logloss: 0.252412\tvalid_1's multi_logloss: 0.664784\n",
      "[648]\ttraining's multi_logloss: 0.251934\tvalid_1's multi_logloss: 0.664617\n",
      "[649]\ttraining's multi_logloss: 0.251465\tvalid_1's multi_logloss: 0.664415\n",
      "[650]\ttraining's multi_logloss: 0.251003\tvalid_1's multi_logloss: 0.664269\n",
      "[651]\ttraining's multi_logloss: 0.250523\tvalid_1's multi_logloss: 0.664123\n",
      "[652]\ttraining's multi_logloss: 0.250035\tvalid_1's multi_logloss: 0.663949\n",
      "[653]\ttraining's multi_logloss: 0.249548\tvalid_1's multi_logloss: 0.663827\n",
      "[654]\ttraining's multi_logloss: 0.249053\tvalid_1's multi_logloss: 0.663694\n",
      "[655]\ttraining's multi_logloss: 0.248608\tvalid_1's multi_logloss: 0.663552\n",
      "[656]\ttraining's multi_logloss: 0.24813\tvalid_1's multi_logloss: 0.66337\n",
      "[657]\ttraining's multi_logloss: 0.247674\tvalid_1's multi_logloss: 0.6632\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[658]\ttraining's multi_logloss: 0.247312\tvalid_1's multi_logloss: 0.663074\n",
      "[659]\ttraining's multi_logloss: 0.246869\tvalid_1's multi_logloss: 0.662922\n",
      "[660]\ttraining's multi_logloss: 0.246427\tvalid_1's multi_logloss: 0.662812\n",
      "[661]\ttraining's multi_logloss: 0.245938\tvalid_1's multi_logloss: 0.662624\n",
      "[662]\ttraining's multi_logloss: 0.245475\tvalid_1's multi_logloss: 0.662473\n",
      "[663]\ttraining's multi_logloss: 0.245045\tvalid_1's multi_logloss: 0.662358\n",
      "[664]\ttraining's multi_logloss: 0.24455\tvalid_1's multi_logloss: 0.662129\n",
      "[665]\ttraining's multi_logloss: 0.244047\tvalid_1's multi_logloss: 0.661916\n",
      "[666]\ttraining's multi_logloss: 0.243599\tvalid_1's multi_logloss: 0.661739\n",
      "[667]\ttraining's multi_logloss: 0.243186\tvalid_1's multi_logloss: 0.661632\n",
      "[668]\ttraining's multi_logloss: 0.242737\tvalid_1's multi_logloss: 0.66149\n",
      "[669]\ttraining's multi_logloss: 0.242289\tvalid_1's multi_logloss: 0.661335\n",
      "[670]\ttraining's multi_logloss: 0.24184\tvalid_1's multi_logloss: 0.66114\n",
      "[671]\ttraining's multi_logloss: 0.241361\tvalid_1's multi_logloss: 0.661045\n",
      "[672]\ttraining's multi_logloss: 0.240883\tvalid_1's multi_logloss: 0.660897\n",
      "[673]\ttraining's multi_logloss: 0.240402\tvalid_1's multi_logloss: 0.660719\n",
      "[674]\ttraining's multi_logloss: 0.239937\tvalid_1's multi_logloss: 0.660608\n",
      "[675]\ttraining's multi_logloss: 0.239544\tvalid_1's multi_logloss: 0.660537\n",
      "[676]\ttraining's multi_logloss: 0.239102\tvalid_1's multi_logloss: 0.660409\n",
      "[677]\ttraining's multi_logloss: 0.238638\tvalid_1's multi_logloss: 0.660271\n",
      "[678]\ttraining's multi_logloss: 0.238202\tvalid_1's multi_logloss: 0.660131\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[679]\ttraining's multi_logloss: 0.237809\tvalid_1's multi_logloss: 0.660072\n",
      "[680]\ttraining's multi_logloss: 0.237402\tvalid_1's multi_logloss: 0.659921\n",
      "[681]\ttraining's multi_logloss: 0.23698\tvalid_1's multi_logloss: 0.659704\n",
      "[682]\ttraining's multi_logloss: 0.236514\tvalid_1's multi_logloss: 0.659494\n",
      "[683]\ttraining's multi_logloss: 0.236075\tvalid_1's multi_logloss: 0.659335\n",
      "[684]\ttraining's multi_logloss: 0.235629\tvalid_1's multi_logloss: 0.659203\n",
      "[685]\ttraining's multi_logloss: 0.235184\tvalid_1's multi_logloss: 0.65903\n",
      "[686]\ttraining's multi_logloss: 0.234771\tvalid_1's multi_logloss: 0.658878\n",
      "[687]\ttraining's multi_logloss: 0.234304\tvalid_1's multi_logloss: 0.658712\n",
      "[688]\ttraining's multi_logloss: 0.233843\tvalid_1's multi_logloss: 0.658546\n",
      "[689]\ttraining's multi_logloss: 0.233392\tvalid_1's multi_logloss: 0.658387\n",
      "[690]\ttraining's multi_logloss: 0.232917\tvalid_1's multi_logloss: 0.658193\n",
      "[691]\ttraining's multi_logloss: 0.232481\tvalid_1's multi_logloss: 0.658015\n",
      "[692]\ttraining's multi_logloss: 0.232038\tvalid_1's multi_logloss: 0.657927\n",
      "[693]\ttraining's multi_logloss: 0.231605\tvalid_1's multi_logloss: 0.657764\n",
      "[694]\ttraining's multi_logloss: 0.231221\tvalid_1's multi_logloss: 0.657618\n",
      "[695]\ttraining's multi_logloss: 0.230834\tvalid_1's multi_logloss: 0.657521\n",
      "[696]\ttraining's multi_logloss: 0.230424\tvalid_1's multi_logloss: 0.657346\n",
      "[697]\ttraining's multi_logloss: 0.22998\tvalid_1's multi_logloss: 0.65722\n",
      "[698]\ttraining's multi_logloss: 0.229524\tvalid_1's multi_logloss: 0.6571\n",
      "[699]\ttraining's multi_logloss: 0.229119\tvalid_1's multi_logloss: 0.656966\n",
      "[700]\ttraining's multi_logloss: 0.22871\tvalid_1's multi_logloss: 0.656833\n",
      "[701]\ttraining's multi_logloss: 0.228291\tvalid_1's multi_logloss: 0.656715\n",
      "[702]\ttraining's multi_logloss: 0.227848\tvalid_1's multi_logloss: 0.656581\n",
      "[703]\ttraining's multi_logloss: 0.227434\tvalid_1's multi_logloss: 0.656489\n",
      "[704]\ttraining's multi_logloss: 0.227024\tvalid_1's multi_logloss: 0.656363\n",
      "[705]\ttraining's multi_logloss: 0.226647\tvalid_1's multi_logloss: 0.656265\n",
      "[706]\ttraining's multi_logloss: 0.226195\tvalid_1's multi_logloss: 0.656112\n",
      "[707]\ttraining's multi_logloss: 0.22579\tvalid_1's multi_logloss: 0.655954\n",
      "[708]\ttraining's multi_logloss: 0.225351\tvalid_1's multi_logloss: 0.655715\n",
      "[709]\ttraining's multi_logloss: 0.22499\tvalid_1's multi_logloss: 0.655632\n",
      "[710]\ttraining's multi_logloss: 0.22457\tvalid_1's multi_logloss: 0.655539\n",
      "[711]\ttraining's multi_logloss: 0.224146\tvalid_1's multi_logloss: 0.6554\n",
      "[712]\ttraining's multi_logloss: 0.22372\tvalid_1's multi_logloss: 0.65528\n",
      "[713]\ttraining's multi_logloss: 0.223308\tvalid_1's multi_logloss: 0.655151\n",
      "[714]\ttraining's multi_logloss: 0.222914\tvalid_1's multi_logloss: 0.654979\n",
      "[715]\ttraining's multi_logloss: 0.222523\tvalid_1's multi_logloss: 0.654865\n",
      "[716]\ttraining's multi_logloss: 0.2221\tvalid_1's multi_logloss: 0.654736\n",
      "[717]\ttraining's multi_logloss: 0.221687\tvalid_1's multi_logloss: 0.654657\n",
      "[718]\ttraining's multi_logloss: 0.22129\tvalid_1's multi_logloss: 0.654496\n",
      "[719]\ttraining's multi_logloss: 0.220886\tvalid_1's multi_logloss: 0.654393\n",
      "[720]\ttraining's multi_logloss: 0.22049\tvalid_1's multi_logloss: 0.654261\n",
      "[721]\ttraining's multi_logloss: 0.220119\tvalid_1's multi_logloss: 0.654081\n",
      "[722]\ttraining's multi_logloss: 0.219744\tvalid_1's multi_logloss: 0.653964\n",
      "[723]\ttraining's multi_logloss: 0.219346\tvalid_1's multi_logloss: 0.653788\n",
      "[724]\ttraining's multi_logloss: 0.218941\tvalid_1's multi_logloss: 0.653709\n",
      "[725]\ttraining's multi_logloss: 0.218534\tvalid_1's multi_logloss: 0.653533\n",
      "[726]\ttraining's multi_logloss: 0.218124\tvalid_1's multi_logloss: 0.653382\n",
      "[727]\ttraining's multi_logloss: 0.217722\tvalid_1's multi_logloss: 0.653306\n",
      "[728]\ttraining's multi_logloss: 0.217292\tvalid_1's multi_logloss: 0.653136\n",
      "[729]\ttraining's multi_logloss: 0.216913\tvalid_1's multi_logloss: 0.652946\n",
      "[730]\ttraining's multi_logloss: 0.216496\tvalid_1's multi_logloss: 0.652733\n",
      "[731]\ttraining's multi_logloss: 0.216119\tvalid_1's multi_logloss: 0.65262\n",
      "[732]\ttraining's multi_logloss: 0.215743\tvalid_1's multi_logloss: 0.652473\n",
      "[733]\ttraining's multi_logloss: 0.21536\tvalid_1's multi_logloss: 0.652414\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[734]\ttraining's multi_logloss: 0.215062\tvalid_1's multi_logloss: 0.652349\n",
      "[735]\ttraining's multi_logloss: 0.214669\tvalid_1's multi_logloss: 0.652183\n",
      "[736]\ttraining's multi_logloss: 0.214287\tvalid_1's multi_logloss: 0.652066\n",
      "[737]\ttraining's multi_logloss: 0.213886\tvalid_1's multi_logloss: 0.651877\n",
      "[738]\ttraining's multi_logloss: 0.213471\tvalid_1's multi_logloss: 0.651693\n",
      "[739]\ttraining's multi_logloss: 0.213078\tvalid_1's multi_logloss: 0.651549\n",
      "[740]\ttraining's multi_logloss: 0.21267\tvalid_1's multi_logloss: 0.651406\n",
      "[741]\ttraining's multi_logloss: 0.212304\tvalid_1's multi_logloss: 0.65136\n",
      "[742]\ttraining's multi_logloss: 0.211974\tvalid_1's multi_logloss: 0.651337\n",
      "[743]\ttraining's multi_logloss: 0.211572\tvalid_1's multi_logloss: 0.651198\n",
      "[744]\ttraining's multi_logloss: 0.211171\tvalid_1's multi_logloss: 0.651041\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[745]\ttraining's multi_logloss: 0.210818\tvalid_1's multi_logloss: 0.650868\n",
      "[746]\ttraining's multi_logloss: 0.210452\tvalid_1's multi_logloss: 0.650797\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[747]\ttraining's multi_logloss: 0.21015\tvalid_1's multi_logloss: 0.650725\n",
      "[748]\ttraining's multi_logloss: 0.209798\tvalid_1's multi_logloss: 0.650652\n",
      "[749]\ttraining's multi_logloss: 0.20938\tvalid_1's multi_logloss: 0.650427\n",
      "[750]\ttraining's multi_logloss: 0.208999\tvalid_1's multi_logloss: 0.650359\n",
      "[751]\ttraining's multi_logloss: 0.20863\tvalid_1's multi_logloss: 0.65023\n",
      "[752]\ttraining's multi_logloss: 0.208256\tvalid_1's multi_logloss: 0.650175\n",
      "[753]\ttraining's multi_logloss: 0.207871\tvalid_1's multi_logloss: 0.650052\n",
      "[754]\ttraining's multi_logloss: 0.207517\tvalid_1's multi_logloss: 0.649859\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[755]\ttraining's multi_logloss: 0.207214\tvalid_1's multi_logloss: 0.64974\n",
      "[756]\ttraining's multi_logloss: 0.206828\tvalid_1's multi_logloss: 0.649701\n",
      "[757]\ttraining's multi_logloss: 0.206452\tvalid_1's multi_logloss: 0.649512\n",
      "[758]\ttraining's multi_logloss: 0.206091\tvalid_1's multi_logloss: 0.649414\n",
      "[759]\ttraining's multi_logloss: 0.205711\tvalid_1's multi_logloss: 0.649322\n",
      "[760]\ttraining's multi_logloss: 0.205369\tvalid_1's multi_logloss: 0.649217\n",
      "[761]\ttraining's multi_logloss: 0.205046\tvalid_1's multi_logloss: 0.649185\n",
      "[762]\ttraining's multi_logloss: 0.204709\tvalid_1's multi_logloss: 0.649084\n",
      "[763]\ttraining's multi_logloss: 0.204334\tvalid_1's multi_logloss: 0.649007\n",
      "[764]\ttraining's multi_logloss: 0.203975\tvalid_1's multi_logloss: 0.648904\n",
      "[765]\ttraining's multi_logloss: 0.2036\tvalid_1's multi_logloss: 0.648765\n",
      "[766]\ttraining's multi_logloss: 0.20327\tvalid_1's multi_logloss: 0.648701\n",
      "[767]\ttraining's multi_logloss: 0.202907\tvalid_1's multi_logloss: 0.648593\n",
      "[768]\ttraining's multi_logloss: 0.202539\tvalid_1's multi_logloss: 0.648565\n",
      "[769]\ttraining's multi_logloss: 0.202187\tvalid_1's multi_logloss: 0.648459\n",
      "[770]\ttraining's multi_logloss: 0.201831\tvalid_1's multi_logloss: 0.648352\n",
      "[771]\ttraining's multi_logloss: 0.201505\tvalid_1's multi_logloss: 0.64822\n",
      "[772]\ttraining's multi_logloss: 0.201161\tvalid_1's multi_logloss: 0.648056\n",
      "[773]\ttraining's multi_logloss: 0.200793\tvalid_1's multi_logloss: 0.647995\n",
      "[774]\ttraining's multi_logloss: 0.200426\tvalid_1's multi_logloss: 0.647943\n",
      "[775]\ttraining's multi_logloss: 0.200075\tvalid_1's multi_logloss: 0.647813\n",
      "[776]\ttraining's multi_logloss: 0.199729\tvalid_1's multi_logloss: 0.647814\n",
      "[777]\ttraining's multi_logloss: 0.199377\tvalid_1's multi_logloss: 0.647747\n",
      "[778]\ttraining's multi_logloss: 0.19906\tvalid_1's multi_logloss: 0.64763\n",
      "[779]\ttraining's multi_logloss: 0.198675\tvalid_1's multi_logloss: 0.647598\n",
      "[780]\ttraining's multi_logloss: 0.198329\tvalid_1's multi_logloss: 0.647482\n",
      "[781]\ttraining's multi_logloss: 0.197992\tvalid_1's multi_logloss: 0.647358\n",
      "[782]\ttraining's multi_logloss: 0.197609\tvalid_1's multi_logloss: 0.647286\n",
      "[783]\ttraining's multi_logloss: 0.197236\tvalid_1's multi_logloss: 0.64723\n",
      "[784]\ttraining's multi_logloss: 0.196907\tvalid_1's multi_logloss: 0.647121\n",
      "[785]\ttraining's multi_logloss: 0.196531\tvalid_1's multi_logloss: 0.64699\n",
      "[786]\ttraining's multi_logloss: 0.196217\tvalid_1's multi_logloss: 0.646893\n",
      "[787]\ttraining's multi_logloss: 0.195889\tvalid_1's multi_logloss: 0.646818\n",
      "[788]\ttraining's multi_logloss: 0.195572\tvalid_1's multi_logloss: 0.646666\n",
      "[789]\ttraining's multi_logloss: 0.195217\tvalid_1's multi_logloss: 0.64654\n",
      "[790]\ttraining's multi_logloss: 0.194884\tvalid_1's multi_logloss: 0.646434\n",
      "[791]\ttraining's multi_logloss: 0.194538\tvalid_1's multi_logloss: 0.646314\n",
      "[792]\ttraining's multi_logloss: 0.194211\tvalid_1's multi_logloss: 0.646254\n",
      "[793]\ttraining's multi_logloss: 0.193854\tvalid_1's multi_logloss: 0.646165\n",
      "[794]\ttraining's multi_logloss: 0.193556\tvalid_1's multi_logloss: 0.646107\n",
      "[795]\ttraining's multi_logloss: 0.193216\tvalid_1's multi_logloss: 0.645981\n",
      "[796]\ttraining's multi_logloss: 0.192868\tvalid_1's multi_logloss: 0.645936\n",
      "[797]\ttraining's multi_logloss: 0.192534\tvalid_1's multi_logloss: 0.645845\n",
      "[798]\ttraining's multi_logloss: 0.19221\tvalid_1's multi_logloss: 0.645737\n",
      "[799]\ttraining's multi_logloss: 0.191874\tvalid_1's multi_logloss: 0.645735\n",
      "[800]\ttraining's multi_logloss: 0.191517\tvalid_1's multi_logloss: 0.645687\n",
      "[801]\ttraining's multi_logloss: 0.19119\tvalid_1's multi_logloss: 0.645589\n",
      "[802]\ttraining's multi_logloss: 0.190847\tvalid_1's multi_logloss: 0.645537\n",
      "[803]\ttraining's multi_logloss: 0.190498\tvalid_1's multi_logloss: 0.645446\n",
      "[804]\ttraining's multi_logloss: 0.190154\tvalid_1's multi_logloss: 0.645285\n",
      "[805]\ttraining's multi_logloss: 0.189828\tvalid_1's multi_logloss: 0.645124\n",
      "[806]\ttraining's multi_logloss: 0.189545\tvalid_1's multi_logloss: 0.64507\n",
      "[807]\ttraining's multi_logloss: 0.189215\tvalid_1's multi_logloss: 0.644995\n",
      "[808]\ttraining's multi_logloss: 0.188932\tvalid_1's multi_logloss: 0.644943\n",
      "[809]\ttraining's multi_logloss: 0.18859\tvalid_1's multi_logloss: 0.644796\n",
      "[810]\ttraining's multi_logloss: 0.188281\tvalid_1's multi_logloss: 0.644706\n",
      "[811]\ttraining's multi_logloss: 0.187959\tvalid_1's multi_logloss: 0.644694\n",
      "[812]\ttraining's multi_logloss: 0.187632\tvalid_1's multi_logloss: 0.644612\n",
      "[813]\ttraining's multi_logloss: 0.187301\tvalid_1's multi_logloss: 0.644522\n",
      "[814]\ttraining's multi_logloss: 0.18696\tvalid_1's multi_logloss: 0.644436\n",
      "[815]\ttraining's multi_logloss: 0.186606\tvalid_1's multi_logloss: 0.644317\n",
      "[816]\ttraining's multi_logloss: 0.186313\tvalid_1's multi_logloss: 0.644107\n",
      "[817]\ttraining's multi_logloss: 0.186013\tvalid_1's multi_logloss: 0.643963\n",
      "[818]\ttraining's multi_logloss: 0.185722\tvalid_1's multi_logloss: 0.643841\n",
      "[819]\ttraining's multi_logloss: 0.185403\tvalid_1's multi_logloss: 0.643711\n",
      "[820]\ttraining's multi_logloss: 0.185069\tvalid_1's multi_logloss: 0.643617\n",
      "[821]\ttraining's multi_logloss: 0.184751\tvalid_1's multi_logloss: 0.643535\n",
      "[822]\ttraining's multi_logloss: 0.184471\tvalid_1's multi_logloss: 0.643425\n",
      "[823]\ttraining's multi_logloss: 0.184178\tvalid_1's multi_logloss: 0.64335\n",
      "[824]\ttraining's multi_logloss: 0.183916\tvalid_1's multi_logloss: 0.643224\n",
      "[825]\ttraining's multi_logloss: 0.183606\tvalid_1's multi_logloss: 0.643107\n",
      "[826]\ttraining's multi_logloss: 0.183309\tvalid_1's multi_logloss: 0.64296\n",
      "[827]\ttraining's multi_logloss: 0.182991\tvalid_1's multi_logloss: 0.642916\n",
      "[828]\ttraining's multi_logloss: 0.182673\tvalid_1's multi_logloss: 0.64283\n",
      "[829]\ttraining's multi_logloss: 0.182357\tvalid_1's multi_logloss: 0.642769\n",
      "[830]\ttraining's multi_logloss: 0.182057\tvalid_1's multi_logloss: 0.642634\n",
      "[831]\ttraining's multi_logloss: 0.18176\tvalid_1's multi_logloss: 0.642496\n",
      "[832]\ttraining's multi_logloss: 0.181481\tvalid_1's multi_logloss: 0.642358\n",
      "[833]\ttraining's multi_logloss: 0.18119\tvalid_1's multi_logloss: 0.642268\n",
      "[834]\ttraining's multi_logloss: 0.180862\tvalid_1's multi_logloss: 0.642105\n",
      "[835]\ttraining's multi_logloss: 0.180574\tvalid_1's multi_logloss: 0.641997\n",
      "[836]\ttraining's multi_logloss: 0.180256\tvalid_1's multi_logloss: 0.641894\n",
      "[837]\ttraining's multi_logloss: 0.17994\tvalid_1's multi_logloss: 0.64181\n",
      "[838]\ttraining's multi_logloss: 0.179655\tvalid_1's multi_logloss: 0.641702\n",
      "[839]\ttraining's multi_logloss: 0.179327\tvalid_1's multi_logloss: 0.64171\n",
      "[840]\ttraining's multi_logloss: 0.179002\tvalid_1's multi_logloss: 0.641606\n",
      "[841]\ttraining's multi_logloss: 0.178676\tvalid_1's multi_logloss: 0.641563\n",
      "[842]\ttraining's multi_logloss: 0.178372\tvalid_1's multi_logloss: 0.641416\n",
      "[843]\ttraining's multi_logloss: 0.178047\tvalid_1's multi_logloss: 0.641349\n",
      "[844]\ttraining's multi_logloss: 0.177698\tvalid_1's multi_logloss: 0.641189\n",
      "[845]\ttraining's multi_logloss: 0.177393\tvalid_1's multi_logloss: 0.641131\n",
      "[846]\ttraining's multi_logloss: 0.177088\tvalid_1's multi_logloss: 0.641053\n",
      "[847]\ttraining's multi_logloss: 0.176811\tvalid_1's multi_logloss: 0.640971\n",
      "[848]\ttraining's multi_logloss: 0.176519\tvalid_1's multi_logloss: 0.640939\n",
      "[849]\ttraining's multi_logloss: 0.176195\tvalid_1's multi_logloss: 0.640888\n",
      "[850]\ttraining's multi_logloss: 0.175857\tvalid_1's multi_logloss: 0.640723\n",
      "[851]\ttraining's multi_logloss: 0.175554\tvalid_1's multi_logloss: 0.640639\n",
      "[852]\ttraining's multi_logloss: 0.175238\tvalid_1's multi_logloss: 0.640528\n",
      "[853]\ttraining's multi_logloss: 0.174934\tvalid_1's multi_logloss: 0.640476\n",
      "[854]\ttraining's multi_logloss: 0.174655\tvalid_1's multi_logloss: 0.64041\n",
      "[855]\ttraining's multi_logloss: 0.174348\tvalid_1's multi_logloss: 0.640383\n",
      "[856]\ttraining's multi_logloss: 0.174055\tvalid_1's multi_logloss: 0.640269\n",
      "[857]\ttraining's multi_logloss: 0.173767\tvalid_1's multi_logloss: 0.640178\n",
      "[858]\ttraining's multi_logloss: 0.173454\tvalid_1's multi_logloss: 0.640107\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[859]\ttraining's multi_logloss: 0.173213\tvalid_1's multi_logloss: 0.640007\n",
      "[860]\ttraining's multi_logloss: 0.172886\tvalid_1's multi_logloss: 0.639849\n",
      "[861]\ttraining's multi_logloss: 0.17261\tvalid_1's multi_logloss: 0.639774\n",
      "[862]\ttraining's multi_logloss: 0.172302\tvalid_1's multi_logloss: 0.639725\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[863]\ttraining's multi_logloss: 0.172062\tvalid_1's multi_logloss: 0.639545\n",
      "[864]\ttraining's multi_logloss: 0.171734\tvalid_1's multi_logloss: 0.639463\n",
      "[865]\ttraining's multi_logloss: 0.171433\tvalid_1's multi_logloss: 0.639355\n",
      "[866]\ttraining's multi_logloss: 0.17118\tvalid_1's multi_logloss: 0.63926\n",
      "[867]\ttraining's multi_logloss: 0.170912\tvalid_1's multi_logloss: 0.639171\n",
      "[868]\ttraining's multi_logloss: 0.170583\tvalid_1's multi_logloss: 0.639083\n",
      "[869]\ttraining's multi_logloss: 0.170278\tvalid_1's multi_logloss: 0.63903\n",
      "[870]\ttraining's multi_logloss: 0.170014\tvalid_1's multi_logloss: 0.638931\n",
      "[871]\ttraining's multi_logloss: 0.169718\tvalid_1's multi_logloss: 0.638892\n",
      "[872]\ttraining's multi_logloss: 0.169404\tvalid_1's multi_logloss: 0.63875\n",
      "[873]\ttraining's multi_logloss: 0.169086\tvalid_1's multi_logloss: 0.638627\n",
      "[874]\ttraining's multi_logloss: 0.168793\tvalid_1's multi_logloss: 0.638514\n",
      "[875]\ttraining's multi_logloss: 0.168482\tvalid_1's multi_logloss: 0.638387\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[876]\ttraining's multi_logloss: 0.16823\tvalid_1's multi_logloss: 0.638331\n",
      "[877]\ttraining's multi_logloss: 0.167946\tvalid_1's multi_logloss: 0.638313\n",
      "[878]\ttraining's multi_logloss: 0.167662\tvalid_1's multi_logloss: 0.638248\n",
      "[879]\ttraining's multi_logloss: 0.167379\tvalid_1's multi_logloss: 0.638091\n",
      "[880]\ttraining's multi_logloss: 0.167091\tvalid_1's multi_logloss: 0.638038\n",
      "[881]\ttraining's multi_logloss: 0.166828\tvalid_1's multi_logloss: 0.637904\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[882]\ttraining's multi_logloss: 0.1666\tvalid_1's multi_logloss: 0.637774\n",
      "[883]\ttraining's multi_logloss: 0.166307\tvalid_1's multi_logloss: 0.637669\n",
      "[884]\ttraining's multi_logloss: 0.166022\tvalid_1's multi_logloss: 0.63758\n",
      "[885]\ttraining's multi_logloss: 0.165761\tvalid_1's multi_logloss: 0.637547\n",
      "[886]\ttraining's multi_logloss: 0.165462\tvalid_1's multi_logloss: 0.637405\n",
      "[887]\ttraining's multi_logloss: 0.165175\tvalid_1's multi_logloss: 0.637388\n",
      "[888]\ttraining's multi_logloss: 0.164863\tvalid_1's multi_logloss: 0.637281\n",
      "[889]\ttraining's multi_logloss: 0.164578\tvalid_1's multi_logloss: 0.63725\n",
      "[890]\ttraining's multi_logloss: 0.164272\tvalid_1's multi_logloss: 0.637104\n",
      "[891]\ttraining's multi_logloss: 0.163958\tvalid_1's multi_logloss: 0.63699\n",
      "[892]\ttraining's multi_logloss: 0.163645\tvalid_1's multi_logloss: 0.636842\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[893]\ttraining's multi_logloss: 0.163415\tvalid_1's multi_logloss: 0.636801\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[894]\ttraining's multi_logloss: 0.163195\tvalid_1's multi_logloss: 0.636673\n",
      "[895]\ttraining's multi_logloss: 0.162904\tvalid_1's multi_logloss: 0.636574\n",
      "[896]\ttraining's multi_logloss: 0.16263\tvalid_1's multi_logloss: 0.63648\n",
      "[897]\ttraining's multi_logloss: 0.16232\tvalid_1's multi_logloss: 0.636395\n",
      "[898]\ttraining's multi_logloss: 0.162043\tvalid_1's multi_logloss: 0.636278\n",
      "[899]\ttraining's multi_logloss: 0.16181\tvalid_1's multi_logloss: 0.636306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[900]\ttraining's multi_logloss: 0.161563\tvalid_1's multi_logloss: 0.636231\n",
      "[901]\ttraining's multi_logloss: 0.161272\tvalid_1's multi_logloss: 0.636169\n",
      "[902]\ttraining's multi_logloss: 0.160991\tvalid_1's multi_logloss: 0.636085\n",
      "[903]\ttraining's multi_logloss: 0.160696\tvalid_1's multi_logloss: 0.63606\n",
      "[904]\ttraining's multi_logloss: 0.160415\tvalid_1's multi_logloss: 0.635995\n",
      "[905]\ttraining's multi_logloss: 0.160165\tvalid_1's multi_logloss: 0.635924\n",
      "[906]\ttraining's multi_logloss: 0.159875\tvalid_1's multi_logloss: 0.635833\n",
      "[907]\ttraining's multi_logloss: 0.1596\tvalid_1's multi_logloss: 0.635788\n",
      "[908]\ttraining's multi_logloss: 0.15932\tvalid_1's multi_logloss: 0.635705\n",
      "[909]\ttraining's multi_logloss: 0.15903\tvalid_1's multi_logloss: 0.635615\n",
      "[910]\ttraining's multi_logloss: 0.158742\tvalid_1's multi_logloss: 0.635524\n",
      "[911]\ttraining's multi_logloss: 0.15848\tvalid_1's multi_logloss: 0.63547\n",
      "[912]\ttraining's multi_logloss: 0.158198\tvalid_1's multi_logloss: 0.635413\n",
      "[913]\ttraining's multi_logloss: 0.157934\tvalid_1's multi_logloss: 0.635398\n",
      "[914]\ttraining's multi_logloss: 0.157667\tvalid_1's multi_logloss: 0.635306\n",
      "[915]\ttraining's multi_logloss: 0.15738\tvalid_1's multi_logloss: 0.635235\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[916]\ttraining's multi_logloss: 0.157146\tvalid_1's multi_logloss: 0.635069\n",
      "[917]\ttraining's multi_logloss: 0.156879\tvalid_1's multi_logloss: 0.635002\n",
      "[918]\ttraining's multi_logloss: 0.156601\tvalid_1's multi_logloss: 0.634866\n",
      "[919]\ttraining's multi_logloss: 0.156324\tvalid_1's multi_logloss: 0.634784\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[920]\ttraining's multi_logloss: 0.156089\tvalid_1's multi_logloss: 0.634712\n",
      "[921]\ttraining's multi_logloss: 0.155835\tvalid_1's multi_logloss: 0.634609\n",
      "[922]\ttraining's multi_logloss: 0.15557\tvalid_1's multi_logloss: 0.634454\n",
      "[923]\ttraining's multi_logloss: 0.155301\tvalid_1's multi_logloss: 0.634383\n",
      "[924]\ttraining's multi_logloss: 0.155011\tvalid_1's multi_logloss: 0.63426\n",
      "[925]\ttraining's multi_logloss: 0.15474\tvalid_1's multi_logloss: 0.634241\n",
      "[926]\ttraining's multi_logloss: 0.154506\tvalid_1's multi_logloss: 0.63419\n",
      "[927]\ttraining's multi_logloss: 0.154256\tvalid_1's multi_logloss: 0.634105\n",
      "[928]\ttraining's multi_logloss: 0.153994\tvalid_1's multi_logloss: 0.634016\n",
      "[929]\ttraining's multi_logloss: 0.153741\tvalid_1's multi_logloss: 0.633939\n",
      "[930]\ttraining's multi_logloss: 0.153472\tvalid_1's multi_logloss: 0.633836\n",
      "[931]\ttraining's multi_logloss: 0.153214\tvalid_1's multi_logloss: 0.633736\n",
      "[932]\ttraining's multi_logloss: 0.152955\tvalid_1's multi_logloss: 0.633704\n",
      "[933]\ttraining's multi_logloss: 0.152682\tvalid_1's multi_logloss: 0.633682\n",
      "[934]\ttraining's multi_logloss: 0.152396\tvalid_1's multi_logloss: 0.633607\n",
      "[935]\ttraining's multi_logloss: 0.152107\tvalid_1's multi_logloss: 0.633502\n",
      "[936]\ttraining's multi_logloss: 0.151846\tvalid_1's multi_logloss: 0.633458\n",
      "[937]\ttraining's multi_logloss: 0.151591\tvalid_1's multi_logloss: 0.633385\n",
      "[938]\ttraining's multi_logloss: 0.151338\tvalid_1's multi_logloss: 0.633272\n",
      "[939]\ttraining's multi_logloss: 0.151051\tvalid_1's multi_logloss: 0.633231\n",
      "[940]\ttraining's multi_logloss: 0.15077\tvalid_1's multi_logloss: 0.633103\n",
      "[941]\ttraining's multi_logloss: 0.1505\tvalid_1's multi_logloss: 0.633012\n",
      "[942]\ttraining's multi_logloss: 0.150225\tvalid_1's multi_logloss: 0.632959\n",
      "[943]\ttraining's multi_logloss: 0.149951\tvalid_1's multi_logloss: 0.632905\n",
      "[944]\ttraining's multi_logloss: 0.14969\tvalid_1's multi_logloss: 0.632879\n",
      "[945]\ttraining's multi_logloss: 0.149436\tvalid_1's multi_logloss: 0.6328\n",
      "[946]\ttraining's multi_logloss: 0.149175\tvalid_1's multi_logloss: 0.632747\n",
      "[947]\ttraining's multi_logloss: 0.148942\tvalid_1's multi_logloss: 0.632696\n",
      "[948]\ttraining's multi_logloss: 0.148682\tvalid_1's multi_logloss: 0.6326\n",
      "[949]\ttraining's multi_logloss: 0.148418\tvalid_1's multi_logloss: 0.632532\n",
      "[950]\ttraining's multi_logloss: 0.148215\tvalid_1's multi_logloss: 0.632509\n",
      "[951]\ttraining's multi_logloss: 0.147944\tvalid_1's multi_logloss: 0.632441\n",
      "[952]\ttraining's multi_logloss: 0.14772\tvalid_1's multi_logloss: 0.632358\n",
      "[953]\ttraining's multi_logloss: 0.147478\tvalid_1's multi_logloss: 0.63229\n",
      "[954]\ttraining's multi_logloss: 0.147223\tvalid_1's multi_logloss: 0.632169\n",
      "[955]\ttraining's multi_logloss: 0.146945\tvalid_1's multi_logloss: 0.632136\n",
      "[956]\ttraining's multi_logloss: 0.146721\tvalid_1's multi_logloss: 0.632143\n",
      "[957]\ttraining's multi_logloss: 0.146455\tvalid_1's multi_logloss: 0.632082\n",
      "[958]\ttraining's multi_logloss: 0.14621\tvalid_1's multi_logloss: 0.63203\n",
      "[959]\ttraining's multi_logloss: 0.145949\tvalid_1's multi_logloss: 0.631985\n",
      "[960]\ttraining's multi_logloss: 0.145709\tvalid_1's multi_logloss: 0.631889\n",
      "[961]\ttraining's multi_logloss: 0.145456\tvalid_1's multi_logloss: 0.631761\n",
      "[962]\ttraining's multi_logloss: 0.1452\tvalid_1's multi_logloss: 0.631698\n",
      "[963]\ttraining's multi_logloss: 0.144953\tvalid_1's multi_logloss: 0.631709\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[964]\ttraining's multi_logloss: 0.144747\tvalid_1's multi_logloss: 0.631633\n",
      "[965]\ttraining's multi_logloss: 0.144503\tvalid_1's multi_logloss: 0.631588\n",
      "[966]\ttraining's multi_logloss: 0.144272\tvalid_1's multi_logloss: 0.631578\n",
      "[967]\ttraining's multi_logloss: 0.143999\tvalid_1's multi_logloss: 0.631518\n",
      "[968]\ttraining's multi_logloss: 0.143768\tvalid_1's multi_logloss: 0.631463\n",
      "[969]\ttraining's multi_logloss: 0.143524\tvalid_1's multi_logloss: 0.631384\n",
      "[970]\ttraining's multi_logloss: 0.143279\tvalid_1's multi_logloss: 0.631272\n",
      "[971]\ttraining's multi_logloss: 0.143035\tvalid_1's multi_logloss: 0.631236\n",
      "[972]\ttraining's multi_logloss: 0.142776\tvalid_1's multi_logloss: 0.631115\n",
      "[973]\ttraining's multi_logloss: 0.142564\tvalid_1's multi_logloss: 0.631031\n",
      "[974]\ttraining's multi_logloss: 0.142309\tvalid_1's multi_logloss: 0.630944\n",
      "[975]\ttraining's multi_logloss: 0.142081\tvalid_1's multi_logloss: 0.63086\n",
      "[976]\ttraining's multi_logloss: 0.14185\tvalid_1's multi_logloss: 0.630769\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[977]\ttraining's multi_logloss: 0.141637\tvalid_1's multi_logloss: 0.630704\n",
      "[978]\ttraining's multi_logloss: 0.141434\tvalid_1's multi_logloss: 0.630639\n",
      "[979]\ttraining's multi_logloss: 0.141192\tvalid_1's multi_logloss: 0.630583\n",
      "[980]\ttraining's multi_logloss: 0.140944\tvalid_1's multi_logloss: 0.630474\n",
      "[981]\ttraining's multi_logloss: 0.140693\tvalid_1's multi_logloss: 0.630451\n",
      "[982]\ttraining's multi_logloss: 0.140461\tvalid_1's multi_logloss: 0.630372\n",
      "[983]\ttraining's multi_logloss: 0.14021\tvalid_1's multi_logloss: 0.630321\n",
      "[984]\ttraining's multi_logloss: 0.139962\tvalid_1's multi_logloss: 0.630297\n",
      "[985]\ttraining's multi_logloss: 0.139712\tvalid_1's multi_logloss: 0.630227\n",
      "[986]\ttraining's multi_logloss: 0.139494\tvalid_1's multi_logloss: 0.630244\n",
      "[987]\ttraining's multi_logloss: 0.139268\tvalid_1's multi_logloss: 0.630237\n",
      "[988]\ttraining's multi_logloss: 0.139053\tvalid_1's multi_logloss: 0.63015\n",
      "[989]\ttraining's multi_logloss: 0.138815\tvalid_1's multi_logloss: 0.630119\n",
      "[990]\ttraining's multi_logloss: 0.138581\tvalid_1's multi_logloss: 0.630056\n",
      "[991]\ttraining's multi_logloss: 0.13836\tvalid_1's multi_logloss: 0.630021\n",
      "[992]\ttraining's multi_logloss: 0.138114\tvalid_1's multi_logloss: 0.62998\n",
      "[993]\ttraining's multi_logloss: 0.137853\tvalid_1's multi_logloss: 0.629895\n",
      "[994]\ttraining's multi_logloss: 0.137618\tvalid_1's multi_logloss: 0.629802\n",
      "[995]\ttraining's multi_logloss: 0.137378\tvalid_1's multi_logloss: 0.629775\n",
      "[996]\ttraining's multi_logloss: 0.137138\tvalid_1's multi_logloss: 0.629648\n",
      "[997]\ttraining's multi_logloss: 0.136922\tvalid_1's multi_logloss: 0.629591\n",
      "[998]\ttraining's multi_logloss: 0.136694\tvalid_1's multi_logloss: 0.629514\n",
      "[999]\ttraining's multi_logloss: 0.136453\tvalid_1's multi_logloss: 0.62938\n",
      "[1000]\ttraining's multi_logloss: 0.136212\tvalid_1's multi_logloss: 0.629299\n",
      "[1001]\ttraining's multi_logloss: 0.135966\tvalid_1's multi_logloss: 0.629241\n",
      "[1002]\ttraining's multi_logloss: 0.135736\tvalid_1's multi_logloss: 0.629171\n",
      "[1003]\ttraining's multi_logloss: 0.135533\tvalid_1's multi_logloss: 0.629059\n",
      "[1004]\ttraining's multi_logloss: 0.135334\tvalid_1's multi_logloss: 0.628948\n",
      "[1005]\ttraining's multi_logloss: 0.135103\tvalid_1's multi_logloss: 0.628951\n",
      "[1006]\ttraining's multi_logloss: 0.134875\tvalid_1's multi_logloss: 0.628845\n",
      "[1007]\ttraining's multi_logloss: 0.134658\tvalid_1's multi_logloss: 0.628787\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1008]\ttraining's multi_logloss: 0.134468\tvalid_1's multi_logloss: 0.628707\n",
      "[1009]\ttraining's multi_logloss: 0.134241\tvalid_1's multi_logloss: 0.628634\n",
      "[1010]\ttraining's multi_logloss: 0.134053\tvalid_1's multi_logloss: 0.628608\n",
      "[1011]\ttraining's multi_logloss: 0.13382\tvalid_1's multi_logloss: 0.628589\n",
      "[1012]\ttraining's multi_logloss: 0.133629\tvalid_1's multi_logloss: 0.6285\n",
      "[1013]\ttraining's multi_logloss: 0.133398\tvalid_1's multi_logloss: 0.628415\n",
      "[1014]\ttraining's multi_logloss: 0.1332\tvalid_1's multi_logloss: 0.628399\n",
      "[1015]\ttraining's multi_logloss: 0.132986\tvalid_1's multi_logloss: 0.628371\n",
      "[1016]\ttraining's multi_logloss: 0.132768\tvalid_1's multi_logloss: 0.628336\n",
      "[1017]\ttraining's multi_logloss: 0.132537\tvalid_1's multi_logloss: 0.628278\n",
      "[1018]\ttraining's multi_logloss: 0.132313\tvalid_1's multi_logloss: 0.628193\n",
      "[1019]\ttraining's multi_logloss: 0.132088\tvalid_1's multi_logloss: 0.628147\n",
      "[1020]\ttraining's multi_logloss: 0.131857\tvalid_1's multi_logloss: 0.628078\n",
      "[1021]\ttraining's multi_logloss: 0.131636\tvalid_1's multi_logloss: 0.628052\n",
      "[1022]\ttraining's multi_logloss: 0.131447\tvalid_1's multi_logloss: 0.627925\n",
      "[1023]\ttraining's multi_logloss: 0.131212\tvalid_1's multi_logloss: 0.627847\n",
      "[1024]\ttraining's multi_logloss: 0.13099\tvalid_1's multi_logloss: 0.627767\n",
      "[1025]\ttraining's multi_logloss: 0.130761\tvalid_1's multi_logloss: 0.627753\n",
      "[1026]\ttraining's multi_logloss: 0.130583\tvalid_1's multi_logloss: 0.627702\n",
      "[1027]\ttraining's multi_logloss: 0.130367\tvalid_1's multi_logloss: 0.627618\n",
      "[1028]\ttraining's multi_logloss: 0.130147\tvalid_1's multi_logloss: 0.627559\n",
      "[1029]\ttraining's multi_logloss: 0.129934\tvalid_1's multi_logloss: 0.627503\n",
      "[1030]\ttraining's multi_logloss: 0.129713\tvalid_1's multi_logloss: 0.627463\n",
      "[1031]\ttraining's multi_logloss: 0.129524\tvalid_1's multi_logloss: 0.627347\n",
      "[1032]\ttraining's multi_logloss: 0.129296\tvalid_1's multi_logloss: 0.627189\n",
      "[1033]\ttraining's multi_logloss: 0.129079\tvalid_1's multi_logloss: 0.627077\n",
      "[1034]\ttraining's multi_logloss: 0.12886\tvalid_1's multi_logloss: 0.627042\n",
      "[1035]\ttraining's multi_logloss: 0.128659\tvalid_1's multi_logloss: 0.626983\n",
      "[1036]\ttraining's multi_logloss: 0.128461\tvalid_1's multi_logloss: 0.626915\n",
      "[1037]\ttraining's multi_logloss: 0.12826\tvalid_1's multi_logloss: 0.626898\n",
      "[1038]\ttraining's multi_logloss: 0.12804\tvalid_1's multi_logloss: 0.626858\n",
      "[1039]\ttraining's multi_logloss: 0.12781\tvalid_1's multi_logloss: 0.626859\n",
      "[1040]\ttraining's multi_logloss: 0.127589\tvalid_1's multi_logloss: 0.626779\n",
      "[1041]\ttraining's multi_logloss: 0.127367\tvalid_1's multi_logloss: 0.62674\n",
      "[1042]\ttraining's multi_logloss: 0.127155\tvalid_1's multi_logloss: 0.626679\n",
      "[1043]\ttraining's multi_logloss: 0.126961\tvalid_1's multi_logloss: 0.626572\n",
      "[1044]\ttraining's multi_logloss: 0.126745\tvalid_1's multi_logloss: 0.626496\n",
      "[1045]\ttraining's multi_logloss: 0.126523\tvalid_1's multi_logloss: 0.626417\n",
      "[1046]\ttraining's multi_logloss: 0.126322\tvalid_1's multi_logloss: 0.626368\n",
      "[1047]\ttraining's multi_logloss: 0.126096\tvalid_1's multi_logloss: 0.626295\n",
      "[1048]\ttraining's multi_logloss: 0.125877\tvalid_1's multi_logloss: 0.626236\n",
      "[1049]\ttraining's multi_logloss: 0.125678\tvalid_1's multi_logloss: 0.626214\n",
      "[1050]\ttraining's multi_logloss: 0.125469\tvalid_1's multi_logloss: 0.626182\n",
      "[1051]\ttraining's multi_logloss: 0.125289\tvalid_1's multi_logloss: 0.626065\n",
      "[1052]\ttraining's multi_logloss: 0.125091\tvalid_1's multi_logloss: 0.62603\n",
      "[1053]\ttraining's multi_logloss: 0.124884\tvalid_1's multi_logloss: 0.62593\n",
      "[1054]\ttraining's multi_logloss: 0.124664\tvalid_1's multi_logloss: 0.625871\n",
      "[1055]\ttraining's multi_logloss: 0.124453\tvalid_1's multi_logloss: 0.625748\n",
      "[1056]\ttraining's multi_logloss: 0.124242\tvalid_1's multi_logloss: 0.62563\n",
      "[1057]\ttraining's multi_logloss: 0.124044\tvalid_1's multi_logloss: 0.625544\n",
      "[1058]\ttraining's multi_logloss: 0.123841\tvalid_1's multi_logloss: 0.625464\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1059]\ttraining's multi_logloss: 0.123663\tvalid_1's multi_logloss: 0.6254\n",
      "[1060]\ttraining's multi_logloss: 0.123451\tvalid_1's multi_logloss: 0.62538\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1061]\ttraining's multi_logloss: 0.123279\tvalid_1's multi_logloss: 0.625283\n",
      "[1062]\ttraining's multi_logloss: 0.123087\tvalid_1's multi_logloss: 0.625163\n",
      "[1063]\ttraining's multi_logloss: 0.122884\tvalid_1's multi_logloss: 0.625138\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1064]\ttraining's multi_logloss: 0.122705\tvalid_1's multi_logloss: 0.625044\n",
      "[1065]\ttraining's multi_logloss: 0.122512\tvalid_1's multi_logloss: 0.624964\n",
      "[1066]\ttraining's multi_logloss: 0.122312\tvalid_1's multi_logloss: 0.624935\n",
      "[1067]\ttraining's multi_logloss: 0.12212\tvalid_1's multi_logloss: 0.624827\n",
      "[1068]\ttraining's multi_logloss: 0.121946\tvalid_1's multi_logloss: 0.624749\n",
      "[1069]\ttraining's multi_logloss: 0.12174\tvalid_1's multi_logloss: 0.624672\n",
      "[1070]\ttraining's multi_logloss: 0.12152\tvalid_1's multi_logloss: 0.624553\n",
      "[1071]\ttraining's multi_logloss: 0.121328\tvalid_1's multi_logloss: 0.624494\n",
      "[1072]\ttraining's multi_logloss: 0.121175\tvalid_1's multi_logloss: 0.624414\n",
      "[1073]\ttraining's multi_logloss: 0.120978\tvalid_1's multi_logloss: 0.624365\n",
      "[1074]\ttraining's multi_logloss: 0.120798\tvalid_1's multi_logloss: 0.624269\n",
      "[1075]\ttraining's multi_logloss: 0.120593\tvalid_1's multi_logloss: 0.624236\n",
      "[1076]\ttraining's multi_logloss: 0.1204\tvalid_1's multi_logloss: 0.62417\n",
      "[1077]\ttraining's multi_logloss: 0.120186\tvalid_1's multi_logloss: 0.624108\n",
      "[1078]\ttraining's multi_logloss: 0.119992\tvalid_1's multi_logloss: 0.624007\n",
      "[1079]\ttraining's multi_logloss: 0.119804\tvalid_1's multi_logloss: 0.62399\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1080]\ttraining's multi_logloss: 0.119641\tvalid_1's multi_logloss: 0.623976\n",
      "[1081]\ttraining's multi_logloss: 0.119466\tvalid_1's multi_logloss: 0.623953\n",
      "[1082]\ttraining's multi_logloss: 0.119266\tvalid_1's multi_logloss: 0.623901\n",
      "[1083]\ttraining's multi_logloss: 0.119077\tvalid_1's multi_logloss: 0.62385\n",
      "[1084]\ttraining's multi_logloss: 0.118873\tvalid_1's multi_logloss: 0.623766\n",
      "[1085]\ttraining's multi_logloss: 0.118683\tvalid_1's multi_logloss: 0.623755\n",
      "[1086]\ttraining's multi_logloss: 0.118484\tvalid_1's multi_logloss: 0.623753\n",
      "[1087]\ttraining's multi_logloss: 0.118287\tvalid_1's multi_logloss: 0.623725\n",
      "[1088]\ttraining's multi_logloss: 0.118072\tvalid_1's multi_logloss: 0.623615\n",
      "[1089]\ttraining's multi_logloss: 0.117911\tvalid_1's multi_logloss: 0.623576\n",
      "[1090]\ttraining's multi_logloss: 0.117705\tvalid_1's multi_logloss: 0.623524\n",
      "[1091]\ttraining's multi_logloss: 0.1175\tvalid_1's multi_logloss: 0.623419\n",
      "[1092]\ttraining's multi_logloss: 0.1173\tvalid_1's multi_logloss: 0.623309\n",
      "[1093]\ttraining's multi_logloss: 0.11712\tvalid_1's multi_logloss: 0.623313\n",
      "[1094]\ttraining's multi_logloss: 0.116932\tvalid_1's multi_logloss: 0.623268\n",
      "[1095]\ttraining's multi_logloss: 0.116741\tvalid_1's multi_logloss: 0.623268\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1096]\ttraining's multi_logloss: 0.116574\tvalid_1's multi_logloss: 0.623308\n",
      "[1097]\ttraining's multi_logloss: 0.116387\tvalid_1's multi_logloss: 0.623276\n",
      "[1098]\ttraining's multi_logloss: 0.116216\tvalid_1's multi_logloss: 0.623236\n",
      "[1099]\ttraining's multi_logloss: 0.116044\tvalid_1's multi_logloss: 0.623172\n",
      "[1100]\ttraining's multi_logloss: 0.115855\tvalid_1's multi_logloss: 0.623148\n",
      "[1101]\ttraining's multi_logloss: 0.115664\tvalid_1's multi_logloss: 0.623085\n",
      "[1102]\ttraining's multi_logloss: 0.115496\tvalid_1's multi_logloss: 0.623051\n",
      "[1103]\ttraining's multi_logloss: 0.115307\tvalid_1's multi_logloss: 0.623016\n",
      "[1104]\ttraining's multi_logloss: 0.115145\tvalid_1's multi_logloss: 0.622988\n",
      "[1105]\ttraining's multi_logloss: 0.114958\tvalid_1's multi_logloss: 0.622915\n",
      "[1106]\ttraining's multi_logloss: 0.114775\tvalid_1's multi_logloss: 0.622887\n",
      "[1107]\ttraining's multi_logloss: 0.114582\tvalid_1's multi_logloss: 0.622885\n",
      "[1108]\ttraining's multi_logloss: 0.114403\tvalid_1's multi_logloss: 0.622901\n",
      "[1109]\ttraining's multi_logloss: 0.114211\tvalid_1's multi_logloss: 0.622853\n",
      "[1110]\ttraining's multi_logloss: 0.114018\tvalid_1's multi_logloss: 0.622763\n",
      "[1111]\ttraining's multi_logloss: 0.113846\tvalid_1's multi_logloss: 0.622691\n",
      "[1112]\ttraining's multi_logloss: 0.113666\tvalid_1's multi_logloss: 0.62267\n",
      "[1113]\ttraining's multi_logloss: 0.11345\tvalid_1's multi_logloss: 0.622596\n",
      "[1114]\ttraining's multi_logloss: 0.113256\tvalid_1's multi_logloss: 0.622547\n",
      "[1115]\ttraining's multi_logloss: 0.113065\tvalid_1's multi_logloss: 0.622505\n",
      "[1116]\ttraining's multi_logloss: 0.112864\tvalid_1's multi_logloss: 0.622486\n",
      "[1117]\ttraining's multi_logloss: 0.112657\tvalid_1's multi_logloss: 0.622441\n",
      "[1118]\ttraining's multi_logloss: 0.112466\tvalid_1's multi_logloss: 0.622374\n",
      "[1119]\ttraining's multi_logloss: 0.112278\tvalid_1's multi_logloss: 0.622322\n",
      "[1120]\ttraining's multi_logloss: 0.112086\tvalid_1's multi_logloss: 0.622296\n",
      "[1121]\ttraining's multi_logloss: 0.111893\tvalid_1's multi_logloss: 0.622258\n",
      "[1122]\ttraining's multi_logloss: 0.1117\tvalid_1's multi_logloss: 0.622227\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1123]\ttraining's multi_logloss: 0.111545\tvalid_1's multi_logloss: 0.622165\n",
      "[1124]\ttraining's multi_logloss: 0.111353\tvalid_1's multi_logloss: 0.622192\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1125]\ttraining's multi_logloss: 0.11119\tvalid_1's multi_logloss: 0.622136\n",
      "[1126]\ttraining's multi_logloss: 0.111008\tvalid_1's multi_logloss: 0.622152\n",
      "[1127]\ttraining's multi_logloss: 0.110817\tvalid_1's multi_logloss: 0.622059\n",
      "[1128]\ttraining's multi_logloss: 0.110629\tvalid_1's multi_logloss: 0.62205\n",
      "[1129]\ttraining's multi_logloss: 0.110429\tvalid_1's multi_logloss: 0.622046\n",
      "[1130]\ttraining's multi_logloss: 0.110241\tvalid_1's multi_logloss: 0.621975\n",
      "[1131]\ttraining's multi_logloss: 0.11005\tvalid_1's multi_logloss: 0.621974\n",
      "[1132]\ttraining's multi_logloss: 0.109868\tvalid_1's multi_logloss: 0.621976\n",
      "[1133]\ttraining's multi_logloss: 0.10968\tvalid_1's multi_logloss: 0.621892\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1134]\ttraining's multi_logloss: 0.109516\tvalid_1's multi_logloss: 0.621809\n",
      "[1135]\ttraining's multi_logloss: 0.10935\tvalid_1's multi_logloss: 0.621764\n",
      "[1136]\ttraining's multi_logloss: 0.109188\tvalid_1's multi_logloss: 0.621687\n",
      "[1137]\ttraining's multi_logloss: 0.109023\tvalid_1's multi_logloss: 0.621617\n",
      "[1138]\ttraining's multi_logloss: 0.108845\tvalid_1's multi_logloss: 0.62159\n",
      "[1139]\ttraining's multi_logloss: 0.108647\tvalid_1's multi_logloss: 0.621561\n",
      "[1140]\ttraining's multi_logloss: 0.108509\tvalid_1's multi_logloss: 0.621487\n",
      "[1141]\ttraining's multi_logloss: 0.108336\tvalid_1's multi_logloss: 0.621463\n",
      "[1142]\ttraining's multi_logloss: 0.108149\tvalid_1's multi_logloss: 0.621475\n",
      "[1143]\ttraining's multi_logloss: 0.10798\tvalid_1's multi_logloss: 0.621468\n",
      "[1144]\ttraining's multi_logloss: 0.107783\tvalid_1's multi_logloss: 0.621399\n",
      "[1145]\ttraining's multi_logloss: 0.107652\tvalid_1's multi_logloss: 0.621354\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1146]\ttraining's multi_logloss: 0.107503\tvalid_1's multi_logloss: 0.621291\n",
      "[1147]\ttraining's multi_logloss: 0.107335\tvalid_1's multi_logloss: 0.621269\n",
      "[1148]\ttraining's multi_logloss: 0.107181\tvalid_1's multi_logloss: 0.621241\n",
      "[1149]\ttraining's multi_logloss: 0.107046\tvalid_1's multi_logloss: 0.621211\n",
      "[1150]\ttraining's multi_logloss: 0.106862\tvalid_1's multi_logloss: 0.621188\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1151]\ttraining's multi_logloss: 0.106713\tvalid_1's multi_logloss: 0.621192\n",
      "[1152]\ttraining's multi_logloss: 0.106559\tvalid_1's multi_logloss: 0.621146\n",
      "[1153]\ttraining's multi_logloss: 0.106409\tvalid_1's multi_logloss: 0.621095\n",
      "[1154]\ttraining's multi_logloss: 0.106247\tvalid_1's multi_logloss: 0.621038\n",
      "[1155]\ttraining's multi_logloss: 0.106062\tvalid_1's multi_logloss: 0.620968\n",
      "[1156]\ttraining's multi_logloss: 0.105899\tvalid_1's multi_logloss: 0.620943\n",
      "[1157]\ttraining's multi_logloss: 0.105732\tvalid_1's multi_logloss: 0.620876\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1158]\ttraining's multi_logloss: 0.105572\tvalid_1's multi_logloss: 0.62083\n",
      "[1159]\ttraining's multi_logloss: 0.105387\tvalid_1's multi_logloss: 0.620755\n",
      "[1160]\ttraining's multi_logloss: 0.105212\tvalid_1's multi_logloss: 0.620742\n",
      "[1161]\ttraining's multi_logloss: 0.105044\tvalid_1's multi_logloss: 0.62067\n",
      "[1162]\ttraining's multi_logloss: 0.104867\tvalid_1's multi_logloss: 0.620668\n",
      "[1163]\ttraining's multi_logloss: 0.104712\tvalid_1's multi_logloss: 0.620625\n",
      "[1164]\ttraining's multi_logloss: 0.104559\tvalid_1's multi_logloss: 0.620632\n",
      "[1165]\ttraining's multi_logloss: 0.104412\tvalid_1's multi_logloss: 0.620596\n",
      "[1166]\ttraining's multi_logloss: 0.104246\tvalid_1's multi_logloss: 0.620615\n",
      "[1167]\ttraining's multi_logloss: 0.104084\tvalid_1's multi_logloss: 0.620556\n",
      "[1168]\ttraining's multi_logloss: 0.103912\tvalid_1's multi_logloss: 0.620509\n",
      "[1169]\ttraining's multi_logloss: 0.103737\tvalid_1's multi_logloss: 0.620465\n",
      "[1170]\ttraining's multi_logloss: 0.10355\tvalid_1's multi_logloss: 0.620391\n",
      "[1171]\ttraining's multi_logloss: 0.103377\tvalid_1's multi_logloss: 0.620354\n",
      "[1172]\ttraining's multi_logloss: 0.103201\tvalid_1's multi_logloss: 0.620329\n",
      "[1173]\ttraining's multi_logloss: 0.103048\tvalid_1's multi_logloss: 0.620347\n",
      "[1174]\ttraining's multi_logloss: 0.102884\tvalid_1's multi_logloss: 0.620284\n",
      "[1175]\ttraining's multi_logloss: 0.102738\tvalid_1's multi_logloss: 0.62021\n",
      "[1176]\ttraining's multi_logloss: 0.102573\tvalid_1's multi_logloss: 0.620242\n",
      "[1177]\ttraining's multi_logloss: 0.102413\tvalid_1's multi_logloss: 0.620246\n",
      "[1178]\ttraining's multi_logloss: 0.102275\tvalid_1's multi_logloss: 0.620181\n",
      "[1179]\ttraining's multi_logloss: 0.102125\tvalid_1's multi_logloss: 0.620154\n",
      "[1180]\ttraining's multi_logloss: 0.101974\tvalid_1's multi_logloss: 0.620148\n",
      "[1181]\ttraining's multi_logloss: 0.101809\tvalid_1's multi_logloss: 0.620134\n",
      "[1182]\ttraining's multi_logloss: 0.101661\tvalid_1's multi_logloss: 0.620109\n",
      "[1183]\ttraining's multi_logloss: 0.101511\tvalid_1's multi_logloss: 0.620033\n",
      "[1184]\ttraining's multi_logloss: 0.101335\tvalid_1's multi_logloss: 0.619948\n",
      "[1185]\ttraining's multi_logloss: 0.101193\tvalid_1's multi_logloss: 0.619915\n",
      "[1186]\ttraining's multi_logloss: 0.101054\tvalid_1's multi_logloss: 0.619918\n",
      "[1187]\ttraining's multi_logloss: 0.100881\tvalid_1's multi_logloss: 0.619897\n",
      "[1188]\ttraining's multi_logloss: 0.100721\tvalid_1's multi_logloss: 0.619893\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1189]\ttraining's multi_logloss: 0.100587\tvalid_1's multi_logloss: 0.619847\n",
      "[1190]\ttraining's multi_logloss: 0.100433\tvalid_1's multi_logloss: 0.619803\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1191]\ttraining's multi_logloss: 0.100295\tvalid_1's multi_logloss: 0.619823\n",
      "[1192]\ttraining's multi_logloss: 0.100158\tvalid_1's multi_logloss: 0.619759\n",
      "[1193]\ttraining's multi_logloss: 0.1\tvalid_1's multi_logloss: 0.61976\n",
      "[1194]\ttraining's multi_logloss: 0.0998653\tvalid_1's multi_logloss: 0.61971\n",
      "[1195]\ttraining's multi_logloss: 0.0996974\tvalid_1's multi_logloss: 0.619675\n",
      "[1196]\ttraining's multi_logloss: 0.0995575\tvalid_1's multi_logloss: 0.619564\n",
      "[1197]\ttraining's multi_logloss: 0.0994063\tvalid_1's multi_logloss: 0.619552\n",
      "[1198]\ttraining's multi_logloss: 0.0992418\tvalid_1's multi_logloss: 0.619471\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1199]\ttraining's multi_logloss: 0.0991025\tvalid_1's multi_logloss: 0.619437\n",
      "[1200]\ttraining's multi_logloss: 0.0989281\tvalid_1's multi_logloss: 0.61938\n",
      "[1201]\ttraining's multi_logloss: 0.0987667\tvalid_1's multi_logloss: 0.619417\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1202]\ttraining's multi_logloss: 0.0986445\tvalid_1's multi_logloss: 0.619396\n",
      "[1203]\ttraining's multi_logloss: 0.0984987\tvalid_1's multi_logloss: 0.619332\n",
      "[1204]\ttraining's multi_logloss: 0.0983336\tvalid_1's multi_logloss: 0.619291\n",
      "[1205]\ttraining's multi_logloss: 0.0981557\tvalid_1's multi_logloss: 0.619208\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1206]\ttraining's multi_logloss: 0.0980174\tvalid_1's multi_logloss: 0.619138\n",
      "[1207]\ttraining's multi_logloss: 0.0978678\tvalid_1's multi_logloss: 0.619079\n",
      "[1208]\ttraining's multi_logloss: 0.0977371\tvalid_1's multi_logloss: 0.61903\n",
      "[1209]\ttraining's multi_logloss: 0.0975582\tvalid_1's multi_logloss: 0.618921\n",
      "[1210]\ttraining's multi_logloss: 0.097396\tvalid_1's multi_logloss: 0.618862\n",
      "[1211]\ttraining's multi_logloss: 0.0972498\tvalid_1's multi_logloss: 0.618836\n",
      "[1212]\ttraining's multi_logloss: 0.0970891\tvalid_1's multi_logloss: 0.618805\n",
      "[1213]\ttraining's multi_logloss: 0.0969558\tvalid_1's multi_logloss: 0.618772\n",
      "[1214]\ttraining's multi_logloss: 0.096805\tvalid_1's multi_logloss: 0.618672\n",
      "[1215]\ttraining's multi_logloss: 0.0966393\tvalid_1's multi_logloss: 0.618641\n",
      "[1216]\ttraining's multi_logloss: 0.0964774\tvalid_1's multi_logloss: 0.618623\n",
      "[1217]\ttraining's multi_logloss: 0.0963154\tvalid_1's multi_logloss: 0.618584\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1218]\ttraining's multi_logloss: 0.0961914\tvalid_1's multi_logloss: 0.61857\n",
      "[1219]\ttraining's multi_logloss: 0.0960251\tvalid_1's multi_logloss: 0.618541\n",
      "[1220]\ttraining's multi_logloss: 0.0958669\tvalid_1's multi_logloss: 0.618497\n",
      "[1221]\ttraining's multi_logloss: 0.0957276\tvalid_1's multi_logloss: 0.618455\n",
      "[1222]\ttraining's multi_logloss: 0.0955672\tvalid_1's multi_logloss: 0.618435\n",
      "[1223]\ttraining's multi_logloss: 0.0954062\tvalid_1's multi_logloss: 0.618377\n",
      "[1224]\ttraining's multi_logloss: 0.0952777\tvalid_1's multi_logloss: 0.61835\n",
      "[1225]\ttraining's multi_logloss: 0.0951088\tvalid_1's multi_logloss: 0.618323\n",
      "[1226]\ttraining's multi_logloss: 0.094948\tvalid_1's multi_logloss: 0.618279\n",
      "[1227]\ttraining's multi_logloss: 0.094805\tvalid_1's multi_logloss: 0.618273\n",
      "[1228]\ttraining's multi_logloss: 0.0946618\tvalid_1's multi_logloss: 0.618269\n",
      "[1229]\ttraining's multi_logloss: 0.0945062\tvalid_1's multi_logloss: 0.618231\n",
      "[1230]\ttraining's multi_logloss: 0.0943581\tvalid_1's multi_logloss: 0.618247\n",
      "[1231]\ttraining's multi_logloss: 0.0942159\tvalid_1's multi_logloss: 0.618228\n",
      "[1232]\ttraining's multi_logloss: 0.0940568\tvalid_1's multi_logloss: 0.618231\n",
      "[1233]\ttraining's multi_logloss: 0.0939024\tvalid_1's multi_logloss: 0.618153\n",
      "[1234]\ttraining's multi_logloss: 0.0937336\tvalid_1's multi_logloss: 0.61811\n",
      "[1235]\ttraining's multi_logloss: 0.0935909\tvalid_1's multi_logloss: 0.618055\n",
      "[1236]\ttraining's multi_logloss: 0.0934347\tvalid_1's multi_logloss: 0.617995\n",
      "[1237]\ttraining's multi_logloss: 0.0932937\tvalid_1's multi_logloss: 0.617942\n",
      "[1238]\ttraining's multi_logloss: 0.0931243\tvalid_1's multi_logloss: 0.617888\n",
      "[1239]\ttraining's multi_logloss: 0.0929687\tvalid_1's multi_logloss: 0.617889\n",
      "[1240]\ttraining's multi_logloss: 0.0928121\tvalid_1's multi_logloss: 0.617855\n",
      "[1241]\ttraining's multi_logloss: 0.0926616\tvalid_1's multi_logloss: 0.617832\n",
      "[1242]\ttraining's multi_logloss: 0.0925011\tvalid_1's multi_logloss: 0.617795\n",
      "[1243]\ttraining's multi_logloss: 0.0923566\tvalid_1's multi_logloss: 0.617797\n",
      "[1244]\ttraining's multi_logloss: 0.0922038\tvalid_1's multi_logloss: 0.617741\n",
      "[1245]\ttraining's multi_logloss: 0.0920359\tvalid_1's multi_logloss: 0.617721\n",
      "[1246]\ttraining's multi_logloss: 0.0918841\tvalid_1's multi_logloss: 0.617678\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1247]\ttraining's multi_logloss: 0.0917542\tvalid_1's multi_logloss: 0.617698\n",
      "[1248]\ttraining's multi_logloss: 0.0915981\tvalid_1's multi_logloss: 0.617672\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1249]\ttraining's multi_logloss: 0.091464\tvalid_1's multi_logloss: 0.61768\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1250]\ttraining's multi_logloss: 0.0913302\tvalid_1's multi_logloss: 0.617612\n",
      "[1251]\ttraining's multi_logloss: 0.0911796\tvalid_1's multi_logloss: 0.61758\n",
      "[1252]\ttraining's multi_logloss: 0.0910476\tvalid_1's multi_logloss: 0.617494\n",
      "[1253]\ttraining's multi_logloss: 0.0909012\tvalid_1's multi_logloss: 0.617431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1254]\ttraining's multi_logloss: 0.090768\tvalid_1's multi_logloss: 0.617386\n",
      "[1255]\ttraining's multi_logloss: 0.0906015\tvalid_1's multi_logloss: 0.617347\n",
      "[1256]\ttraining's multi_logloss: 0.0904436\tvalid_1's multi_logloss: 0.617325\n",
      "[1257]\ttraining's multi_logloss: 0.0903129\tvalid_1's multi_logloss: 0.617281\n",
      "[1258]\ttraining's multi_logloss: 0.0901484\tvalid_1's multi_logloss: 0.61725\n",
      "[1259]\ttraining's multi_logloss: 0.0900033\tvalid_1's multi_logloss: 0.617264\n",
      "[1260]\ttraining's multi_logloss: 0.0898516\tvalid_1's multi_logloss: 0.617307\n",
      "[1261]\ttraining's multi_logloss: 0.0897077\tvalid_1's multi_logloss: 0.617291\n",
      "[1262]\ttraining's multi_logloss: 0.0895493\tvalid_1's multi_logloss: 0.617274\n",
      "[1263]\ttraining's multi_logloss: 0.0894174\tvalid_1's multi_logloss: 0.617297\n",
      "[1264]\ttraining's multi_logloss: 0.0892529\tvalid_1's multi_logloss: 0.617265\n",
      "[1265]\ttraining's multi_logloss: 0.0891041\tvalid_1's multi_logloss: 0.617249\n",
      "[1266]\ttraining's multi_logloss: 0.0889615\tvalid_1's multi_logloss: 0.617219\n",
      "[1267]\ttraining's multi_logloss: 0.0888358\tvalid_1's multi_logloss: 0.617188\n",
      "[1268]\ttraining's multi_logloss: 0.0886839\tvalid_1's multi_logloss: 0.617187\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1269]\ttraining's multi_logloss: 0.0885702\tvalid_1's multi_logloss: 0.617119\n",
      "[1270]\ttraining's multi_logloss: 0.0884249\tvalid_1's multi_logloss: 0.617063\n",
      "[1271]\ttraining's multi_logloss: 0.0882779\tvalid_1's multi_logloss: 0.617017\n",
      "[1272]\ttraining's multi_logloss: 0.0881235\tvalid_1's multi_logloss: 0.617025\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1273]\ttraining's multi_logloss: 0.0880164\tvalid_1's multi_logloss: 0.616985\n",
      "[1274]\ttraining's multi_logloss: 0.0878629\tvalid_1's multi_logloss: 0.616928\n",
      "[1275]\ttraining's multi_logloss: 0.0877286\tvalid_1's multi_logloss: 0.616916\n",
      "[1276]\ttraining's multi_logloss: 0.087587\tvalid_1's multi_logloss: 0.616834\n",
      "[1277]\ttraining's multi_logloss: 0.0874442\tvalid_1's multi_logloss: 0.61679\n",
      "[1278]\ttraining's multi_logloss: 0.0872952\tvalid_1's multi_logloss: 0.616699\n",
      "[1279]\ttraining's multi_logloss: 0.0871446\tvalid_1's multi_logloss: 0.616681\n",
      "[1280]\ttraining's multi_logloss: 0.0869917\tvalid_1's multi_logloss: 0.616636\n",
      "[1281]\ttraining's multi_logloss: 0.0868353\tvalid_1's multi_logloss: 0.616583\n",
      "[1282]\ttraining's multi_logloss: 0.0866809\tvalid_1's multi_logloss: 0.616545\n",
      "[1283]\ttraining's multi_logloss: 0.0865349\tvalid_1's multi_logloss: 0.616488\n",
      "[1284]\ttraining's multi_logloss: 0.0864155\tvalid_1's multi_logloss: 0.616486\n",
      "[1285]\ttraining's multi_logloss: 0.0863065\tvalid_1's multi_logloss: 0.616495\n",
      "[1286]\ttraining's multi_logloss: 0.0861557\tvalid_1's multi_logloss: 0.616435\n",
      "[1287]\ttraining's multi_logloss: 0.0860549\tvalid_1's multi_logloss: 0.616374\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1288]\ttraining's multi_logloss: 0.0859489\tvalid_1's multi_logloss: 0.61633\n",
      "[1289]\ttraining's multi_logloss: 0.0858344\tvalid_1's multi_logloss: 0.616349\n",
      "[1290]\ttraining's multi_logloss: 0.0856757\tvalid_1's multi_logloss: 0.616298\n",
      "[1291]\ttraining's multi_logloss: 0.0855434\tvalid_1's multi_logloss: 0.616267\n",
      "[1292]\ttraining's multi_logloss: 0.0854008\tvalid_1's multi_logloss: 0.616196\n",
      "[1293]\ttraining's multi_logloss: 0.0852801\tvalid_1's multi_logloss: 0.616184\n",
      "[1294]\ttraining's multi_logloss: 0.0851614\tvalid_1's multi_logloss: 0.616154\n",
      "[1295]\ttraining's multi_logloss: 0.0850174\tvalid_1's multi_logloss: 0.616132\n",
      "[1296]\ttraining's multi_logloss: 0.0848765\tvalid_1's multi_logloss: 0.616154\n",
      "[1297]\ttraining's multi_logloss: 0.0847583\tvalid_1's multi_logloss: 0.616126\n",
      "[1298]\ttraining's multi_logloss: 0.0846417\tvalid_1's multi_logloss: 0.616146\n",
      "[1299]\ttraining's multi_logloss: 0.0845073\tvalid_1's multi_logloss: 0.616105\n",
      "[1300]\ttraining's multi_logloss: 0.0843798\tvalid_1's multi_logloss: 0.616072\n",
      "[1301]\ttraining's multi_logloss: 0.0842526\tvalid_1's multi_logloss: 0.615991\n",
      "[1302]\ttraining's multi_logloss: 0.0841242\tvalid_1's multi_logloss: 0.615986\n",
      "[1303]\ttraining's multi_logloss: 0.0840063\tvalid_1's multi_logloss: 0.616023\n",
      "[1304]\ttraining's multi_logloss: 0.0838877\tvalid_1's multi_logloss: 0.616021\n",
      "[1305]\ttraining's multi_logloss: 0.0837732\tvalid_1's multi_logloss: 0.615986\n",
      "[1306]\ttraining's multi_logloss: 0.0836468\tvalid_1's multi_logloss: 0.615954\n",
      "[1307]\ttraining's multi_logloss: 0.0835432\tvalid_1's multi_logloss: 0.615889\n",
      "[1308]\ttraining's multi_logloss: 0.0834221\tvalid_1's multi_logloss: 0.615849\n",
      "[1309]\ttraining's multi_logloss: 0.083284\tvalid_1's multi_logloss: 0.615783\n",
      "[1310]\ttraining's multi_logloss: 0.0831421\tvalid_1's multi_logloss: 0.615742\n",
      "[1311]\ttraining's multi_logloss: 0.0829897\tvalid_1's multi_logloss: 0.615715\n",
      "[1312]\ttraining's multi_logloss: 0.0828781\tvalid_1's multi_logloss: 0.615709\n",
      "[1313]\ttraining's multi_logloss: 0.0827632\tvalid_1's multi_logloss: 0.615696\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1314]\ttraining's multi_logloss: 0.0826478\tvalid_1's multi_logloss: 0.615664\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1315]\ttraining's multi_logloss: 0.0825456\tvalid_1's multi_logloss: 0.615633\n",
      "[1316]\ttraining's multi_logloss: 0.0824098\tvalid_1's multi_logloss: 0.615574\n",
      "[1317]\ttraining's multi_logloss: 0.0822969\tvalid_1's multi_logloss: 0.615551\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1318]\ttraining's multi_logloss: 0.0821879\tvalid_1's multi_logloss: 0.615575\n",
      "[1319]\ttraining's multi_logloss: 0.0820542\tvalid_1's multi_logloss: 0.615548\n",
      "[1320]\ttraining's multi_logloss: 0.0819563\tvalid_1's multi_logloss: 0.615531\n",
      "[1321]\ttraining's multi_logloss: 0.0818234\tvalid_1's multi_logloss: 0.615537\n",
      "[1322]\ttraining's multi_logloss: 0.0816869\tvalid_1's multi_logloss: 0.615516\n",
      "[1323]\ttraining's multi_logloss: 0.0815584\tvalid_1's multi_logloss: 0.615498\n",
      "[1324]\ttraining's multi_logloss: 0.0814136\tvalid_1's multi_logloss: 0.615447\n",
      "[1325]\ttraining's multi_logloss: 0.0813016\tvalid_1's multi_logloss: 0.615451\n",
      "[1326]\ttraining's multi_logloss: 0.0811728\tvalid_1's multi_logloss: 0.615455\n",
      "[1327]\ttraining's multi_logloss: 0.0810363\tvalid_1's multi_logloss: 0.615385\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1328]\ttraining's multi_logloss: 0.08093\tvalid_1's multi_logloss: 0.615375\n",
      "[1329]\ttraining's multi_logloss: 0.0808118\tvalid_1's multi_logloss: 0.615328\n",
      "[1330]\ttraining's multi_logloss: 0.080689\tvalid_1's multi_logloss: 0.615209\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1331]\ttraining's multi_logloss: 0.0805716\tvalid_1's multi_logloss: 0.61513\n",
      "[1332]\ttraining's multi_logloss: 0.0804448\tvalid_1's multi_logloss: 0.615105\n",
      "[1333]\ttraining's multi_logloss: 0.0803296\tvalid_1's multi_logloss: 0.615018\n",
      "[1334]\ttraining's multi_logloss: 0.0801888\tvalid_1's multi_logloss: 0.614994\n",
      "[1335]\ttraining's multi_logloss: 0.0800507\tvalid_1's multi_logloss: 0.614964\n",
      "[1336]\ttraining's multi_logloss: 0.0799321\tvalid_1's multi_logloss: 0.614926\n",
      "[1337]\ttraining's multi_logloss: 0.0797858\tvalid_1's multi_logloss: 0.614863\n",
      "[1338]\ttraining's multi_logloss: 0.0796615\tvalid_1's multi_logloss: 0.614886\n",
      "[1339]\ttraining's multi_logloss: 0.0795442\tvalid_1's multi_logloss: 0.614879\n",
      "[1340]\ttraining's multi_logloss: 0.0794015\tvalid_1's multi_logloss: 0.614826\n",
      "[1341]\ttraining's multi_logloss: 0.0792751\tvalid_1's multi_logloss: 0.614806\n",
      "[1342]\ttraining's multi_logloss: 0.0791676\tvalid_1's multi_logloss: 0.61476\n",
      "[1343]\ttraining's multi_logloss: 0.0790379\tvalid_1's multi_logloss: 0.614752\n",
      "[1344]\ttraining's multi_logloss: 0.0789077\tvalid_1's multi_logloss: 0.614759\n",
      "[1345]\ttraining's multi_logloss: 0.0787907\tvalid_1's multi_logloss: 0.614754\n",
      "[1346]\ttraining's multi_logloss: 0.0786596\tvalid_1's multi_logloss: 0.614745\n",
      "[1347]\ttraining's multi_logloss: 0.0785383\tvalid_1's multi_logloss: 0.614692\n",
      "[1348]\ttraining's multi_logloss: 0.0784169\tvalid_1's multi_logloss: 0.614703\n",
      "[1349]\ttraining's multi_logloss: 0.0782828\tvalid_1's multi_logloss: 0.614646\n",
      "[1350]\ttraining's multi_logloss: 0.0781553\tvalid_1's multi_logloss: 0.614632\n",
      "[1351]\ttraining's multi_logloss: 0.0780224\tvalid_1's multi_logloss: 0.614649\n",
      "[1352]\ttraining's multi_logloss: 0.0778859\tvalid_1's multi_logloss: 0.614647\n",
      "[1353]\ttraining's multi_logloss: 0.0777523\tvalid_1's multi_logloss: 0.614614\n",
      "[1354]\ttraining's multi_logloss: 0.0776131\tvalid_1's multi_logloss: 0.614582\n",
      "[1355]\ttraining's multi_logloss: 0.0774893\tvalid_1's multi_logloss: 0.61449\n",
      "[1356]\ttraining's multi_logloss: 0.0773613\tvalid_1's multi_logloss: 0.614518\n",
      "[1357]\ttraining's multi_logloss: 0.0772477\tvalid_1's multi_logloss: 0.614504\n",
      "[1358]\ttraining's multi_logloss: 0.0771271\tvalid_1's multi_logloss: 0.61451\n",
      "[1359]\ttraining's multi_logloss: 0.0769969\tvalid_1's multi_logloss: 0.614541\n",
      "[1360]\ttraining's multi_logloss: 0.0768828\tvalid_1's multi_logloss: 0.614543\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1361]\ttraining's multi_logloss: 0.0767752\tvalid_1's multi_logloss: 0.614527\n",
      "[1362]\ttraining's multi_logloss: 0.0766541\tvalid_1's multi_logloss: 0.61454\n",
      "[1363]\ttraining's multi_logloss: 0.0765306\tvalid_1's multi_logloss: 0.6145\n",
      "[1364]\ttraining's multi_logloss: 0.0764096\tvalid_1's multi_logloss: 0.614482\n",
      "[1365]\ttraining's multi_logloss: 0.0763184\tvalid_1's multi_logloss: 0.614468\n",
      "[1366]\ttraining's multi_logloss: 0.0761843\tvalid_1's multi_logloss: 0.61445\n",
      "[1367]\ttraining's multi_logloss: 0.0760734\tvalid_1's multi_logloss: 0.614434\n",
      "[1368]\ttraining's multi_logloss: 0.0759472\tvalid_1's multi_logloss: 0.614433\n",
      "[1369]\ttraining's multi_logloss: 0.0758203\tvalid_1's multi_logloss: 0.614413\n",
      "[1370]\ttraining's multi_logloss: 0.0757041\tvalid_1's multi_logloss: 0.614427\n",
      "[1371]\ttraining's multi_logloss: 0.0755564\tvalid_1's multi_logloss: 0.614391\n",
      "[1372]\ttraining's multi_logloss: 0.0754209\tvalid_1's multi_logloss: 0.614367\n",
      "[1373]\ttraining's multi_logloss: 0.0752894\tvalid_1's multi_logloss: 0.614336\n",
      "[1374]\ttraining's multi_logloss: 0.0751606\tvalid_1's multi_logloss: 0.614281\n",
      "[1375]\ttraining's multi_logloss: 0.0750426\tvalid_1's multi_logloss: 0.614271\n",
      "[1376]\ttraining's multi_logloss: 0.074934\tvalid_1's multi_logloss: 0.614231\n",
      "[1377]\ttraining's multi_logloss: 0.0748106\tvalid_1's multi_logloss: 0.61423\n",
      "[1378]\ttraining's multi_logloss: 0.0746831\tvalid_1's multi_logloss: 0.61423\n",
      "[1379]\ttraining's multi_logloss: 0.0745528\tvalid_1's multi_logloss: 0.614162\n",
      "[1380]\ttraining's multi_logloss: 0.0744391\tvalid_1's multi_logloss: 0.614141\n",
      "[1381]\ttraining's multi_logloss: 0.0743064\tvalid_1's multi_logloss: 0.614041\n",
      "[1382]\ttraining's multi_logloss: 0.0741756\tvalid_1's multi_logloss: 0.614054\n",
      "[1383]\ttraining's multi_logloss: 0.0740465\tvalid_1's multi_logloss: 0.61402\n",
      "[1384]\ttraining's multi_logloss: 0.0739057\tvalid_1's multi_logloss: 0.613984\n",
      "[1385]\ttraining's multi_logloss: 0.0737717\tvalid_1's multi_logloss: 0.613978\n",
      "[1386]\ttraining's multi_logloss: 0.0736594\tvalid_1's multi_logloss: 0.613962\n",
      "[1387]\ttraining's multi_logloss: 0.0735451\tvalid_1's multi_logloss: 0.613903\n",
      "[1388]\ttraining's multi_logloss: 0.0734131\tvalid_1's multi_logloss: 0.613832\n",
      "[1389]\ttraining's multi_logloss: 0.0733017\tvalid_1's multi_logloss: 0.613837\n",
      "[1390]\ttraining's multi_logloss: 0.0731858\tvalid_1's multi_logloss: 0.613814\n",
      "[1391]\ttraining's multi_logloss: 0.0730712\tvalid_1's multi_logloss: 0.613766\n",
      "[1392]\ttraining's multi_logloss: 0.0729621\tvalid_1's multi_logloss: 0.613686\n",
      "[1393]\ttraining's multi_logloss: 0.0728507\tvalid_1's multi_logloss: 0.613719\n",
      "[1394]\ttraining's multi_logloss: 0.0727157\tvalid_1's multi_logloss: 0.613701\n",
      "[1395]\ttraining's multi_logloss: 0.072598\tvalid_1's multi_logloss: 0.613625\n",
      "[1396]\ttraining's multi_logloss: 0.0724721\tvalid_1's multi_logloss: 0.613581\n",
      "[1397]\ttraining's multi_logloss: 0.0723645\tvalid_1's multi_logloss: 0.613577\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1398]\ttraining's multi_logloss: 0.072263\tvalid_1's multi_logloss: 0.613595\n",
      "[1399]\ttraining's multi_logloss: 0.0721313\tvalid_1's multi_logloss: 0.613566\n",
      "[1400]\ttraining's multi_logloss: 0.0720186\tvalid_1's multi_logloss: 0.613519\n",
      "[1401]\ttraining's multi_logloss: 0.071906\tvalid_1's multi_logloss: 0.61343\n",
      "[1402]\ttraining's multi_logloss: 0.0717877\tvalid_1's multi_logloss: 0.613435\n",
      "[1403]\ttraining's multi_logloss: 0.0716662\tvalid_1's multi_logloss: 0.613419\n",
      "[1404]\ttraining's multi_logloss: 0.0715617\tvalid_1's multi_logloss: 0.613408\n",
      "[1405]\ttraining's multi_logloss: 0.0714416\tvalid_1's multi_logloss: 0.613389\n",
      "[1406]\ttraining's multi_logloss: 0.0713237\tvalid_1's multi_logloss: 0.613384\n",
      "[1407]\ttraining's multi_logloss: 0.0712036\tvalid_1's multi_logloss: 0.613397\n",
      "[1408]\ttraining's multi_logloss: 0.0710854\tvalid_1's multi_logloss: 0.61337\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1409]\ttraining's multi_logloss: 0.0709855\tvalid_1's multi_logloss: 0.613348\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1410]\ttraining's multi_logloss: 0.0708872\tvalid_1's multi_logloss: 0.613398\n",
      "[1411]\ttraining's multi_logloss: 0.0707771\tvalid_1's multi_logloss: 0.61339\n",
      "[1412]\ttraining's multi_logloss: 0.0706558\tvalid_1's multi_logloss: 0.613374\n",
      "[1413]\ttraining's multi_logloss: 0.0705385\tvalid_1's multi_logloss: 0.61336\n",
      "[1414]\ttraining's multi_logloss: 0.0704161\tvalid_1's multi_logloss: 0.613317\n",
      "[1415]\ttraining's multi_logloss: 0.0703003\tvalid_1's multi_logloss: 0.613321\n",
      "[1416]\ttraining's multi_logloss: 0.0701848\tvalid_1's multi_logloss: 0.613349\n",
      "[1417]\ttraining's multi_logloss: 0.0700877\tvalid_1's multi_logloss: 0.61334\n",
      "[1418]\ttraining's multi_logloss: 0.0699606\tvalid_1's multi_logloss: 0.613291\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1419]\ttraining's multi_logloss: 0.0698712\tvalid_1's multi_logloss: 0.613294\n",
      "[1420]\ttraining's multi_logloss: 0.0697457\tvalid_1's multi_logloss: 0.61327\n",
      "[1421]\ttraining's multi_logloss: 0.0696152\tvalid_1's multi_logloss: 0.613203\n",
      "[1422]\ttraining's multi_logloss: 0.0694877\tvalid_1's multi_logloss: 0.613201\n",
      "[1423]\ttraining's multi_logloss: 0.069381\tvalid_1's multi_logloss: 0.613168\n",
      "[1424]\ttraining's multi_logloss: 0.0692682\tvalid_1's multi_logloss: 0.613171\n",
      "[1425]\ttraining's multi_logloss: 0.0691564\tvalid_1's multi_logloss: 0.613151\n",
      "[1426]\ttraining's multi_logloss: 0.0690309\tvalid_1's multi_logloss: 0.613118\n",
      "[1427]\ttraining's multi_logloss: 0.068924\tvalid_1's multi_logloss: 0.613105\n",
      "[1428]\ttraining's multi_logloss: 0.0688007\tvalid_1's multi_logloss: 0.613065\n",
      "[1429]\ttraining's multi_logloss: 0.0686891\tvalid_1's multi_logloss: 0.613062\n",
      "[1430]\ttraining's multi_logloss: 0.0685789\tvalid_1's multi_logloss: 0.613092\n",
      "[1431]\ttraining's multi_logloss: 0.0684642\tvalid_1's multi_logloss: 0.613077\n",
      "[1432]\ttraining's multi_logloss: 0.0683543\tvalid_1's multi_logloss: 0.613027\n",
      "[1433]\ttraining's multi_logloss: 0.068225\tvalid_1's multi_logloss: 0.612985\n",
      "[1434]\ttraining's multi_logloss: 0.0681216\tvalid_1's multi_logloss: 0.612921\n",
      "[1435]\ttraining's multi_logloss: 0.068014\tvalid_1's multi_logloss: 0.612979\n",
      "[1436]\ttraining's multi_logloss: 0.0679024\tvalid_1's multi_logloss: 0.613017\n",
      "[1437]\ttraining's multi_logloss: 0.0677881\tvalid_1's multi_logloss: 0.612964\n",
      "[1438]\ttraining's multi_logloss: 0.0676764\tvalid_1's multi_logloss: 0.612913\n",
      "[1439]\ttraining's multi_logloss: 0.067581\tvalid_1's multi_logloss: 0.61291\n",
      "[1440]\ttraining's multi_logloss: 0.0674983\tvalid_1's multi_logloss: 0.612897\n",
      "[1441]\ttraining's multi_logloss: 0.0673773\tvalid_1's multi_logloss: 0.612906\n",
      "[1442]\ttraining's multi_logloss: 0.0672569\tvalid_1's multi_logloss: 0.612897\n",
      "[1443]\ttraining's multi_logloss: 0.0671644\tvalid_1's multi_logloss: 0.612898\n",
      "[1444]\ttraining's multi_logloss: 0.0670552\tvalid_1's multi_logloss: 0.612867\n",
      "[1445]\ttraining's multi_logloss: 0.0669659\tvalid_1's multi_logloss: 0.612843\n",
      "[1446]\ttraining's multi_logloss: 0.0668571\tvalid_1's multi_logloss: 0.612809\n",
      "[1447]\ttraining's multi_logloss: 0.0667563\tvalid_1's multi_logloss: 0.612766\n",
      "[1448]\ttraining's multi_logloss: 0.0666729\tvalid_1's multi_logloss: 0.612683\n",
      "[1449]\ttraining's multi_logloss: 0.0665673\tvalid_1's multi_logloss: 0.612741\n",
      "[1450]\ttraining's multi_logloss: 0.0664697\tvalid_1's multi_logloss: 0.612717\n",
      "[1451]\ttraining's multi_logloss: 0.0663636\tvalid_1's multi_logloss: 0.612771\n",
      "[1452]\ttraining's multi_logloss: 0.0662716\tvalid_1's multi_logloss: 0.61275\n",
      "[1453]\ttraining's multi_logloss: 0.0661678\tvalid_1's multi_logloss: 0.612736\n",
      "[1454]\ttraining's multi_logloss: 0.0660737\tvalid_1's multi_logloss: 0.612663\n",
      "[1455]\ttraining's multi_logloss: 0.0659679\tvalid_1's multi_logloss: 0.612652\n",
      "[1456]\ttraining's multi_logloss: 0.0658672\tvalid_1's multi_logloss: 0.612704\n",
      "[1457]\ttraining's multi_logloss: 0.0657555\tvalid_1's multi_logloss: 0.612631\n",
      "[1458]\ttraining's multi_logloss: 0.0656401\tvalid_1's multi_logloss: 0.612545\n",
      "[1459]\ttraining's multi_logloss: 0.0655554\tvalid_1's multi_logloss: 0.612529\n",
      "[1460]\ttraining's multi_logloss: 0.0654553\tvalid_1's multi_logloss: 0.61254\n",
      "[1461]\ttraining's multi_logloss: 0.0653714\tvalid_1's multi_logloss: 0.612531\n",
      "[1462]\ttraining's multi_logloss: 0.0652801\tvalid_1's multi_logloss: 0.612465\n",
      "[1463]\ttraining's multi_logloss: 0.0651755\tvalid_1's multi_logloss: 0.612404\n",
      "[1464]\ttraining's multi_logloss: 0.0650802\tvalid_1's multi_logloss: 0.612437\n",
      "[1465]\ttraining's multi_logloss: 0.0649851\tvalid_1's multi_logloss: 0.612472\n",
      "[1466]\ttraining's multi_logloss: 0.0648928\tvalid_1's multi_logloss: 0.612415\n",
      "[1467]\ttraining's multi_logloss: 0.0647828\tvalid_1's multi_logloss: 0.612422\n",
      "[1468]\ttraining's multi_logloss: 0.0646762\tvalid_1's multi_logloss: 0.612401\n",
      "[1469]\ttraining's multi_logloss: 0.0645733\tvalid_1's multi_logloss: 0.612415\n",
      "[1470]\ttraining's multi_logloss: 0.0644804\tvalid_1's multi_logloss: 0.612371\n",
      "[1471]\ttraining's multi_logloss: 0.0643903\tvalid_1's multi_logloss: 0.61238\n",
      "[1472]\ttraining's multi_logloss: 0.0642822\tvalid_1's multi_logloss: 0.612295\n",
      "[1473]\ttraining's multi_logloss: 0.0641761\tvalid_1's multi_logloss: 0.612315\n",
      "[1474]\ttraining's multi_logloss: 0.0640737\tvalid_1's multi_logloss: 0.612334\n",
      "[1475]\ttraining's multi_logloss: 0.063968\tvalid_1's multi_logloss: 0.612356\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1476]\ttraining's multi_logloss: 0.0638758\tvalid_1's multi_logloss: 0.612302\n",
      "[1477]\ttraining's multi_logloss: 0.0637742\tvalid_1's multi_logloss: 0.612228\n",
      "[1478]\ttraining's multi_logloss: 0.0636594\tvalid_1's multi_logloss: 0.612177\n",
      "[1479]\ttraining's multi_logloss: 0.0635843\tvalid_1's multi_logloss: 0.612116\n",
      "[1480]\ttraining's multi_logloss: 0.0634808\tvalid_1's multi_logloss: 0.612068\n",
      "[1481]\ttraining's multi_logloss: 0.0633674\tvalid_1's multi_logloss: 0.612084\n",
      "[1482]\ttraining's multi_logloss: 0.0632792\tvalid_1's multi_logloss: 0.61206\n",
      "[1483]\ttraining's multi_logloss: 0.0631749\tvalid_1's multi_logloss: 0.612065\n",
      "[1484]\ttraining's multi_logloss: 0.0630649\tvalid_1's multi_logloss: 0.612064\n",
      "[1485]\ttraining's multi_logloss: 0.0629781\tvalid_1's multi_logloss: 0.612038\n",
      "[1486]\ttraining's multi_logloss: 0.0628893\tvalid_1's multi_logloss: 0.611986\n",
      "[1487]\ttraining's multi_logloss: 0.0627895\tvalid_1's multi_logloss: 0.611966\n",
      "[1488]\ttraining's multi_logloss: 0.0626838\tvalid_1's multi_logloss: 0.611954\n",
      "[1489]\ttraining's multi_logloss: 0.0625745\tvalid_1's multi_logloss: 0.611908\n",
      "[1490]\ttraining's multi_logloss: 0.0624758\tvalid_1's multi_logloss: 0.611863\n",
      "[1491]\ttraining's multi_logloss: 0.0623668\tvalid_1's multi_logloss: 0.611855\n",
      "[1492]\ttraining's multi_logloss: 0.0622627\tvalid_1's multi_logloss: 0.61182\n",
      "[1493]\ttraining's multi_logloss: 0.0621626\tvalid_1's multi_logloss: 0.611757\n",
      "[1494]\ttraining's multi_logloss: 0.0620688\tvalid_1's multi_logloss: 0.611773\n",
      "[1495]\ttraining's multi_logloss: 0.0619607\tvalid_1's multi_logloss: 0.611705\n",
      "[1496]\ttraining's multi_logloss: 0.0618585\tvalid_1's multi_logloss: 0.611747\n",
      "[1497]\ttraining's multi_logloss: 0.0617662\tvalid_1's multi_logloss: 0.611714\n",
      "[1498]\ttraining's multi_logloss: 0.0616587\tvalid_1's multi_logloss: 0.611685\n",
      "[1499]\ttraining's multi_logloss: 0.0615565\tvalid_1's multi_logloss: 0.611626\n",
      "[1500]\ttraining's multi_logloss: 0.0614669\tvalid_1's multi_logloss: 0.611546\n",
      "[1501]\ttraining's multi_logloss: 0.0613654\tvalid_1's multi_logloss: 0.611582\n",
      "[1502]\ttraining's multi_logloss: 0.0612652\tvalid_1's multi_logloss: 0.611532\n",
      "[1503]\ttraining's multi_logloss: 0.061158\tvalid_1's multi_logloss: 0.6116\n",
      "[1504]\ttraining's multi_logloss: 0.0610608\tvalid_1's multi_logloss: 0.611559\n",
      "[1505]\ttraining's multi_logloss: 0.0609665\tvalid_1's multi_logloss: 0.611562\n",
      "[1506]\ttraining's multi_logloss: 0.0608833\tvalid_1's multi_logloss: 0.611554\n",
      "[1507]\ttraining's multi_logloss: 0.0607785\tvalid_1's multi_logloss: 0.61158\n",
      "[1508]\ttraining's multi_logloss: 0.0606656\tvalid_1's multi_logloss: 0.61148\n",
      "[1509]\ttraining's multi_logloss: 0.0605667\tvalid_1's multi_logloss: 0.611457\n",
      "[1510]\ttraining's multi_logloss: 0.0604794\tvalid_1's multi_logloss: 0.61143\n",
      "[1511]\ttraining's multi_logloss: 0.0603772\tvalid_1's multi_logloss: 0.611407\n",
      "[1512]\ttraining's multi_logloss: 0.0602867\tvalid_1's multi_logloss: 0.611346\n",
      "[1513]\ttraining's multi_logloss: 0.0601829\tvalid_1's multi_logloss: 0.611309\n",
      "[1514]\ttraining's multi_logloss: 0.0600972\tvalid_1's multi_logloss: 0.61129\n",
      "[1515]\ttraining's multi_logloss: 0.0600012\tvalid_1's multi_logloss: 0.611325\n",
      "[1516]\ttraining's multi_logloss: 0.0599038\tvalid_1's multi_logloss: 0.61131\n",
      "[1517]\ttraining's multi_logloss: 0.0598078\tvalid_1's multi_logloss: 0.611305\n",
      "[1518]\ttraining's multi_logloss: 0.0597058\tvalid_1's multi_logloss: 0.611213\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1519]\ttraining's multi_logloss: 0.059621\tvalid_1's multi_logloss: 0.611164\n",
      "[1520]\ttraining's multi_logloss: 0.0595214\tvalid_1's multi_logloss: 0.611129\n",
      "[1521]\ttraining's multi_logloss: 0.059413\tvalid_1's multi_logloss: 0.611131\n",
      "[1522]\ttraining's multi_logloss: 0.0593052\tvalid_1's multi_logloss: 0.611121\n",
      "[1523]\ttraining's multi_logloss: 0.0592053\tvalid_1's multi_logloss: 0.611097\n",
      "[1524]\ttraining's multi_logloss: 0.0591051\tvalid_1's multi_logloss: 0.611032\n",
      "[1525]\ttraining's multi_logloss: 0.0590077\tvalid_1's multi_logloss: 0.610974\n",
      "[1526]\ttraining's multi_logloss: 0.0589172\tvalid_1's multi_logloss: 0.610924\n",
      "[1527]\ttraining's multi_logloss: 0.0588228\tvalid_1's multi_logloss: 0.610917\n",
      "[1528]\ttraining's multi_logloss: 0.0587485\tvalid_1's multi_logloss: 0.610885\n",
      "[1529]\ttraining's multi_logloss: 0.0586552\tvalid_1's multi_logloss: 0.610886\n",
      "[1530]\ttraining's multi_logloss: 0.0585588\tvalid_1's multi_logloss: 0.610833\n",
      "[1531]\ttraining's multi_logloss: 0.0584825\tvalid_1's multi_logloss: 0.610747\n",
      "[1532]\ttraining's multi_logloss: 0.0583797\tvalid_1's multi_logloss: 0.610756\n",
      "[1533]\ttraining's multi_logloss: 0.0582915\tvalid_1's multi_logloss: 0.61074\n",
      "[1534]\ttraining's multi_logloss: 0.0581989\tvalid_1's multi_logloss: 0.610747\n",
      "[1535]\ttraining's multi_logloss: 0.0581184\tvalid_1's multi_logloss: 0.610667\n",
      "[1536]\ttraining's multi_logloss: 0.0580193\tvalid_1's multi_logloss: 0.610648\n",
      "[1537]\ttraining's multi_logloss: 0.0579125\tvalid_1's multi_logloss: 0.610575\n",
      "[1538]\ttraining's multi_logloss: 0.0578128\tvalid_1's multi_logloss: 0.610582\n",
      "[1539]\ttraining's multi_logloss: 0.0577107\tvalid_1's multi_logloss: 0.610557\n",
      "[1540]\ttraining's multi_logloss: 0.0576185\tvalid_1's multi_logloss: 0.610537\n",
      "[1541]\ttraining's multi_logloss: 0.0575377\tvalid_1's multi_logloss: 0.610466\n",
      "[1542]\ttraining's multi_logloss: 0.0574492\tvalid_1's multi_logloss: 0.610438\n",
      "[1543]\ttraining's multi_logloss: 0.0573589\tvalid_1's multi_logloss: 0.610414\n",
      "[1544]\ttraining's multi_logloss: 0.0572593\tvalid_1's multi_logloss: 0.610399\n",
      "[1545]\ttraining's multi_logloss: 0.0571729\tvalid_1's multi_logloss: 0.610308\n",
      "[1546]\ttraining's multi_logloss: 0.0570795\tvalid_1's multi_logloss: 0.610295\n",
      "[1547]\ttraining's multi_logloss: 0.0569891\tvalid_1's multi_logloss: 0.61027\n",
      "[1548]\ttraining's multi_logloss: 0.0568933\tvalid_1's multi_logloss: 0.610222\n",
      "[1549]\ttraining's multi_logloss: 0.0568059\tvalid_1's multi_logloss: 0.610161\n",
      "[1550]\ttraining's multi_logloss: 0.0567098\tvalid_1's multi_logloss: 0.610087\n",
      "[1551]\ttraining's multi_logloss: 0.0566115\tvalid_1's multi_logloss: 0.610007\n",
      "[1552]\ttraining's multi_logloss: 0.0565168\tvalid_1's multi_logloss: 0.609942\n",
      "[1553]\ttraining's multi_logloss: 0.056437\tvalid_1's multi_logloss: 0.609842\n",
      "[1554]\ttraining's multi_logloss: 0.0563289\tvalid_1's multi_logloss: 0.609783\n",
      "[1555]\ttraining's multi_logloss: 0.0562282\tvalid_1's multi_logloss: 0.609779\n",
      "[1556]\ttraining's multi_logloss: 0.0561292\tvalid_1's multi_logloss: 0.609718\n",
      "[1557]\ttraining's multi_logloss: 0.0560417\tvalid_1's multi_logloss: 0.609713\n",
      "[1558]\ttraining's multi_logloss: 0.0559606\tvalid_1's multi_logloss: 0.609635\n",
      "[1559]\ttraining's multi_logloss: 0.0558761\tvalid_1's multi_logloss: 0.609612\n",
      "[1560]\ttraining's multi_logloss: 0.0557875\tvalid_1's multi_logloss: 0.60957\n",
      "[1561]\ttraining's multi_logloss: 0.0556927\tvalid_1's multi_logloss: 0.609567\n",
      "[1562]\ttraining's multi_logloss: 0.0555874\tvalid_1's multi_logloss: 0.609517\n",
      "[1563]\ttraining's multi_logloss: 0.0554945\tvalid_1's multi_logloss: 0.609536\n",
      "[1564]\ttraining's multi_logloss: 0.0554154\tvalid_1's multi_logloss: 0.609477\n",
      "[1565]\ttraining's multi_logloss: 0.0553281\tvalid_1's multi_logloss: 0.609472\n",
      "[1566]\ttraining's multi_logloss: 0.0552512\tvalid_1's multi_logloss: 0.609454\n",
      "[1567]\ttraining's multi_logloss: 0.0551561\tvalid_1's multi_logloss: 0.609403\n",
      "[1568]\ttraining's multi_logloss: 0.0550762\tvalid_1's multi_logloss: 0.609391\n",
      "[1569]\ttraining's multi_logloss: 0.0549915\tvalid_1's multi_logloss: 0.609401\n",
      "[1570]\ttraining's multi_logloss: 0.0549004\tvalid_1's multi_logloss: 0.609433\n",
      "[1571]\ttraining's multi_logloss: 0.0548165\tvalid_1's multi_logloss: 0.609461\n",
      "[1572]\ttraining's multi_logloss: 0.0547423\tvalid_1's multi_logloss: 0.609376\n",
      "[1573]\ttraining's multi_logloss: 0.0546551\tvalid_1's multi_logloss: 0.609344\n",
      "[1574]\ttraining's multi_logloss: 0.0545652\tvalid_1's multi_logloss: 0.609325\n",
      "[1575]\ttraining's multi_logloss: 0.054471\tvalid_1's multi_logloss: 0.609328\n",
      "[1576]\ttraining's multi_logloss: 0.0543805\tvalid_1's multi_logloss: 0.60931\n",
      "[1577]\ttraining's multi_logloss: 0.0543005\tvalid_1's multi_logloss: 0.609267\n",
      "[1578]\ttraining's multi_logloss: 0.0542257\tvalid_1's multi_logloss: 0.609257\n",
      "[1579]\ttraining's multi_logloss: 0.0541391\tvalid_1's multi_logloss: 0.609266\n",
      "[1580]\ttraining's multi_logloss: 0.0540658\tvalid_1's multi_logloss: 0.60928\n",
      "[1581]\ttraining's multi_logloss: 0.0539919\tvalid_1's multi_logloss: 0.609263\n",
      "[1582]\ttraining's multi_logloss: 0.0539059\tvalid_1's multi_logloss: 0.609267\n",
      "[1583]\ttraining's multi_logloss: 0.053826\tvalid_1's multi_logloss: 0.609208\n",
      "[1584]\ttraining's multi_logloss: 0.0537353\tvalid_1's multi_logloss: 0.609162\n",
      "[1585]\ttraining's multi_logloss: 0.0536471\tvalid_1's multi_logloss: 0.609191\n",
      "[1586]\ttraining's multi_logloss: 0.0535621\tvalid_1's multi_logloss: 0.609167\n",
      "[1587]\ttraining's multi_logloss: 0.0534774\tvalid_1's multi_logloss: 0.609167\n",
      "[1588]\ttraining's multi_logloss: 0.0533846\tvalid_1's multi_logloss: 0.609153\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1589]\ttraining's multi_logloss: 0.0533104\tvalid_1's multi_logloss: 0.609108\n",
      "[1590]\ttraining's multi_logloss: 0.0532277\tvalid_1's multi_logloss: 0.609045\n",
      "[1591]\ttraining's multi_logloss: 0.053136\tvalid_1's multi_logloss: 0.608999\n",
      "[1592]\ttraining's multi_logloss: 0.0530508\tvalid_1's multi_logloss: 0.609001\n",
      "[1593]\ttraining's multi_logloss: 0.052969\tvalid_1's multi_logloss: 0.608988\n",
      "[1594]\ttraining's multi_logloss: 0.0528899\tvalid_1's multi_logloss: 0.609022\n",
      "[1595]\ttraining's multi_logloss: 0.0528032\tvalid_1's multi_logloss: 0.609029\n",
      "[1596]\ttraining's multi_logloss: 0.0527219\tvalid_1's multi_logloss: 0.609008\n",
      "[1597]\ttraining's multi_logloss: 0.052653\tvalid_1's multi_logloss: 0.608971\n",
      "[1598]\ttraining's multi_logloss: 0.0525803\tvalid_1's multi_logloss: 0.608926\n",
      "[1599]\ttraining's multi_logloss: 0.0524931\tvalid_1's multi_logloss: 0.608916\n",
      "[1600]\ttraining's multi_logloss: 0.0524117\tvalid_1's multi_logloss: 0.608886\n",
      "[1601]\ttraining's multi_logloss: 0.0523335\tvalid_1's multi_logloss: 0.608858\n",
      "[1602]\ttraining's multi_logloss: 0.0522485\tvalid_1's multi_logloss: 0.608822\n",
      "[1603]\ttraining's multi_logloss: 0.0521576\tvalid_1's multi_logloss: 0.608809\n",
      "[1604]\ttraining's multi_logloss: 0.0520779\tvalid_1's multi_logloss: 0.60884\n",
      "[1605]\ttraining's multi_logloss: 0.0520031\tvalid_1's multi_logloss: 0.6088\n",
      "[1606]\ttraining's multi_logloss: 0.0519222\tvalid_1's multi_logloss: 0.608773\n",
      "[1607]\ttraining's multi_logloss: 0.0518337\tvalid_1's multi_logloss: 0.608723\n",
      "[1608]\ttraining's multi_logloss: 0.0517474\tvalid_1's multi_logloss: 0.608677\n",
      "[1609]\ttraining's multi_logloss: 0.0516771\tvalid_1's multi_logloss: 0.608602\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1610]\ttraining's multi_logloss: 0.051596\tvalid_1's multi_logloss: 0.608571\n",
      "[1611]\ttraining's multi_logloss: 0.0515083\tvalid_1's multi_logloss: 0.608525\n",
      "[1612]\ttraining's multi_logloss: 0.0514408\tvalid_1's multi_logloss: 0.608535\n",
      "[1613]\ttraining's multi_logloss: 0.0513546\tvalid_1's multi_logloss: 0.608565\n",
      "[1614]\ttraining's multi_logloss: 0.0512647\tvalid_1's multi_logloss: 0.608611\n",
      "[1615]\ttraining's multi_logloss: 0.0511884\tvalid_1's multi_logloss: 0.608564\n",
      "[1616]\ttraining's multi_logloss: 0.0511005\tvalid_1's multi_logloss: 0.608601\n",
      "[1617]\ttraining's multi_logloss: 0.0510283\tvalid_1's multi_logloss: 0.608659\n",
      "[1618]\ttraining's multi_logloss: 0.0509507\tvalid_1's multi_logloss: 0.608611\n",
      "[1619]\ttraining's multi_logloss: 0.0508808\tvalid_1's multi_logloss: 0.608567\n",
      "[1620]\ttraining's multi_logloss: 0.0507982\tvalid_1's multi_logloss: 0.608528\n",
      "[1621]\ttraining's multi_logloss: 0.0507101\tvalid_1's multi_logloss: 0.608537\n",
      "Early stopping, best iteration is:\n",
      "[1611]\ttraining's multi_logloss: 0.0515083\tvalid_1's multi_logloss: 0.608525\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_test = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "params = {\n",
    "'boosting_type': 'gbdt', 'min_data_in_leaf':10, 'feature_fraction':0.9, 'bagging_fraction':1,\n",
    "'bagging_freq':20, 'max_depth':32,\n",
    "'num_leaves':100,\n",
    "'learning_rate':0.005,\n",
    "'objective': 'multiclass',\n",
    "'metric': 'multi_logloss',\n",
    "'num_class':3\n",
    "}\n",
    "gbm = lgb.train(\n",
    "params,\n",
    "lgb_train,\n",
    "num_boost_round=3000,\n",
    "valid_sets=[lgb_train, lgb_test],\n",
    "early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77       469\n",
      "           1       0.67      0.69      0.68       410\n",
      "           2       0.77      0.72      0.74       428\n",
      "\n",
      "    accuracy                           0.73      1307\n",
      "   macro avg       0.73      0.73      0.73      1307\n",
      "weighted avg       0.73      0.73      0.73      1307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_y = gbm.predict(X_test)\n",
    "pred_y = [np.argmax(y) for y in predicted_y]\n",
    "print(classification_report(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Performance of LOSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loso_validation():\n",
    "\n",
    "    dr_feat_path = r'X:\\IDEaS_2\\Driving Simulator\\Data\\Subjectwise\\ecg_eda_t2_scld'\n",
    "    mat_feat_path = r'X:\\IDEaS_2\\MATBII\\Data\\Subjectwise\\ecg_eda_t2_scld'\n",
    "    bas_feat_path = r'X:\\IDEaS_2\\Driving Simulator\\Data\\Subjectwise\\base_ecg_eda_t2'\n",
    "\n",
    "    xtrainMat, yMat = load_matb(mat_feat_path, 'label')\n",
    "    xtrainBas, yBas = load_viragebase(bas_feat_path, 'label')\n",
    "\n",
    "    subjects = os.listdir(dr_feat_path)\n",
    "\n",
    "    for sdriv in subjects:\n",
    "        xtestDriv = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(sdriv)))\n",
    "        ytestDriv = xtestDriv['label'].copy()\n",
    "\n",
    "        xtrainDriv = pd.DataFrame()\n",
    "        for subTrain in subjects:\n",
    "            if subTrain != sdriv:\n",
    "                train = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(subTrain)))\n",
    "                xtrainDriv = xtrainDriv.append(train)\n",
    "                xtrainDriv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        XtrainDriv = xtrainDriv[selected_cols].copy()\n",
    "        xtestDriv = xtestDriv[selected_cols].copy()\n",
    "        ytrainDriv = list(XtrainDriv['label'].copy())\n",
    "\n",
    "        XtrainDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "        xtestDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "\n",
    "        XtrainDriv = XtrainDriv.append(xtrainMat)\n",
    "        XtrainDriv = XtrainDriv.append(xtrainBas)\n",
    "\n",
    "        ytrain = ytrainDriv + yMat + yBas\n",
    "        X = XtrainDriv.values\n",
    "\n",
    "        for idx, val in enumerate(ytrain):\n",
    "            if val <= 4:\n",
    "                ytrain[idx] = 0\n",
    "            else: ytrain[idx] = 2\n",
    "\n",
    "        for idx, val in enumerate(ytestDriv):\n",
    "            if val <= 4:\n",
    "                ytestDriv[idx] = 0\n",
    "            else: ytestDriv[idx] = 2\n",
    "\n",
    "        paramsrf = {'n_estimators': 3000,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1,\n",
    "        'max_features': 'sqrt',\n",
    "        'max_depth': 50,\n",
    "        'bootstrap': False, 'verbose':0, 'n_jobs': -1, 'class_weight': 'balanced'}\n",
    "\n",
    "        clf2 = RandomForestClassifier(**paramsrf)\n",
    "        clf3 = lightgbm.LGBMClassifier(n_estimators = 3000, num_leaves=100, learning_rate=0.005, class_weight='balanced')\n",
    "        clf4 = SVC(C=1000, probability=True, class_weight='balanced')\n",
    "\n",
    "        eclf = VotingClassifier(estimators=[('rf', clf2), ('gbm', clf3), ('svm', clf4)], voting='soft') # \n",
    "        hist = eclf.fit(X, ytrain)\n",
    "        yPred = hist.predict(xtestDriv)\n",
    "        print('Test Subject: {}'.format(sdriv))\n",
    "        print(classification_report(ytestDriv, yPred, zero_division=1))\n",
    "    return\n",
    "\n",
    "loso_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Subject: 1030.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      1.00      0.52        19\n",
      "           1       1.00      0.19      0.31        43\n",
      "\n",
      "    accuracy                           0.44        62\n",
      "   macro avg       0.68      0.59      0.42        62\n",
      "weighted avg       0.80      0.44      0.38        62\n",
      "\n",
      "Test Subject: 1105.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.71      0.47        24\n",
      "           1       0.84      0.53      0.65        68\n",
      "\n",
      "    accuracy                           0.58        92\n",
      "   macro avg       0.59      0.62      0.56        92\n",
      "weighted avg       0.71      0.58      0.60        92\n",
      "\n",
      "Test Subject: 1106.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      1.00      0.33         2\n",
      "           1       1.00      0.50      0.67        16\n",
      "\n",
      "    accuracy                           0.56        18\n",
      "   macro avg       0.60      0.75      0.50        18\n",
      "weighted avg       0.91      0.56      0.63        18\n",
      "\n",
      "Test Subject: 1241.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        11\n",
      "   macro avg       1.00      1.00      1.00        11\n",
      "weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "Test Subject: 1271.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.64      0.46        25\n",
      "           1       0.80      0.56      0.66        66\n",
      "\n",
      "    accuracy                           0.58        91\n",
      "   macro avg       0.58      0.60      0.56        91\n",
      "weighted avg       0.68      0.58      0.60        91\n",
      "\n",
      "Test Subject: 1314.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        12\n",
      "   macro avg       1.00      1.00      1.00        12\n",
      "weighted avg       1.00      1.00      1.00        12\n",
      "\n",
      "Test Subject: 1323.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73        23\n",
      "           1       0.55      0.43      0.48        14\n",
      "\n",
      "    accuracy                           0.65        37\n",
      "   macro avg       0.62      0.61      0.61        37\n",
      "weighted avg       0.64      0.65      0.64        37\n",
      "\n",
      "Test Subject: 1337.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.54      0.70        24\n",
      "           1       0.08      1.00      0.15         1\n",
      "\n",
      "    accuracy                           0.56        25\n",
      "   macro avg       0.54      0.77      0.43        25\n",
      "weighted avg       0.96      0.56      0.68        25\n",
      "\n",
      "Test Subject: 1417.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      1.00      0.24        10\n",
      "           1       1.00      0.38      0.56       104\n",
      "\n",
      "    accuracy                           0.44       114\n",
      "   macro avg       0.57      0.69      0.40       114\n",
      "weighted avg       0.92      0.44      0.53       114\n",
      "\n",
      "Test Subject: 1434.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.56      0.67         9\n",
      "           1       0.90      0.97      0.93        36\n",
      "\n",
      "    accuracy                           0.89        45\n",
      "   macro avg       0.87      0.76      0.80        45\n",
      "weighted avg       0.88      0.89      0.88        45\n",
      "\n",
      "Test Subject: 1544.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.48      0.65        21\n",
      "           1       0.15      1.00      0.27         2\n",
      "\n",
      "    accuracy                           0.52        23\n",
      "   macro avg       0.58      0.74      0.46        23\n",
      "weighted avg       0.93      0.52      0.61        23\n",
      "\n",
      "Test Subject: 1547.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.87      0.87      0.87        30\n",
      "\n",
      "    accuracy                           0.76        34\n",
      "   macro avg       0.43      0.43      0.43        34\n",
      "weighted avg       0.76      0.76      0.76        34\n",
      "\n",
      "Test Subject: 1595.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.78        10\n",
      "           1       0.86      0.60      0.71        10\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.77      0.75      0.74        20\n",
      "weighted avg       0.77      0.75      0.74        20\n",
      "\n",
      "Test Subject: 1629.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40         4\n",
      "           1       0.40      1.00      0.57         2\n",
      "\n",
      "    accuracy                           0.50         6\n",
      "   macro avg       0.70      0.62      0.49         6\n",
      "weighted avg       0.80      0.50      0.46         6\n",
      "\n",
      "Test Subject: 1716.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.70        51\n",
      "           1       0.67      0.57      0.61        46\n",
      "\n",
      "    accuracy                           0.66        97\n",
      "   macro avg       0.66      0.66      0.65        97\n",
      "weighted avg       0.66      0.66      0.66        97\n",
      "\n",
      "Test Subject: 1717.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76        14\n",
      "           1       1.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.61        23\n",
      "   macro avg       0.80      0.50      0.38        23\n",
      "weighted avg       0.76      0.61      0.46        23\n",
      "\n",
      "Test Subject: 1744.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.67      0.36        15\n",
      "           1       0.58      0.19      0.29        37\n",
      "\n",
      "    accuracy                           0.33        52\n",
      "   macro avg       0.42      0.43      0.32        52\n",
      "weighted avg       0.49      0.33      0.31        52\n",
      "\n",
      "Test Subject: 1868.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.11      0.19        19\n",
      "           1       0.15      1.00      0.26         3\n",
      "\n",
      "    accuracy                           0.23        22\n",
      "   macro avg       0.57      0.55      0.23        22\n",
      "weighted avg       0.88      0.23      0.20        22\n",
      "\n",
      "Test Subject: 1892.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.83      0.71         6\n",
      "           1       0.89      0.73      0.80        11\n",
      "\n",
      "    accuracy                           0.76        17\n",
      "   macro avg       0.76      0.78      0.76        17\n",
      "weighted avg       0.80      0.76      0.77        17\n",
      "\n",
      "Test Subject: 1953.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60         8\n",
      "           1       0.85      0.65      0.73        17\n",
      "\n",
      "    accuracy                           0.68        25\n",
      "   macro avg       0.67      0.70      0.67        25\n",
      "weighted avg       0.74      0.68      0.69        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def losoValidation(folder):\n",
    "    dr_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\{}'.format(folder) # Norm_ECG_EDA_Features_Combined_scld\n",
    "    mat_feat_path = r'X:\\RealTimeSegment\\MatbII\\Extracted\\{}'.format(folder)\n",
    "    bas_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\Norm_ECG_EDA_Features_Baseline_Combined'\n",
    "\n",
    "    xtrainBas, yBas = load_viragebase(bas_feat_path, 'label')\n",
    "    xtrainMat, yMat = load_matb(mat_feat_path, 'scaled label')\n",
    "\n",
    "    subjects = os.listdir(dr_feat_path)\n",
    "    xtrainDriv = pd.DataFrame()\n",
    "\n",
    "    for sdriv in subjects:\n",
    "\n",
    "        xtestDriv = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(sdriv)))\n",
    "\n",
    "        xtestDriv[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0)\n",
    "        xtestDriv.replace([np.inf, -np.inf], 0, inplace=True)        \n",
    "        xtestDriv.dropna(inplace=True) # .reset_index(drop=True, inplace=True)\n",
    "\n",
    "        ytestDriv = list(xtestDriv['label'].copy())\n",
    "\n",
    "        xtrainDriv = pd.DataFrame()\n",
    "        for subTrain in subjects:\n",
    "            if subTrain != sdriv:\n",
    "                train = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(subTrain)))\n",
    "\n",
    "                train[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0)\n",
    "                train.replace([np.inf, -np.inf], 0, inplace=True)        \n",
    "                train.dropna(inplace=True)\n",
    "                xtrainDriv = xtrainDriv.append(train)\n",
    "                xtrainDriv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        XtrainDriv = xtrainDriv[SELECTCOLS].copy()\n",
    "        xtestDriv = xtestDriv[SELECTCOLS].copy()\n",
    "        ytrainDriv = list(XtrainDriv['scaled label'].copy())\n",
    "\n",
    "        XtrainDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "        xtestDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "\n",
    "        XtrainDriv = XtrainDriv.append(xtrainMat)\n",
    "        XtrainDriv = XtrainDriv.append(xtrainBas)\n",
    "\n",
    "        ytrain = ytrainDriv + yMat + yBas\n",
    "        X = XtrainDriv.values\n",
    "\n",
    "        for idx, val in enumerate(ytrain):\n",
    "            if val <= 4:\n",
    "                ytrain[idx] = 0\n",
    "            else: ytrain[idx] = 1\n",
    "\n",
    "        for idx, val in enumerate(ytestDriv):\n",
    "            if val <= 4:\n",
    "                ytestDriv[idx] = 0\n",
    "            else: ytestDriv[idx] = 1\n",
    "\n",
    "        paramsrf = {\n",
    "            'n_estimators': 3000,\n",
    "            'min_samples_split': 2,\n",
    "            'min_samples_leaf': 1,\n",
    "            'max_features': 'sqrt',\n",
    "            'max_depth': 50,\n",
    "            'bootstrap': False\n",
    "            }\n",
    "\n",
    "        paramlgbm = {\n",
    "            'n_estimators': 3000,\n",
    "            'num_leaves': 100,\n",
    "            'learning_rate': 0.05,\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 24\n",
    "            }\n",
    "\n",
    "        clf2 = RandomForestClassifier(**paramsrf)\n",
    "        clf3 = lightgbm.LGBMClassifier(**paramlgbm)\n",
    "        clf4 = SVC(C=700, probability=True, class_weight='balanced')\n",
    "\n",
    "        estimatorList = [('rf', clf2), ('svm', clf4), ('gbm', clf3)]\n",
    "        eclf = VotingClassifier(estimators=estimatorList, voting='soft')\n",
    "\n",
    "        # clf2 = RandomForestClassifier(**paramsrf)\n",
    "        # clf3 = lightgbm.LGBMClassifier(n_estimators = 3000, num_leaves=100, learning_rate=0.005, class_weight='balanced')\n",
    "        # clf4 = SVC(C=1000, probability=True, class_weight='balanced')\n",
    "        # eclf = VotingClassifier(estimators=[('rf', clf2), ('gbm', clf3), ('svm', clf4)], voting='soft') # \n",
    "\n",
    "        hist = eclf.fit(X, ytrain)\n",
    "        yPred = hist.predict(xtestDriv)\n",
    "        print('Test Subject: {}'.format(sdriv))\n",
    "        print(classification_report(ytestDriv, yPred, zero_division=1))            \n",
    "\n",
    "losoValidation('Norm_ECG_EDA_Features_Combined_scld')\n",
    "    # return XtrainDriv, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Subject: 1030.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.95      0.55        19\n",
      "           1       0.93      0.33      0.48        43\n",
      "\n",
      "    accuracy                           0.52        62\n",
      "   macro avg       0.66      0.64      0.51        62\n",
      "weighted avg       0.76      0.52      0.50        62\n",
      "\n",
      "Test Subject: 1105.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.88      0.49        24\n",
      "           1       0.90      0.40      0.55        68\n",
      "\n",
      "    accuracy                           0.52        92\n",
      "   macro avg       0.62      0.64      0.52        92\n",
      "weighted avg       0.75      0.52      0.53        92\n",
      "\n",
      "Test Subject: 1106.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      1.00      0.33         2\n",
      "           1       1.00      0.50      0.67        16\n",
      "\n",
      "    accuracy                           0.56        18\n",
      "   macro avg       0.60      0.75      0.50        18\n",
      "weighted avg       0.91      0.56      0.63        18\n",
      "\n",
      "Test Subject: 1241.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        11\n",
      "   macro avg       1.00      1.00      1.00        11\n",
      "weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "Test Subject: 1271.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.56      0.41        25\n",
      "           1       0.77      0.56      0.65        66\n",
      "\n",
      "    accuracy                           0.56        91\n",
      "   macro avg       0.55      0.56      0.53        91\n",
      "weighted avg       0.65      0.56      0.58        91\n",
      "\n",
      "Test Subject: 1314.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        12\n",
      "   macro avg       1.00      1.00      1.00        12\n",
      "weighted avg       1.00      1.00      1.00        12\n",
      "\n",
      "Test Subject: 1323.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.48      0.52        23\n",
      "           1       0.33      0.43      0.38        14\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.46      0.45      0.45        37\n",
      "weighted avg       0.49      0.46      0.47        37\n",
      "\n",
      "Test Subject: 1337.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.46      0.63        24\n",
      "           1       0.07      1.00      0.13         1\n",
      "\n",
      "    accuracy                           0.48        25\n",
      "   macro avg       0.54      0.73      0.38        25\n",
      "weighted avg       0.96      0.48      0.61        25\n",
      "\n",
      "Test Subject: 1417.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      1.00      0.26        10\n",
      "           1       1.00      0.45      0.62       104\n",
      "\n",
      "    accuracy                           0.50       114\n",
      "   macro avg       0.57      0.73      0.44       114\n",
      "weighted avg       0.93      0.50      0.59       114\n",
      "\n",
      "Test Subject: 1434.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.22      0.36         9\n",
      "           1       0.84      1.00      0.91        36\n",
      "\n",
      "    accuracy                           0.84        45\n",
      "   macro avg       0.92      0.61      0.64        45\n",
      "weighted avg       0.87      0.84      0.80        45\n",
      "\n",
      "Test Subject: 1544.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.29      0.44        21\n",
      "           1       0.12      1.00      0.21         2\n",
      "\n",
      "    accuracy                           0.35        23\n",
      "   macro avg       0.56      0.64      0.33        23\n",
      "weighted avg       0.92      0.35      0.42        23\n",
      "\n",
      "Test Subject: 1547.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.25      0.20         4\n",
      "           1       0.89      0.83      0.86        30\n",
      "\n",
      "    accuracy                           0.76        34\n",
      "   macro avg       0.53      0.54      0.53        34\n",
      "weighted avg       0.81      0.76      0.78        34\n",
      "\n",
      "Test Subject: 1595.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70        10\n",
      "           1       0.70      0.70      0.70        10\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.70      0.70      0.70        20\n",
      "weighted avg       0.70      0.70      0.70        20\n",
      "\n",
      "Test Subject: 1629.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00         4\n",
      "           1       0.33      1.00      0.50         2\n",
      "\n",
      "    accuracy                           0.33         6\n",
      "   macro avg       0.67      0.50      0.25         6\n",
      "weighted avg       0.78      0.33      0.17         6\n",
      "\n",
      "Test Subject: 1716.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67        51\n",
      "           1       0.63      0.48      0.54        46\n",
      "\n",
      "    accuracy                           0.62        97\n",
      "   macro avg       0.62      0.61      0.61        97\n",
      "weighted avg       0.62      0.62      0.61        97\n",
      "\n",
      "Test Subject: 1717.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80        14\n",
      "           1       1.00      0.22      0.36         9\n",
      "\n",
      "    accuracy                           0.70        23\n",
      "   macro avg       0.83      0.61      0.58        23\n",
      "weighted avg       0.80      0.70      0.63        23\n",
      "\n",
      "Test Subject: 1892.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         6\n",
      "           1       0.85      1.00      0.92        11\n",
      "\n",
      "    accuracy                           0.88        17\n",
      "   macro avg       0.92      0.83      0.86        17\n",
      "weighted avg       0.90      0.88      0.88        17\n",
      "\n",
      "Test Subject: 1953.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.75      0.63         8\n",
      "           1       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.72        25\n",
      "   macro avg       0.70      0.73      0.70        25\n",
      "weighted avg       0.76      0.72      0.73        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def losoValidation(folder):\n",
    "    dr_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\{}'.format(folder) # Norm_ECG_EDA_Features_Combined_scld\n",
    "    mat_feat_path = r'X:\\RealTimeSegment\\MatbII\\Extracted\\{}'.format(folder)\n",
    "    bas_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\Norm_ECG_EDA_Features_Baseline_Combined'\n",
    "\n",
    "    xtrainBas, yBas = load_viragebase(bas_feat_path, 'label')\n",
    "    xtrainMat, yMat = load_matb(mat_feat_path, 'scaled label')\n",
    "\n",
    "    subjects = os.listdir(dr_feat_path)\n",
    "    xtrainDriv = pd.DataFrame()\n",
    "\n",
    "    for sdriv in subjects:\n",
    "        if sdriv in ['1868.csv', '1744.csv']:\n",
    "            continue\n",
    "        xtestDriv = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(sdriv)))\n",
    "\n",
    "        xtestDriv[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0)\n",
    "        xtestDriv.replace([np.inf, -np.inf], 0, inplace=True)        \n",
    "        xtestDriv.dropna(inplace=True) # .reset_index(drop=True, inplace=True)\n",
    "\n",
    "        ytestDriv = list(xtestDriv['label'].copy())\n",
    "\n",
    "        xtrainDriv = pd.DataFrame()\n",
    "        for subTrain in subjects:\n",
    "            if subTrain != sdriv:\n",
    "                train = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(subTrain)))\n",
    "\n",
    "                train[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0)\n",
    "                train.replace([np.inf, -np.inf], 0, inplace=True)        \n",
    "                train.dropna(inplace=True)\n",
    "                xtrainDriv = xtrainDriv.append(train)\n",
    "                xtrainDriv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        XtrainDriv = xtrainDriv[SELECTCOLS].copy()\n",
    "        xtestDriv = xtestDriv[SELECTCOLS].copy()\n",
    "        ytrainDriv = list(XtrainDriv['scaled label'].copy())\n",
    "\n",
    "        XtrainDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "        xtestDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "\n",
    "        XtrainDriv = XtrainDriv.append(xtrainMat)\n",
    "        XtrainDriv = XtrainDriv.append(xtrainBas)\n",
    "\n",
    "        ytrain = ytrainDriv + yMat + yBas\n",
    "        X = XtrainDriv.values\n",
    "\n",
    "        for idx, val in enumerate(ytrain):\n",
    "            if val <= 4:\n",
    "                ytrain[idx] = 0\n",
    "            else: ytrain[idx] = 1\n",
    "\n",
    "        for idx, val in enumerate(ytestDriv):\n",
    "            if val <= 4:\n",
    "                ytestDriv[idx] = 0\n",
    "            else: ytestDriv[idx] = 1\n",
    "\n",
    "        paramsrf = {\n",
    "            'n_estimators': 3000,\n",
    "            'min_samples_split': 2,\n",
    "            'min_samples_leaf': 1,\n",
    "            'max_features': 'sqrt',\n",
    "            'max_depth': 50,\n",
    "            'bootstrap': False\n",
    "            }\n",
    "\n",
    "        paramlgbm = {\n",
    "            'n_estimators': 3000,\n",
    "            'num_leaves': 100,\n",
    "            'learning_rate': 0.001,\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 24\n",
    "            }\n",
    "\n",
    "        clf2 = RandomForestClassifier(**paramsrf)\n",
    "        clf3 = lightgbm.LGBMClassifier(**paramlgbm)\n",
    "        clf4 = SVC(C=700, probability=True, class_weight='balanced', kernel='rbf')\n",
    "\n",
    "        estimatorList = [('rf', clf2), ('svm', clf4), ('gbm', clf3)] #  \n",
    "        eclf = VotingClassifier(estimators=estimatorList, voting='soft')\n",
    "\n",
    "        # clf2 = RandomForestClassifier(**paramsrf)\n",
    "        # clf3 = lightgbm.LGBMClassifier(n_estimators = 3000, num_leaves=100, learning_rate=0.005, class_weight='balanced')\n",
    "        # clf4 = SVC(C=1000, probability=True, class_weight='balanced')\n",
    "        # eclf = VotingClassifier(estimators=[('rf', clf2), ('gbm', clf3), ('svm', clf4)], voting='soft') # \n",
    "\n",
    "        hist = eclf.fit(X, ytrain)\n",
    "        yPred = hist.predict(xtestDriv)\n",
    "        print('Test Subject: {}'.format(sdriv))\n",
    "        print(classification_report(ytestDriv, yPred, zero_division=1))            \n",
    "\n",
    "losoValidation('Norm_ECG_EDA_Features_Combined_scld')\n",
    "    # return XtrainDriv, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Subject: 1030.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      1.00      0.51        19\n",
      "           1       1.00      0.14      0.24        43\n",
      "\n",
      "    accuracy                           0.40        62\n",
      "   macro avg       0.67      0.57      0.38        62\n",
      "weighted avg       0.80      0.40      0.33        62\n",
      "\n",
      "Test Subject: 1105.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.71      0.45        24\n",
      "           1       0.82      0.49      0.61        68\n",
      "\n",
      "    accuracy                           0.54        92\n",
      "   macro avg       0.58      0.60      0.53        92\n",
      "weighted avg       0.70      0.54      0.57        92\n",
      "\n",
      "Test Subject: 1106.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.50      0.18         2\n",
      "           1       0.89      0.50      0.64        16\n",
      "\n",
      "    accuracy                           0.50        18\n",
      "   macro avg       0.50      0.50      0.41        18\n",
      "weighted avg       0.80      0.50      0.59        18\n",
      "\n",
      "Test Subject: 1241.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        11\n",
      "   macro avg       1.00      1.00      1.00        11\n",
      "weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "Test Subject: 1271.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.56      0.43        25\n",
      "           1       0.78      0.61      0.68        66\n",
      "\n",
      "    accuracy                           0.59        91\n",
      "   macro avg       0.57      0.58      0.56        91\n",
      "weighted avg       0.66      0.59      0.61        91\n",
      "\n",
      "Test Subject: 1314.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        12\n",
      "   macro avg       1.00      1.00      1.00        12\n",
      "weighted avg       1.00      1.00      1.00        12\n",
      "\n",
      "Test Subject: 1323.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75        23\n",
      "           1       0.58      0.50      0.54        14\n",
      "\n",
      "    accuracy                           0.68        37\n",
      "   macro avg       0.65      0.64      0.64        37\n",
      "weighted avg       0.67      0.68      0.67        37\n",
      "\n",
      "Test Subject: 1337.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67        24\n",
      "           1       0.08      1.00      0.14         1\n",
      "\n",
      "    accuracy                           0.52        25\n",
      "   macro avg       0.54      0.75      0.40        25\n",
      "weighted avg       0.96      0.52      0.65        25\n",
      "\n",
      "Test Subject: 1417.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      1.00      0.26        10\n",
      "           1       1.00      0.46      0.63       104\n",
      "\n",
      "    accuracy                           0.51       114\n",
      "   macro avg       0.58      0.73      0.45       114\n",
      "weighted avg       0.93      0.51      0.60       114\n",
      "\n",
      "Test Subject: 1434.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.56      0.67         9\n",
      "           1       0.90      0.97      0.93        36\n",
      "\n",
      "    accuracy                           0.89        45\n",
      "   macro avg       0.87      0.76      0.80        45\n",
      "weighted avg       0.88      0.89      0.88        45\n",
      "\n",
      "Test Subject: 1544.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.57      0.73        21\n",
      "           1       0.18      1.00      0.31         2\n",
      "\n",
      "    accuracy                           0.61        23\n",
      "   macro avg       0.59      0.79      0.52        23\n",
      "weighted avg       0.93      0.61      0.69        23\n",
      "\n",
      "Test Subject: 1547.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.86      0.80      0.83        30\n",
      "\n",
      "    accuracy                           0.71        34\n",
      "   macro avg       0.43      0.40      0.41        34\n",
      "weighted avg       0.76      0.71      0.73        34\n",
      "\n",
      "Test Subject: 1595.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.70      0.67        10\n",
      "           1       0.67      0.60      0.63        10\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.65      0.65      0.65        20\n",
      "weighted avg       0.65      0.65      0.65        20\n",
      "\n",
      "Test Subject: 1629.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.25      0.33         4\n",
      "           1       0.25      0.50      0.33         2\n",
      "\n",
      "    accuracy                           0.33         6\n",
      "   macro avg       0.38      0.38      0.33         6\n",
      "weighted avg       0.42      0.33      0.33         6\n",
      "\n",
      "Test Subject: 1716.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70        51\n",
      "           1       0.67      0.61      0.64        46\n",
      "\n",
      "    accuracy                           0.67        97\n",
      "   macro avg       0.67      0.67      0.67        97\n",
      "weighted avg       0.67      0.67      0.67        97\n",
      "\n",
      "Test Subject: 1717.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76        14\n",
      "           1       1.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.61        23\n",
      "   macro avg       0.80      0.50      0.38        23\n",
      "weighted avg       0.76      0.61      0.46        23\n",
      "\n",
      "Test Subject: 1744.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.60      0.33        15\n",
      "           1       0.50      0.16      0.24        37\n",
      "\n",
      "    accuracy                           0.29        52\n",
      "   macro avg       0.36      0.38      0.29        52\n",
      "weighted avg       0.42      0.29      0.27        52\n",
      "\n",
      "Test Subject: 1868.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.05      0.10        19\n",
      "           1       0.14      1.00      0.25         3\n",
      "\n",
      "    accuracy                           0.18        22\n",
      "   macro avg       0.57      0.53      0.17        22\n",
      "weighted avg       0.88      0.18      0.12        22\n",
      "\n",
      "Test Subject: 1892.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77         6\n",
      "           1       0.90      0.82      0.86        11\n",
      "\n",
      "    accuracy                           0.82        17\n",
      "   macro avg       0.81      0.83      0.81        17\n",
      "weighted avg       0.83      0.82      0.83        17\n",
      "\n",
      "Test Subject: 1953.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60         8\n",
      "           1       0.85      0.65      0.73        17\n",
      "\n",
      "    accuracy                           0.68        25\n",
      "   macro avg       0.67      0.70      0.67        25\n",
      "weighted avg       0.74      0.68      0.69        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def losoValidation(folder):\n",
    "    dr_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\{}'.format(folder) # Norm_ECG_EDA_Features_Combined_scld\n",
    "    mat_feat_path = r'X:\\RealTimeSegment\\MatbII\\Extracted_0\\{}'.format(folder)\n",
    "    bas_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\Norm_ECG_EDA_Features_Baseline_Combined'\n",
    "\n",
    "    xtrainBas, yBas = load_viragebase(bas_feat_path, 'label')\n",
    "    xtrainMat, yMat = load_matb(mat_feat_path, 'scaled label')\n",
    "\n",
    "    subjects = os.listdir(dr_feat_path)\n",
    "    xtrainDriv = pd.DataFrame()\n",
    "\n",
    "    for sdriv in subjects:\n",
    "        xtestDriv = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(sdriv)))\n",
    "\n",
    "        xtestDriv[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0)\n",
    "        xtestDriv.replace([np.inf, -np.inf], 0, inplace=True)        \n",
    "        xtestDriv.dropna(inplace=True) # .reset_index(drop=True, inplace=True)\n",
    "\n",
    "        ytestDriv = list(xtestDriv['label'].copy())\n",
    "\n",
    "        xtrainDriv = pd.DataFrame()\n",
    "        for subTrain in subjects:\n",
    "            if subTrain != sdriv:\n",
    "                train = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(subTrain)))\n",
    "\n",
    "                train[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0)\n",
    "                train.replace([np.inf, -np.inf], 0, inplace=True)        \n",
    "                train.dropna(inplace=True)\n",
    "                xtrainDriv = xtrainDriv.append(train)\n",
    "                xtrainDriv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        XtrainDriv = xtrainDriv[SELECTCOLS].copy()\n",
    "        xtestDriv = xtestDriv[SELECTCOLS].copy()\n",
    "        ytrainDriv = list(XtrainDriv['scaled label'].copy())\n",
    "\n",
    "        XtrainDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "        xtestDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "\n",
    "        XtrainDriv = XtrainDriv.append(xtrainMat)\n",
    "        XtrainDriv = XtrainDriv.append(xtrainBas)\n",
    "\n",
    "        ytrain = ytrainDriv + yMat + yBas\n",
    "        X = XtrainDriv.values\n",
    "\n",
    "        for idx, val in enumerate(ytrain):\n",
    "            if val <= 4:\n",
    "                ytrain[idx] = 0\n",
    "            else: ytrain[idx] = 1\n",
    "\n",
    "        for idx, val in enumerate(ytestDriv):\n",
    "            if val <= 4:\n",
    "                ytestDriv[idx] = 0\n",
    "            else: ytestDriv[idx] = 1\n",
    "\n",
    "        paramsrf = {\n",
    "            'n_estimators': 3000,\n",
    "            'min_samples_split': 2,\n",
    "            'min_samples_leaf': 1,\n",
    "            'max_features': 'sqrt',\n",
    "            'max_depth': 50,\n",
    "            'bootstrap': False\n",
    "            }\n",
    "\n",
    "        paramlgbm = {\n",
    "            'n_estimators': 3000,\n",
    "            'num_leaves': 100,\n",
    "            'learning_rate': 0.05,\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 24\n",
    "            }\n",
    "\n",
    "        clf2 = RandomForestClassifier(**paramsrf)\n",
    "        clf3 = lightgbm.LGBMClassifier(**paramlgbm)\n",
    "        clf4 = SVC(C=700, probability=True, class_weight='balanced')\n",
    "\n",
    "        estimatorList = [('rf', clf2), ('svm', clf4), ('gbm', clf3)]\n",
    "        eclf = VotingClassifier(estimators=estimatorList, voting='soft')\n",
    "\n",
    "        # clf2 = RandomForestClassifier(**paramsrf)\n",
    "        # clf3 = lightgbm.LGBMClassifier(n_estimators = 3000, num_leaves=100, learning_rate=0.005, class_weight='balanced')\n",
    "        # clf4 = SVC(C=1000, probability=True, class_weight='balanced')\n",
    "        # eclf = VotingClassifier(estimators=[('rf', clf2), ('gbm', clf3), ('svm', clf4)], voting='soft') # \n",
    "\n",
    "        hist = eclf.fit(X, ytrain)\n",
    "        yPred = hist.predict(xtestDriv)\n",
    "        print('Test Subject: {}'.format(sdriv))\n",
    "        print(classification_report(ytestDriv, yPred, zero_division=1))            \n",
    "\n",
    "losoValidation('Norm_ECG_EDA_Features_Combined_scld')\n",
    "    # return XtrainDriv, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Subject: 1030.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.97      0.50        38\n",
      "           1       0.97      0.31      0.47       104\n",
      "\n",
      "    accuracy                           0.49       142\n",
      "   macro avg       0.65      0.64      0.49       142\n",
      "weighted avg       0.80      0.49      0.48       142\n",
      "\n",
      "Test Subject: 1105.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.94      0.39        31\n",
      "           1       0.95      0.32      0.48       128\n",
      "\n",
      "    accuracy                           0.44       159\n",
      "   macro avg       0.60      0.63      0.44       159\n",
      "weighted avg       0.82      0.44      0.46       159\n",
      "\n",
      "Test Subject: 1106.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.90      0.34        21\n",
      "           1       0.89      0.18      0.30        87\n",
      "\n",
      "    accuracy                           0.32       108\n",
      "   macro avg       0.55      0.54      0.32       108\n",
      "weighted avg       0.76      0.32      0.31       108\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1348/2108033721.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytestDriv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myPred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m \u001b[0mlosoValidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Norm_ECG_EDA_Features_Combined_scld'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[1;31m# return XtrainDriv, y_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1348/2108033721.py\u001b[0m in \u001b[0;36mlosoValidation\u001b[1;34m(folder)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# eclf = VotingClassifier(estimators=[('rf', clf2), ('gbm', clf3), ('svm', clf4)], voting='soft') #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0myPred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtestDriv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test Subject: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msdriv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m                              % (len(self.weights), len(self.estimators)))\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[0;32m     75\u001b[0m                 delayed(_fit_single_estimator)(\n\u001b[0;32m     76\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\ensemble\\_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[1;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    388\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \"\"\"\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    899\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_classification\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \"\"\"\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[0my_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    182\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'continuous'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'multiclass'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msuffix\u001b[0m  \u001b[1;31m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ideas\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def losoValidation(folder):\n",
    "    dr_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\{}'.format(folder) # Norm_ECG_EDA_Features_Combined_scld\n",
    "    mat_feat_path = r'X:\\RealTimeSegment\\MatbII\\Extracted\\{}'.format(folder)\n",
    "    bas_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\Norm_ECG_EDA_Features_Baseline_Combined'\n",
    "\n",
    "    xtrainBas, yBas = load_viragebase(bas_feat_path, 'label')\n",
    "    xtrainMat, yMat = load_matb(mat_feat_path, 'scaled label')\n",
    "\n",
    "    subjects = os.listdir(dr_feat_path)\n",
    "    xtrainDriv = pd.DataFrame()\n",
    "\n",
    "    for sdriv in subjects:\n",
    "        if sdriv not in ['1629.csv']:\n",
    "            continue\n",
    "        xtestDriv = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(sdriv)))\n",
    "\n",
    "        xtestDriv[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0)\n",
    "        xtestDriv.replace([np.inf, -np.inf], 0, inplace=True)  \n",
    "        xtestDriv.dropna(inplace=True) # .reset_index(drop=True, inplace=True)\n",
    "\n",
    "        ytestDriv = list(xtestDriv['label'].copy())\n",
    "\n",
    "        xtrainDriv = pd.DataFrame()\n",
    "        for subTrain in subjects:\n",
    "            if subTrain != sdriv:\n",
    "                train = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(subTrain)))\n",
    "\n",
    "                train[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0)\n",
    "                train.replace([np.inf, -np.inf], 0, inplace=True)        \n",
    "                train.dropna(inplace=True)\n",
    "                xtrainDriv = xtrainDriv.append(train)\n",
    "                xtrainDriv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        XtrainDriv = xtrainDriv[SELECTCOLS].copy()\n",
    "        xtestDriv = xtestDriv[SELECTCOLS].copy()\n",
    "        ytrainDriv = list(XtrainDriv['scaled label'].copy())\n",
    "\n",
    "        XtrainDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "        xtestDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "\n",
    "        XtrainDriv = XtrainDriv.append(xtrainMat)\n",
    "        XtrainDriv = XtrainDriv.append(xtrainBas)\n",
    "\n",
    "        ytrain = ytrainDriv + yMat + yBas\n",
    "        X = XtrainDriv.values\n",
    "\n",
    "        for idx, val in enumerate(ytrain):\n",
    "            if val <= 4:\n",
    "                ytrain[idx] = 0\n",
    "            else: ytrain[idx] = 1\n",
    "\n",
    "        for idx, val in enumerate(ytestDriv):\n",
    "            if val <= 4:\n",
    "                ytestDriv[idx] = 0\n",
    "            else: ytestDriv[idx] = 1\n",
    "\n",
    "        paramsrf = {\n",
    "            'n_estimators': 3000,\n",
    "            'min_samples_split': 2,\n",
    "            'min_samples_leaf': 1,\n",
    "            'max_features': 'sqrt',\n",
    "            'max_depth': 50,\n",
    "            'bootstrap': False\n",
    "            }\n",
    "\n",
    "        paramlgbm = {\n",
    "            'n_estimators': 3000,\n",
    "            'num_leaves': 100,\n",
    "            'learning_rate': 0.05,\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 24\n",
    "            }\n",
    "\n",
    "        clf2 = RandomForestClassifier(**paramsrf)\n",
    "        clf3 = lightgbm.LGBMClassifier(**paramlgbm)\n",
    "        clf4 = SVC(C=700, probability=True, class_weight='balanced')\n",
    "\n",
    "        estimatorList = [('rf', clf2), ('svm', clf4), ('gbm', clf3)]\n",
    "        eclf = VotingClassifier(estimators=estimatorList, voting='soft')\n",
    "\n",
    "        # clf2 = RandomForestClassifier(**paramsrf)\n",
    "        # clf3 = lightgbm.LGBMClassifier(n_estimators = 3000, num_leaves=100, learning_rate=0.005, class_weight='balanced')\n",
    "        # clf4 = SVC(C=1000, probability=True, class_weight='balanced')\n",
    "        # eclf = VotingClassifier(estimators=[('rf', clf2), ('gbm', clf3), ('svm', clf4)], voting='soft') # \n",
    "\n",
    "        hist = eclf.fit(X, ytrain)\n",
    "        yPred = hist.predict(xtestDriv)\n",
    "        print('Test Subject: {}'.format(sdriv))\n",
    "        print(classification_report(ytestDriv, yPred, zero_division=1))            \n",
    "\n",
    "losoValidation('Norm_ECG_EDA_Features_Combined_scld')\n",
    "    # return XtrainDriv, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrected MatbII features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Subject: 1030.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      1.00      0.54        19\n",
      "           1       1.00      0.23      0.38        43\n",
      "\n",
      "    accuracy                           0.47        62\n",
      "   macro avg       0.68      0.62      0.46        62\n",
      "weighted avg       0.81      0.47      0.43        62\n",
      "\n",
      "Test Subject: 1105.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.88      0.49        24\n",
      "           1       0.90      0.41      0.57        68\n",
      "\n",
      "    accuracy                           0.53        92\n",
      "   macro avg       0.62      0.64      0.53        92\n",
      "weighted avg       0.76      0.53      0.55        92\n",
      "\n",
      "Test Subject: 1106.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      1.00      0.22         2\n",
      "           1       1.00      0.12      0.22        16\n",
      "\n",
      "    accuracy                           0.22        18\n",
      "   macro avg       0.56      0.56      0.22        18\n",
      "weighted avg       0.90      0.22      0.22        18\n",
      "\n",
      "Test Subject: 1241.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        11\n",
      "   macro avg       1.00      1.00      1.00        11\n",
      "weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "Test Subject: 1271.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.40      0.29        25\n",
      "           1       0.69      0.50      0.58        66\n",
      "\n",
      "    accuracy                           0.47        91\n",
      "   macro avg       0.46      0.45      0.44        91\n",
      "weighted avg       0.56      0.47      0.50        91\n",
      "\n",
      "Test Subject: 1314.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        12\n",
      "   macro avg       1.00      1.00      1.00        12\n",
      "weighted avg       1.00      1.00      1.00        12\n",
      "\n",
      "Test Subject: 1323.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.83      0.75        23\n",
      "           1       0.56      0.36      0.43        14\n",
      "\n",
      "    accuracy                           0.65        37\n",
      "   macro avg       0.62      0.59      0.59        37\n",
      "weighted avg       0.63      0.65      0.63        37\n",
      "\n",
      "Test Subject: 1337.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.21      0.34        24\n",
      "           1       0.05      1.00      0.10         1\n",
      "\n",
      "    accuracy                           0.24        25\n",
      "   macro avg       0.53      0.60      0.22        25\n",
      "weighted avg       0.96      0.24      0.33        25\n",
      "\n",
      "Test Subject: 1417.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      1.00      0.24        10\n",
      "           1       1.00      0.40      0.58       104\n",
      "\n",
      "    accuracy                           0.46       114\n",
      "   macro avg       0.57      0.70      0.41       114\n",
      "weighted avg       0.92      0.46      0.55       114\n",
      "\n",
      "Test Subject: 1434.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.44      0.47         9\n",
      "           1       0.86      0.89      0.88        36\n",
      "\n",
      "    accuracy                           0.80        45\n",
      "   macro avg       0.68      0.67      0.67        45\n",
      "weighted avg       0.79      0.80      0.80        45\n",
      "\n",
      "Test Subject: 1544.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80        21\n",
      "           1       0.22      1.00      0.36         2\n",
      "\n",
      "    accuracy                           0.70        23\n",
      "   macro avg       0.61      0.83      0.58        23\n",
      "weighted avg       0.93      0.70      0.76        23\n",
      "\n",
      "Test Subject: 1547.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.25      0.33         4\n",
      "           1       0.91      0.97      0.94        30\n",
      "\n",
      "    accuracy                           0.88        34\n",
      "   macro avg       0.70      0.61      0.63        34\n",
      "weighted avg       0.86      0.88      0.86        34\n",
      "\n",
      "Test Subject: 1595.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73        10\n",
      "           1       0.75      0.60      0.67        10\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.71      0.70      0.70        20\n",
      "weighted avg       0.71      0.70      0.70        20\n",
      "\n",
      "Test Subject: 1629.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.50      0.57         4\n",
      "           1       0.33      0.50      0.40         2\n",
      "\n",
      "    accuracy                           0.50         6\n",
      "   macro avg       0.50      0.50      0.49         6\n",
      "weighted avg       0.56      0.50      0.51         6\n",
      "\n",
      "Test Subject: 1716.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.73      0.65        51\n",
      "           1       0.60      0.46      0.52        46\n",
      "\n",
      "    accuracy                           0.60        97\n",
      "   macro avg       0.60      0.59      0.59        97\n",
      "weighted avg       0.60      0.60      0.59        97\n",
      "\n",
      "Test Subject: 1717.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76        14\n",
      "           1       1.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.61        23\n",
      "   macro avg       0.80      0.50      0.38        23\n",
      "weighted avg       0.76      0.61      0.46        23\n",
      "\n",
      "Test Subject: 1744.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.53      0.30        15\n",
      "           1       0.46      0.16      0.24        37\n",
      "\n",
      "    accuracy                           0.27        52\n",
      "   macro avg       0.33      0.35      0.27        52\n",
      "weighted avg       0.39      0.27      0.26        52\n",
      "\n",
      "Test Subject: 1868.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.26      0.40        19\n",
      "           1       0.12      0.67      0.21         3\n",
      "\n",
      "    accuracy                           0.32        22\n",
      "   macro avg       0.48      0.46      0.31        22\n",
      "weighted avg       0.74      0.32      0.37        22\n",
      "\n",
      "Test Subject: 1892.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         6\n",
      "           1       0.79      1.00      0.88        11\n",
      "\n",
      "    accuracy                           0.82        17\n",
      "   macro avg       0.89      0.75      0.77        17\n",
      "weighted avg       0.86      0.82      0.80        17\n",
      "\n",
      "Test Subject: 1953.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.62      0.56         8\n",
      "           1       0.80      0.71      0.75        17\n",
      "\n",
      "    accuracy                           0.68        25\n",
      "   macro avg       0.65      0.67      0.65        25\n",
      "weighted avg       0.70      0.68      0.69        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def losoValidation(folder):\n",
    "    dr_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\{}'.format(folder) # Norm_ECG_EDA_Features_Combined_scld\n",
    "    mat_feat_path = r'X:\\RealTimeSegment\\MatbII\\Extracted\\{}'.format(folder)\n",
    "    bas_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\Norm_ECG_EDA_Features_Baseline_Combined'\n",
    "\n",
    "    xtrainBas, yBas = load_viragebase(bas_feat_path, 'label')\n",
    "    xtrainMat, yMat = load_matb(mat_feat_path, 'scaled label')\n",
    "\n",
    "    subjects = os.listdir(dr_feat_path)\n",
    "    xtrainDriv = pd.DataFrame()\n",
    "\n",
    "    for sdriv in subjects:\n",
    "        xtestDriv = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(sdriv)))\n",
    "\n",
    "        xtestDriv[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0)\n",
    "        xtestDriv.replace([np.inf, -np.inf], 0, inplace=True)        \n",
    "        xtestDriv.dropna(inplace=True) # .reset_index(drop=True, inplace=True)\n",
    "\n",
    "        ytestDriv = list(xtestDriv['label'].copy())\n",
    "\n",
    "        xtrainDriv = pd.DataFrame()\n",
    "        for subTrain in subjects:\n",
    "            if subTrain != sdriv:\n",
    "                train = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(subTrain)))\n",
    "\n",
    "                train[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0)\n",
    "                train.replace([np.inf, -np.inf], 0, inplace=True)        \n",
    "                train.dropna(inplace=True)\n",
    "                xtrainDriv = xtrainDriv.append(train)\n",
    "                xtrainDriv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        XtrainDriv = xtrainDriv[SELECTCOLS].copy()\n",
    "        xtestDriv = xtestDriv[SELECTCOLS].copy()\n",
    "        ytrainDriv = list(XtrainDriv['scaled label'].copy())\n",
    "\n",
    "        XtrainDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "        xtestDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "\n",
    "        XtrainDriv.reset_index(inplace=True, drop=True)\n",
    "        XtrainDriv = XtrainDriv.append(xtrainMat)\n",
    "        XtrainDriv.reset_index(inplace=True, drop=True)\n",
    "        XtrainDriv = XtrainDriv.append(xtrainBas)\n",
    "        XtrainDriv.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        ytrain = ytrainDriv + yMat + yBas\n",
    "        X = XtrainDriv.values\n",
    "\n",
    "        for idx, val in enumerate(ytrain):\n",
    "            if val <= 4:\n",
    "                ytrain[idx] = 0\n",
    "            else: ytrain[idx] = 1\n",
    "\n",
    "        for idx, val in enumerate(ytestDriv):\n",
    "            if val <= 4:\n",
    "                ytestDriv[idx] = 0\n",
    "            else: ytestDriv[idx] = 1\n",
    "\n",
    "        paramsrf = {\n",
    "            'n_estimators': 3000,\n",
    "            'min_samples_split': 2,\n",
    "            'min_samples_leaf': 1,\n",
    "            'max_features': 'sqrt',\n",
    "            'max_depth': 50,\n",
    "            'bootstrap': False\n",
    "            }\n",
    "\n",
    "        paramlgbm = {\n",
    "            'n_estimators': 3000,\n",
    "            'num_leaves': 100,\n",
    "            'learning_rate': 0.05,\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 24\n",
    "            }\n",
    "\n",
    "        clf2 = RandomForestClassifier(**paramsrf)\n",
    "        clf3 = lightgbm.LGBMClassifier(**paramlgbm)\n",
    "        clf4 = SVC(C=700, probability=True, class_weight='balanced')\n",
    "\n",
    "        estimatorList = [('rf', clf2), ('svm', clf4), ('gbm', clf3)]\n",
    "        eclf = VotingClassifier(estimators=estimatorList, voting='soft')\n",
    "\n",
    "        # clf2 = RandomForestClassifier(**paramsrf)\n",
    "        # clf3 = lightgbm.LGBMClassifier(n_estimators = 3000, num_leaves=100, learning_rate=0.005, class_weight='balanced')\n",
    "        # clf4 = SVC(C=1000, probability=True, class_weight='balanced')\n",
    "        # eclf = VotingClassifier(estimators=[('rf', clf2), ('gbm', clf3), ('svm', clf4)], voting='soft') # \n",
    "\n",
    "        hist = eclf.fit(X, ytrain)\n",
    "        yPred = hist.predict(xtestDriv)\n",
    "        print('Test Subject: {}'.format(sdriv))\n",
    "        print(classification_report(ytestDriv, yPred, zero_division=1))            \n",
    "\n",
    "losoValidation('Norm_ECG_EDA_Features_Combined_scld')\n",
    "    # return XtrainDriv, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Subject: 1030.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.79      0.47        38\n",
      "           1       0.89      0.51      0.65       122\n",
      "\n",
      "    accuracy                           0.57       160\n",
      "   macro avg       0.61      0.65      0.56       160\n",
      "weighted avg       0.75      0.57      0.60       160\n",
      "\n",
      "Test Subject: 1105.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.97      0.43        31\n",
      "           1       0.98      0.38      0.55       128\n",
      "\n",
      "    accuracy                           0.50       159\n",
      "   macro avg       0.63      0.68      0.49       159\n",
      "weighted avg       0.84      0.50      0.53       159\n",
      "\n",
      "Test Subject: 1106.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.88      0.33        24\n",
      "           1       0.95      0.40      0.56       136\n",
      "\n",
      "    accuracy                           0.47       160\n",
      "   macro avg       0.58      0.64      0.45       160\n",
      "weighted avg       0.84      0.47      0.53       160\n",
      "\n",
      "Test Subject: 1241.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.64      0.71       129\n",
      "           1       0.20      0.39      0.27        31\n",
      "\n",
      "    accuracy                           0.59       160\n",
      "   macro avg       0.51      0.51      0.49       160\n",
      "weighted avg       0.69      0.59      0.63       160\n",
      "\n",
      "Test Subject: 1271.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.20      0.15        30\n",
      "           1       0.74      0.60      0.66       113\n",
      "\n",
      "    accuracy                           0.52       143\n",
      "   macro avg       0.43      0.40      0.41       143\n",
      "weighted avg       0.61      0.52      0.56       143\n",
      "\n",
      "Test Subject: 1314.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.17      0.25        18\n",
      "           1       0.89      0.98      0.93       121\n",
      "\n",
      "    accuracy                           0.87       139\n",
      "   macro avg       0.69      0.57      0.59       139\n",
      "weighted avg       0.84      0.87      0.84       139\n",
      "\n",
      "Test Subject: 1323.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.38      0.50        84\n",
      "           1       0.45      0.77      0.57        56\n",
      "\n",
      "    accuracy                           0.54       140\n",
      "   macro avg       0.58      0.57      0.53       140\n",
      "weighted avg       0.61      0.54      0.53       140\n",
      "\n",
      "Test Subject: 1337.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.84       136\n",
      "           1       0.32      0.50      0.39        26\n",
      "\n",
      "    accuracy                           0.75       162\n",
      "   macro avg       0.60      0.65      0.61       162\n",
      "weighted avg       0.80      0.75      0.77       162\n",
      "\n",
      "Test Subject: 1372.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.20      0.33        90\n",
      "           1       0.50      1.00      0.67        72\n",
      "\n",
      "    accuracy                           0.56       162\n",
      "   macro avg       0.75      0.60      0.50       162\n",
      "weighted avg       0.78      0.56      0.48       162\n",
      "\n",
      "Test Subject: 1417.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.92      0.24        12\n",
      "           1       0.99      0.54      0.70       150\n",
      "\n",
      "    accuracy                           0.57       162\n",
      "   macro avg       0.56      0.73      0.47       162\n",
      "weighted avg       0.92      0.57      0.66       162\n",
      "\n",
      "Test Subject: 1434.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.39      0.39        36\n",
      "           1       0.81      0.82      0.81       114\n",
      "\n",
      "    accuracy                           0.71       150\n",
      "   macro avg       0.60      0.60      0.60       150\n",
      "weighted avg       0.71      0.71      0.71       150\n",
      "\n",
      "Test Subject: 1544.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.69      0.80       124\n",
      "           1       0.39      0.86      0.54        29\n",
      "\n",
      "    accuracy                           0.72       153\n",
      "   macro avg       0.67      0.77      0.67       153\n",
      "weighted avg       0.85      0.72      0.75       153\n",
      "\n",
      "Test Subject: 1547.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.18      0.24        28\n",
      "           1       0.81      0.92      0.86       107\n",
      "\n",
      "    accuracy                           0.76       135\n",
      "   macro avg       0.58      0.55      0.55       135\n",
      "weighted avg       0.72      0.76      0.73       135\n",
      "\n",
      "Test Subject: 1595.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.80      0.65        70\n",
      "           1       0.77      0.50      0.61        92\n",
      "\n",
      "    accuracy                           0.63       162\n",
      "   macro avg       0.66      0.65      0.63       162\n",
      "weighted avg       0.67      0.63      0.63       162\n",
      "\n",
      "Test Subject: 1629.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.69      0.56        39\n",
      "           1       0.83      0.66      0.73        87\n",
      "\n",
      "    accuracy                           0.67       126\n",
      "   macro avg       0.65      0.67      0.65       126\n",
      "weighted avg       0.72      0.67      0.68       126\n",
      "\n",
      "Test Subject: 1716.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70        89\n",
      "           1       0.59      0.52      0.55        66\n",
      "\n",
      "    accuracy                           0.64       155\n",
      "   macro avg       0.63      0.62      0.62       155\n",
      "weighted avg       0.63      0.64      0.63       155\n",
      "\n",
      "Test Subject: 1717.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.72      0.77       118\n",
      "           1       0.03      0.05      0.04        20\n",
      "\n",
      "    accuracy                           0.62       138\n",
      "   macro avg       0.42      0.39      0.40       138\n",
      "weighted avg       0.70      0.62      0.66       138\n",
      "\n",
      "Test Subject: 1744.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.30      0.29        23\n",
      "           1       0.80      0.77      0.79        84\n",
      "\n",
      "    accuracy                           0.67       107\n",
      "   macro avg       0.54      0.54      0.54       107\n",
      "weighted avg       0.69      0.67      0.68       107\n",
      "\n",
      "Test Subject: 1868.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.11      0.20        71\n",
      "           1       0.36      0.95      0.52        37\n",
      "\n",
      "    accuracy                           0.40       108\n",
      "   macro avg       0.58      0.53      0.36       108\n",
      "weighted avg       0.65      0.40      0.31       108\n",
      "\n",
      "Test Subject: 1892.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.58      0.66        72\n",
      "           1       0.71      0.85      0.77        85\n",
      "\n",
      "    accuracy                           0.73       157\n",
      "   macro avg       0.73      0.72      0.72       157\n",
      "weighted avg       0.73      0.73      0.72       157\n",
      "\n",
      "Test Subject: 1953.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.52      0.45        44\n",
      "           1       0.79      0.69      0.74       116\n",
      "\n",
      "    accuracy                           0.64       160\n",
      "   macro avg       0.59      0.61      0.59       160\n",
      "weighted avg       0.68      0.64      0.66       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fixed missing sampling rate while calculating phasic and tonic response.\n",
    "def losoValidation(folder):\n",
    "    dr_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\{}'.format(folder) # Norm_ECG_EDA_Features_Combined_scld\n",
    "    mat_feat_path = r'X:\\RealTimeSegment\\MatbII\\Extracted\\{}'.format(folder)\n",
    "    bas_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\Norm_ECG_EDA_Features_Baseline_Combined'\n",
    "\n",
    "    xtrainBas, yBas = load_viragebase(bas_feat_path, 'label')\n",
    "    xtrainMat, yMat = load_matb(mat_feat_path, 'scaled label')\n",
    "\n",
    "    subjects = os.listdir(dr_feat_path)\n",
    "    xtrainDriv = pd.DataFrame()\n",
    "\n",
    "    for sdriv in subjects:\n",
    "        # if sdriv not in ['1629.csv']:\n",
    "        #     continue\n",
    "        xtestDriv = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(sdriv)))\n",
    "\n",
    "        # xtestDriv[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0, inplace=True)\n",
    "        xtestDriv = xtestDriv[SELECTCOLS].copy()\n",
    "        xtestDriv.replace([np.inf, -np.inf], 0, inplace=True)  \n",
    "        xtestDriv.dropna(inplace=True) # .reset_index(drop=True, inplace=True)\n",
    "\n",
    "        ytestDriv = list(xtestDriv['label'].copy())\n",
    "\n",
    "        xtrainDriv = pd.DataFrame()\n",
    "        for subTrain in subjects:\n",
    "            if subTrain != sdriv:\n",
    "                train = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(subTrain)))\n",
    "\n",
    "                # train[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0, inplace=True)\n",
    "                train.replace([np.inf, -np.inf], 0, inplace=True)        \n",
    "                # train.dropna(inplace=True)\n",
    "                xtrainDriv = xtrainDriv.append(train)\n",
    "                xtrainDriv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        XtrainDriv = xtrainDriv[SELECTCOLS].copy()\n",
    "        XtrainDriv.dropna(inplace=True)\n",
    "        ytrainDriv = list(XtrainDriv['scaled label'].copy())\n",
    "\n",
    "        XtrainDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "        xtestDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "\n",
    "        XtrainDriv = XtrainDriv.append(xtrainMat)\n",
    "        XtrainDriv = XtrainDriv.append(xtrainBas)\n",
    "\n",
    "        ytrain = ytrainDriv + yMat + yBas\n",
    "        X = XtrainDriv.values\n",
    "\n",
    "        for idx, val in enumerate(ytrain):\n",
    "            if val <= 4:\n",
    "                ytrain[idx] = 0\n",
    "            else: ytrain[idx] = 1\n",
    "\n",
    "        for idx, val in enumerate(ytestDriv):\n",
    "            if val <= 4:\n",
    "                ytestDriv[idx] = 0\n",
    "            else: ytestDriv[idx] = 1\n",
    "\n",
    "        paramsrf = {\n",
    "            'n_estimators': 3000,\n",
    "            'min_samples_split': 2,\n",
    "            'min_samples_leaf': 1,\n",
    "            'max_features': 'sqrt',\n",
    "            'max_depth': 50,\n",
    "            'bootstrap': False\n",
    "            }\n",
    "\n",
    "        paramlgbm = {\n",
    "            'n_estimators': 3000,\n",
    "            'num_leaves': 100,\n",
    "            'learning_rate': 0.05,\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 24\n",
    "            }\n",
    "\n",
    "        clf2 = RandomForestClassifier(**paramsrf)\n",
    "        clf3 = lightgbm.LGBMClassifier(**paramlgbm)\n",
    "        clf4 = SVC(C=700, probability=True, class_weight='balanced')\n",
    "\n",
    "        estimatorList = [('rf', clf2), ('svm', clf4), ('gbm', clf3)]\n",
    "        eclf = VotingClassifier(estimators=estimatorList, voting='soft')\n",
    "\n",
    "        # clf2 = RandomForestClassifier(**paramsrf)\n",
    "        # clf3 = lightgbm.LGBMClassifier(n_estimators = 3000, num_leaves=100, learning_rate=0.005, class_weight='balanced')\n",
    "        # clf4 = SVC(C=1000, probability=True, class_weight='balanced')\n",
    "        # eclf = VotingClassifier(estimators=[('rf', clf2), ('gbm', clf3), ('svm', clf4)], voting='soft') # \n",
    "\n",
    "        hist = eclf.fit(X, ytrain)\n",
    "        yPred = hist.predict(xtestDriv)\n",
    "        print('Test Subject: {}'.format(sdriv))\n",
    "        print(classification_report(ytestDriv, yPred, zero_division=1))            \n",
    "\n",
    "losoValidation('Norm_ECG_EDA_Features_Combined_scld')\n",
    "    # return XtrainDriv, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Subject: 1030.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.74      0.44        38\n",
      "           1       0.86      0.49      0.62       122\n",
      "\n",
      "    accuracy                           0.55       160\n",
      "   macro avg       0.58      0.61      0.53       160\n",
      "weighted avg       0.73      0.55      0.58       160\n",
      "\n",
      "Test Subject: 1105.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.97      0.40        31\n",
      "           1       0.98      0.31      0.47       128\n",
      "\n",
      "    accuracy                           0.44       159\n",
      "   macro avg       0.61      0.64      0.44       159\n",
      "weighted avg       0.83      0.44      0.46       159\n",
      "\n",
      "Test Subject: 1106.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.79      0.32        24\n",
      "           1       0.92      0.45      0.60       136\n",
      "\n",
      "    accuracy                           0.50       160\n",
      "   macro avg       0.56      0.62      0.46       160\n",
      "weighted avg       0.82      0.50      0.56       160\n",
      "\n",
      "Test Subject: 1241.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.59      0.68       129\n",
      "           1       0.17      0.35      0.23        31\n",
      "\n",
      "    accuracy                           0.54       160\n",
      "   macro avg       0.48      0.47      0.45       160\n",
      "weighted avg       0.67      0.54      0.59       160\n",
      "\n",
      "Test Subject: 1271.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.17      0.12        30\n",
      "           1       0.73      0.60      0.66       113\n",
      "\n",
      "    accuracy                           0.51       143\n",
      "   macro avg       0.42      0.38      0.39       143\n",
      "weighted avg       0.60      0.51      0.55       143\n",
      "\n",
      "Test Subject: 1314.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.17      0.24        18\n",
      "           1       0.89      0.97      0.92       121\n",
      "\n",
      "    accuracy                           0.86       139\n",
      "   macro avg       0.66      0.57      0.58       139\n",
      "weighted avg       0.83      0.86      0.84       139\n",
      "\n",
      "Test Subject: 1323.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.36      0.48        84\n",
      "           1       0.46      0.82      0.59        56\n",
      "\n",
      "    accuracy                           0.54       140\n",
      "   macro avg       0.60      0.59      0.54       140\n",
      "weighted avg       0.63      0.54      0.53       140\n",
      "\n",
      "Test Subject: 1337.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87       136\n",
      "           1       0.37      0.42      0.39        26\n",
      "\n",
      "    accuracy                           0.79       162\n",
      "   macro avg       0.63      0.64      0.63       162\n",
      "weighted avg       0.80      0.79      0.80       162\n",
      "\n",
      "Test Subject: 1372.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27        90\n",
      "           1       0.49      1.00      0.65        72\n",
      "\n",
      "    accuracy                           0.53       162\n",
      "   macro avg       0.74      0.58      0.46       162\n",
      "weighted avg       0.77      0.53      0.44       162\n",
      "\n",
      "Test Subject: 1417.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.92      0.24        12\n",
      "           1       0.99      0.55      0.71       150\n",
      "\n",
      "    accuracy                           0.58       162\n",
      "   macro avg       0.56      0.73      0.48       162\n",
      "weighted avg       0.93      0.58      0.67       162\n",
      "\n",
      "Test Subject: 1434.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.42      0.41        36\n",
      "           1       0.81      0.80      0.81       114\n",
      "\n",
      "    accuracy                           0.71       150\n",
      "   macro avg       0.60      0.61      0.61       150\n",
      "weighted avg       0.71      0.71      0.71       150\n",
      "\n",
      "Test Subject: 1544.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76       124\n",
      "           1       0.32      0.66      0.43        29\n",
      "\n",
      "    accuracy                           0.67       153\n",
      "   macro avg       0.60      0.66      0.60       153\n",
      "weighted avg       0.78      0.67      0.70       153\n",
      "\n",
      "Test Subject: 1547.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.04      0.05        28\n",
      "           1       0.78      0.92      0.84       107\n",
      "\n",
      "    accuracy                           0.73       135\n",
      "   macro avg       0.44      0.48      0.45       135\n",
      "weighted avg       0.64      0.73      0.68       135\n",
      "\n",
      "Test Subject: 1595.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.81      0.69        70\n",
      "           1       0.81      0.59      0.68        92\n",
      "\n",
      "    accuracy                           0.69       162\n",
      "   macro avg       0.70      0.70      0.69       162\n",
      "weighted avg       0.72      0.69      0.68       162\n",
      "\n",
      "Test Subject: 1629.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.67      0.55        39\n",
      "           1       0.82      0.67      0.73        87\n",
      "\n",
      "    accuracy                           0.67       126\n",
      "   macro avg       0.64      0.67      0.64       126\n",
      "weighted avg       0.71      0.67      0.68       126\n",
      "\n",
      "Test Subject: 1716.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.72      0.68        89\n",
      "           1       0.55      0.47      0.51        66\n",
      "\n",
      "    accuracy                           0.61       155\n",
      "   macro avg       0.60      0.59      0.59       155\n",
      "weighted avg       0.61      0.61      0.61       155\n",
      "\n",
      "Test Subject: 1717.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78       118\n",
      "           1       0.03      0.05      0.04        20\n",
      "\n",
      "    accuracy                           0.64       138\n",
      "   macro avg       0.43      0.40      0.41       138\n",
      "weighted avg       0.71      0.64      0.67       138\n",
      "\n",
      "Test Subject: 1744.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.35      0.30        23\n",
      "           1       0.80      0.73      0.76        84\n",
      "\n",
      "    accuracy                           0.64       107\n",
      "   macro avg       0.53      0.54      0.53       107\n",
      "weighted avg       0.69      0.64      0.66       107\n",
      "\n",
      "Test Subject: 1868.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.14      0.24        71\n",
      "           1       0.36      0.92      0.52        37\n",
      "\n",
      "    accuracy                           0.41       108\n",
      "   macro avg       0.56      0.53      0.38       108\n",
      "weighted avg       0.63      0.41      0.33       108\n",
      "\n",
      "Test Subject: 1892.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.54      0.64        72\n",
      "           1       0.69      0.88      0.78        85\n",
      "\n",
      "    accuracy                           0.73       157\n",
      "   macro avg       0.75      0.71      0.71       157\n",
      "weighted avg       0.74      0.73      0.72       157\n",
      "\n",
      "Test Subject: 1953.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.55      0.46        44\n",
      "           1       0.80      0.68      0.73       116\n",
      "\n",
      "    accuracy                           0.64       160\n",
      "   macro avg       0.60      0.61      0.60       160\n",
      "weighted avg       0.69      0.64      0.66       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# standardized baseline before normalizing features\n",
    "\n",
    "def losoValidation(folder):\n",
    "    dr_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\{}'.format(folder) # Norm_ECG_EDA_Features_Combined_scld\n",
    "    mat_feat_path = r'X:\\RealTimeSegment\\MatbII\\Extracted\\{}'.format(folder)\n",
    "    bas_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\Norm_ECG_EDA_Features_Baseline_Combined'\n",
    "\n",
    "    xtrainBas, yBas = load_viragebase(bas_feat_path, 'label')\n",
    "    xtrainMat, yMat = load_matb(mat_feat_path, 'scaled label')\n",
    "\n",
    "    subjects = os.listdir(dr_feat_path)\n",
    "    xtrainDriv = pd.DataFrame()\n",
    "\n",
    "    for sdriv in subjects:\n",
    "        # if sdriv not in ['1629.csv']:\n",
    "        #     continue\n",
    "        xtestDriv = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(sdriv)))\n",
    "\n",
    "        # xtestDriv[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0, inplace=True)\n",
    "        xtestDriv = xtestDriv[SELECTCOLS].copy()\n",
    "        xtestDriv.replace([np.inf, -np.inf], 0, inplace=True)  \n",
    "        xtestDriv.dropna(inplace=True) # .reset_index(drop=True, inplace=True)\n",
    "\n",
    "        ytestDriv = list(xtestDriv['label'].copy())\n",
    "\n",
    "        xtrainDriv = pd.DataFrame()\n",
    "        for subTrain in subjects:\n",
    "            if subTrain != sdriv:\n",
    "                train = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(subTrain)))\n",
    "\n",
    "                # train[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0, inplace=True)\n",
    "                train.replace([np.inf, -np.inf], 0, inplace=True)        \n",
    "                # train.dropna(inplace=True)\n",
    "                xtrainDriv = xtrainDriv.append(train)\n",
    "                xtrainDriv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        XtrainDriv = xtrainDriv[SELECTCOLS].copy()\n",
    "        XtrainDriv.dropna(inplace=True)\n",
    "        ytrainDriv = list(XtrainDriv['scaled label'].copy())\n",
    "\n",
    "        XtrainDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "        xtestDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "\n",
    "        XtrainDriv = XtrainDriv.append(xtrainMat)\n",
    "        XtrainDriv = XtrainDriv.append(xtrainBas)\n",
    "\n",
    "        ytrain = ytrainDriv + yMat + yBas\n",
    "        X = XtrainDriv.values\n",
    "\n",
    "        for idx, val in enumerate(ytrain):\n",
    "            if val <= 4:\n",
    "                ytrain[idx] = 0\n",
    "            else: ytrain[idx] = 1\n",
    "\n",
    "        for idx, val in enumerate(ytestDriv):\n",
    "            if val <= 4:\n",
    "                ytestDriv[idx] = 0\n",
    "            else: ytestDriv[idx] = 1\n",
    "\n",
    "        paramsrf = {\n",
    "            'n_estimators': 3000,\n",
    "            'min_samples_split': 2,\n",
    "            'min_samples_leaf': 1,\n",
    "            'max_features': 'sqrt',\n",
    "            'max_depth': 50,\n",
    "            'bootstrap': False\n",
    "            }\n",
    "\n",
    "        paramlgbm = {\n",
    "            'n_estimators': 3000,\n",
    "            'num_leaves': 100,\n",
    "            'learning_rate': 0.05,\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 24\n",
    "            }\n",
    "\n",
    "        clf2 = RandomForestClassifier(**paramsrf)\n",
    "        clf3 = lightgbm.LGBMClassifier(**paramlgbm)\n",
    "        clf4 = SVC(C=700, probability=True, class_weight='balanced')\n",
    "\n",
    "        estimatorList = [('rf', clf2), ('svm', clf4), ('gbm', clf3)]\n",
    "        eclf = VotingClassifier(estimators=estimatorList, voting='soft')\n",
    "\n",
    "        # clf2 = RandomForestClassifier(**paramsrf)\n",
    "        # clf3 = lightgbm.LGBMClassifier(n_estimators = 3000, num_leaves=100, learning_rate=0.005, class_weight='balanced')\n",
    "        # clf4 = SVC(C=1000, probability=True, class_weight='balanced')\n",
    "        # eclf = VotingClassifier(estimators=[('rf', clf2), ('gbm', clf3), ('svm', clf4)], voting='soft') # \n",
    "\n",
    "        hist = eclf.fit(X, ytrain)\n",
    "        yPred = hist.predict(xtestDriv)\n",
    "        print('Test Subject: {}'.format(sdriv))\n",
    "        print(classification_report(ytestDriv, yPred, zero_division=1))            \n",
    "\n",
    "losoValidation('Norm_ECG_EDA_Features_Combined_scld')\n",
    "    # return XtrainDriv, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1030.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.74      0.45        38\n",
      "           1       0.86      0.52      0.65       122\n",
      "\n",
      "    accuracy                           0.57       160\n",
      "   macro avg       0.59      0.63      0.55       160\n",
      "weighted avg       0.73      0.57      0.60       160\n",
      "\n",
      "Train Subject: 1030.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3600\n",
      "           1       1.00      1.00      1.00      3307\n",
      "\n",
      "    accuracy                           1.00      6907\n",
      "   macro avg       1.00      1.00      1.00      6907\n",
      "weighted avg       1.00      1.00      1.00      6907\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1105.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      1.00      0.42        31\n",
      "           1       1.00      0.33      0.49       128\n",
      "\n",
      "    accuracy                           0.46       159\n",
      "   macro avg       0.63      0.66      0.46       159\n",
      "weighted avg       0.86      0.46      0.48       159\n",
      "\n",
      "Train Subject: 1105.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3613\n",
      "           1       1.00      1.00      1.00      3295\n",
      "\n",
      "    accuracy                           1.00      6908\n",
      "   macro avg       1.00      1.00      1.00      6908\n",
      "weighted avg       1.00      1.00      1.00      6908\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1106.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.79      0.31        24\n",
      "           1       0.92      0.42      0.58       136\n",
      "\n",
      "    accuracy                           0.48       160\n",
      "   macro avg       0.56      0.61      0.44       160\n",
      "weighted avg       0.81      0.47      0.54       160\n",
      "\n",
      "Train Subject: 1106.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3573\n",
      "           1       1.00      1.00      1.00      3334\n",
      "\n",
      "    accuracy                           1.00      6907\n",
      "   macro avg       1.00      1.00      1.00      6907\n",
      "weighted avg       1.00      1.00      1.00      6907\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1241.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.63      0.70       129\n",
      "           1       0.19      0.35      0.24        31\n",
      "\n",
      "    accuracy                           0.57       160\n",
      "   macro avg       0.49      0.49      0.47       160\n",
      "weighted avg       0.68      0.57      0.62       160\n",
      "\n",
      "Train Subject: 1241.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3526\n",
      "           1       1.00      1.00      1.00      3381\n",
      "\n",
      "    accuracy                           1.00      6907\n",
      "   macro avg       1.00      1.00      1.00      6907\n",
      "weighted avg       1.00      1.00      1.00      6907\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1271.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.13      0.10        30\n",
      "           1       0.73      0.62      0.67       113\n",
      "\n",
      "    accuracy                           0.52       143\n",
      "   macro avg       0.41      0.38      0.39       143\n",
      "weighted avg       0.59      0.52      0.55       143\n",
      "\n",
      "Train Subject: 1271.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3596\n",
      "           1       1.00      1.00      1.00      3328\n",
      "\n",
      "    accuracy                           1.00      6924\n",
      "   macro avg       1.00      1.00      1.00      6924\n",
      "weighted avg       1.00      1.00      1.00      6924\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1314.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.11      0.18        18\n",
      "           1       0.88      0.98      0.93       121\n",
      "\n",
      "    accuracy                           0.87       139\n",
      "   macro avg       0.69      0.55      0.56       139\n",
      "weighted avg       0.83      0.87      0.83       139\n",
      "\n",
      "Train Subject: 1314.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3588\n",
      "           1       1.00      1.00      1.00      3340\n",
      "\n",
      "    accuracy                           1.00      6928\n",
      "   macro avg       1.00      1.00      1.00      6928\n",
      "weighted avg       1.00      1.00      1.00      6928\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1323.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.35      0.46        84\n",
      "           1       0.44      0.79      0.57        56\n",
      "\n",
      "    accuracy                           0.52       140\n",
      "   macro avg       0.58      0.57      0.52       140\n",
      "weighted avg       0.60      0.52      0.51       140\n",
      "\n",
      "Train Subject: 1323.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3611\n",
      "           1       1.00      1.00      1.00      3316\n",
      "\n",
      "    accuracy                           1.00      6927\n",
      "   macro avg       1.00      1.00      1.00      6927\n",
      "weighted avg       1.00      1.00      1.00      6927\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1337.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85       136\n",
      "           1       0.35      0.54      0.42        26\n",
      "\n",
      "    accuracy                           0.77       162\n",
      "   macro avg       0.63      0.67      0.64       162\n",
      "weighted avg       0.81      0.77      0.78       162\n",
      "\n",
      "Train Subject: 1337.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3519\n",
      "           1       1.00      1.00      1.00      3386\n",
      "\n",
      "    accuracy                           1.00      6905\n",
      "   macro avg       1.00      1.00      1.00      6905\n",
      "weighted avg       1.00      1.00      1.00      6905\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1372.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.17      0.29        90\n",
      "           1       0.49      1.00      0.66        72\n",
      "\n",
      "    accuracy                           0.54       162\n",
      "   macro avg       0.74      0.58      0.47       162\n",
      "weighted avg       0.77      0.54      0.45       162\n",
      "\n",
      "Train Subject: 1372.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3544\n",
      "           1       1.00      1.00      1.00      3361\n",
      "\n",
      "    accuracy                           1.00      6905\n",
      "   macro avg       1.00      1.00      1.00      6905\n",
      "weighted avg       1.00      1.00      1.00      6905\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1417.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.92      0.23        12\n",
      "           1       0.99      0.51      0.68       150\n",
      "\n",
      "    accuracy                           0.54       162\n",
      "   macro avg       0.56      0.71      0.45       162\n",
      "weighted avg       0.92      0.54      0.64       162\n",
      "\n",
      "Train Subject: 1417.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3622\n",
      "           1       1.00      1.00      1.00      3283\n",
      "\n",
      "    accuracy                           1.00      6905\n",
      "   macro avg       1.00      1.00      1.00      6905\n",
      "weighted avg       1.00      1.00      1.00      6905\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1434.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.33      0.35        36\n",
      "           1       0.79      0.82      0.81       114\n",
      "\n",
      "    accuracy                           0.70       150\n",
      "   macro avg       0.58      0.57      0.58       150\n",
      "weighted avg       0.69      0.70      0.70       150\n",
      "\n",
      "Train Subject: 1434.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3588\n",
      "           1       1.00      1.00      1.00      3329\n",
      "\n",
      "    accuracy                           1.00      6917\n",
      "   macro avg       1.00      1.00      1.00      6917\n",
      "weighted avg       1.00      1.00      1.00      6917\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1544.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.72      0.81       124\n",
      "           1       0.39      0.76      0.51        29\n",
      "\n",
      "    accuracy                           0.73       153\n",
      "   macro avg       0.66      0.74      0.66       153\n",
      "weighted avg       0.82      0.73      0.75       153\n",
      "\n",
      "Train Subject: 1544.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3523\n",
      "           1       1.00      1.00      1.00      3391\n",
      "\n",
      "    accuracy                           1.00      6914\n",
      "   macro avg       1.00      1.00      1.00      6914\n",
      "weighted avg       1.00      1.00      1.00      6914\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1547.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.04      0.05        28\n",
      "           1       0.78      0.90      0.83       107\n",
      "\n",
      "    accuracy                           0.72       135\n",
      "   macro avg       0.43      0.47      0.44       135\n",
      "weighted avg       0.64      0.72      0.67       135\n",
      "\n",
      "Train Subject: 1547.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3583\n",
      "           1       1.00      1.00      1.00      3349\n",
      "\n",
      "    accuracy                           1.00      6932\n",
      "   macro avg       1.00      1.00      1.00      6932\n",
      "weighted avg       1.00      1.00      1.00      6932\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1595.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.76      0.66        70\n",
      "           1       0.76      0.59      0.66        92\n",
      "\n",
      "    accuracy                           0.66       162\n",
      "   macro avg       0.67      0.67      0.66       162\n",
      "weighted avg       0.68      0.66      0.66       162\n",
      "\n",
      "Train Subject: 1595.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3520\n",
      "           1       1.00      1.00      1.00      3385\n",
      "\n",
      "    accuracy                           1.00      6905\n",
      "   macro avg       1.00      1.00      1.00      6905\n",
      "weighted avg       1.00      1.00      1.00      6905\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1629.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.79      0.61        39\n",
      "           1       0.88      0.64      0.74        87\n",
      "\n",
      "    accuracy                           0.69       126\n",
      "   macro avg       0.69      0.72      0.68       126\n",
      "weighted avg       0.76      0.69      0.70       126\n",
      "\n",
      "Train Subject: 1629.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3616\n",
      "           1       1.00      1.00      1.00      3325\n",
      "\n",
      "    accuracy                           1.00      6941\n",
      "   macro avg       1.00      1.00      1.00      6941\n",
      "weighted avg       1.00      1.00      1.00      6941\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1716.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68        89\n",
      "           1       0.56      0.55      0.55        66\n",
      "\n",
      "    accuracy                           0.63       155\n",
      "   macro avg       0.62      0.62      0.62       155\n",
      "weighted avg       0.62      0.63      0.63       155\n",
      "\n",
      "Train Subject: 1716.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3611\n",
      "           1       1.00      1.00      1.00      3301\n",
      "\n",
      "    accuracy                           1.00      6912\n",
      "   macro avg       1.00      1.00      1.00      6912\n",
      "weighted avg       1.00      1.00      1.00      6912\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1717.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       118\n",
      "           1       0.05      0.05      0.05        20\n",
      "\n",
      "    accuracy                           0.71       138\n",
      "   macro avg       0.44      0.44      0.44       138\n",
      "weighted avg       0.72      0.71      0.72       138\n",
      "\n",
      "Train Subject: 1717.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3562\n",
      "           1       1.00      1.00      1.00      3367\n",
      "\n",
      "    accuracy                           1.00      6929\n",
      "   macro avg       1.00      1.00      1.00      6929\n",
      "weighted avg       1.00      1.00      1.00      6929\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1744.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.35      0.30        23\n",
      "           1       0.81      0.74      0.77        84\n",
      "\n",
      "    accuracy                           0.65       107\n",
      "   macro avg       0.54      0.54      0.54       107\n",
      "weighted avg       0.69      0.65      0.67       107\n",
      "\n",
      "Train Subject: 1744.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3624\n",
      "           1       1.00      1.00      1.00      3336\n",
      "\n",
      "    accuracy                           1.00      6960\n",
      "   macro avg       1.00      1.00      1.00      6960\n",
      "weighted avg       1.00      1.00      1.00      6960\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1868.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.15      0.26        71\n",
      "           1       0.36      0.92      0.52        37\n",
      "\n",
      "    accuracy                           0.42       108\n",
      "   macro avg       0.57      0.54      0.39       108\n",
      "weighted avg       0.64      0.42      0.35       108\n",
      "\n",
      "Train Subject: 1868.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3584\n",
      "           1       1.00      1.00      1.00      3375\n",
      "\n",
      "    accuracy                           1.00      6959\n",
      "   macro avg       1.00      1.00      1.00      6959\n",
      "weighted avg       1.00      1.00      1.00      6959\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1892.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.47      0.58        72\n",
      "           1       0.66      0.86      0.74        85\n",
      "\n",
      "    accuracy                           0.68       157\n",
      "   macro avg       0.70      0.67      0.66       157\n",
      "weighted avg       0.70      0.68      0.67       157\n",
      "\n",
      "Train Subject: 1892.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3583\n",
      "           1       1.00      1.00      1.00      3327\n",
      "\n",
      "    accuracy                           1.00      6910\n",
      "   macro avg       1.00      1.00      1.00      6910\n",
      "weighted avg       1.00      1.00      1.00      6910\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Test Subject: 1953.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.66      0.54        44\n",
      "           1       0.84      0.70      0.76       116\n",
      "\n",
      "    accuracy                           0.69       160\n",
      "   macro avg       0.65      0.68      0.65       160\n",
      "weighted avg       0.74      0.69      0.70       160\n",
      "\n",
      "Train Subject: 1953.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3561\n",
      "           1       1.00      1.00      1.00      3346\n",
      "\n",
      "    accuracy                           1.00      6907\n",
      "   macro avg       1.00      1.00      1.00      6907\n",
      "weighted avg       1.00      1.00      1.00      6907\n",
      "\n",
      "---------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHANGED PEAK NaN VALUES TO 10000\n",
    "# DROPPED BAD SUBJECTS!\n",
    "\n",
    "def losoValidation(folder):\n",
    "    dr_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\{}'.format(folder) # Norm_ECG_EDA_Features_Combined_scld\n",
    "    mat_feat_path = r'X:\\RealTimeSegment\\MatbII\\Extracted\\{}'.format(folder)\n",
    "    bas_feat_path = r'X:\\RealTimeSegment\\Driving Simulator\\Extracted\\Norm_ECG_EDA_Features_Baseline_Combined'\n",
    "\n",
    "    xtrainBas, yBas = load_viragebase(bas_feat_path, 'label')\n",
    "    xtrainMat, yMat = load_matb(mat_feat_path, 'scaled label')\n",
    "\n",
    "    subjects = os.listdir(dr_feat_path)\n",
    "    xtrainDriv = pd.DataFrame()\n",
    "\n",
    "    test_cls = {}\n",
    "    train_cls = {}\n",
    "    for sdriv in subjects:\n",
    "        # if sdriv not in ['1629.csv']:\n",
    "        #     continue\n",
    "        xtestDriv = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(sdriv)))\n",
    "\n",
    "        # xtestDriv[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0, inplace=True)\n",
    "        xtestDriv = xtestDriv[SELECTCOLS].copy()\n",
    "        xtestDriv.replace([np.inf, -np.inf], 0, inplace=True)  \n",
    "        xtestDriv.dropna(inplace=True) # .reset_index(drop=True, inplace=True)\n",
    "\n",
    "        ytestDriv = list(xtestDriv['label'].copy())\n",
    "\n",
    "        xtrainDriv = pd.DataFrame()\n",
    "        for subTrain in subjects:\n",
    "            if subTrain != sdriv:\n",
    "                train = pd.read_csv(os.path.join(dr_feat_path, '{}'.format(subTrain)))\n",
    "\n",
    "                # train[['scrAmpDF_min','scrRecoveryTime_min', 'scrRiseTime_min']].fillna(0, inplace=True)\n",
    "                train.replace([np.inf, -np.inf], 0, inplace=True)        \n",
    "                # train.dropna(inplace=True)\n",
    "                xtrainDriv = xtrainDriv.append(train)\n",
    "                xtrainDriv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        XtrainDriv = xtrainDriv[SELECTCOLS].copy()\n",
    "        XtrainDriv.dropna(inplace=True)\n",
    "        ytrainDriv = list(XtrainDriv['scaled label'].copy())\n",
    "\n",
    "        XtrainDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "        xtestDriv.drop(columns=['label', 'complexity', 'scaled label'], inplace=True)\n",
    "\n",
    "        XtrainDriv = XtrainDriv.append(xtrainMat)\n",
    "        XtrainDriv = XtrainDriv.append(xtrainBas)\n",
    "\n",
    "        ytrain = ytrainDriv + yMat + yBas\n",
    "        X = XtrainDriv.values\n",
    "\n",
    "        for idx, val in enumerate(ytrain):\n",
    "            if val <= 4:\n",
    "                ytrain[idx] = 0\n",
    "            else: ytrain[idx] = 1\n",
    "\n",
    "        for idx, val in enumerate(ytestDriv):\n",
    "            if val <= 4:\n",
    "                ytestDriv[idx] = 0\n",
    "            else: ytestDriv[idx] = 1\n",
    "\n",
    "        paramsrf = {\n",
    "            'n_estimators': 3000,\n",
    "            'min_samples_split': 2,\n",
    "            'min_samples_leaf': 1,\n",
    "            'max_features': 'sqrt',\n",
    "            'max_depth': 50,\n",
    "            'bootstrap': False\n",
    "            }\n",
    "\n",
    "        paramlgbm = {\n",
    "            'n_estimators': 3000,\n",
    "            'num_leaves': 100,\n",
    "            'learning_rate': 0.05,\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 24\n",
    "            }\n",
    "\n",
    "        clf2 = RandomForestClassifier(**paramsrf)\n",
    "        clf3 = lightgbm.LGBMClassifier(**paramlgbm)\n",
    "        clf4 = SVC(C=700, probability=True, class_weight='balanced')\n",
    "\n",
    "        estimatorList = [('rf', clf2), ('svm', clf4), ('gbm', clf3)]\n",
    "        eclf = VotingClassifier(estimators=estimatorList, voting='soft')\n",
    "\n",
    "        # clf2 = RandomForestClassifier(**paramsrf)\n",
    "        # clf3 = lightgbm.LGBMClassifier(n_estimators = 3000, num_leaves=100, learning_rate=0.005, class_weight='balanced')\n",
    "        # clf4 = SVC(C=1000, probability=True, class_weight='balanced')\n",
    "        # eclf = VotingClassifier(estimators=[('rf', clf2), ('gbm', clf3), ('svm', clf4)], voting='soft') # \n",
    "\n",
    "        hist = eclf.fit(X, ytrain)\n",
    "        yPred = hist.predict(xtestDriv)\n",
    "        ytrainpred = hist.predict(X)\n",
    "        \n",
    "        test_cls[sdriv] = classification_report(ytestDriv, yPred, zero_division=1, output_dict=True)\n",
    "        train_cls[sdriv] = classification_report(ytrain, ytrainpred, zero_division=1, output_dict=True)\n",
    "        \n",
    "        print('---------------------------------------------------\\n')\n",
    "        print('Test Subject: {}'.format(sdriv))\n",
    "        print(classification_report(ytestDriv, yPred, zero_division=1))\n",
    "\n",
    "        print('Train Subject: {}'.format(sdriv))\n",
    "        print(classification_report(ytrain, ytrainpred, zero_division=1))\n",
    "        print('---------------------------------------------------\\n')\n",
    "\n",
    "    return train_cls, test_cls\n",
    "train_cls, test_cls = losoValidation('Norm_ECG_EDA_Features_Combined_scld')\n",
    "    # return XtrainDriv, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.6254702294699697\n",
      "Mean f1: 0.5544823378126126\n"
     ]
    }
   ],
   "source": [
    "acc=[]\n",
    "f_1=[]\n",
    "for key in test_cls.keys():\n",
    "    if key in ['1868.csv', '1717.csv', '1547.csv']:\n",
    "        continue\n",
    "\n",
    "    accuracy_ = test_cls[key]['accuracy']\n",
    "    acc.append(accuracy_)\n",
    "    fscore_ = test_cls[key]['macro avg']['f1-score']\n",
    "    f_1.append(fscore_)\n",
    "\n",
    "# Average acc and f1\n",
    "print(\"Mean accuracy: {}\".format(np.mean(acc)))\n",
    "print(\"Mean f1: {}\".format(np.mean(f_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('ideas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac849e5261554532e3790d4f708a2062d2910516871bbb9810c6685f1d16f243"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
